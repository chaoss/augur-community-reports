{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull Request Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Limitations for Reporting on Several Repos\n",
    "The visualizations in this notebook are, like most, able to coherently display information for between 1 and 8 different repositories simultaneously. \n",
    "\n",
    "## Alternatives for Reporting on Repo Groups, Comprising Many Repos\n",
    "The included queries could be rewritten to show an entire repository group's characteristics of that is your primary aim. Specifically, any query could replace this line: \n",
    "```\n",
    "                            WHERE repo.repo_id = {repo_id}\n",
    "```\n",
    "\n",
    "with this line to accomplish the goal of comparing different groups of repositories: \n",
    "```\n",
    "                            WHERE repogroups.repo_group_id = {repo_id}\n",
    "```\n",
    "\n",
    "Simply replace the set of id's in the **Pull Request Filter** section with a list of repo_group_id numbers as well, to accomplish this view. \n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd \n",
    "import sqlalchemy as salc\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import datetime\n",
    "import json\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "with open(\"config.json\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "database_connection_string = 'postgres+psycopg2://{}:{}@{}:{}/{}'.format(config['user'], config['password'], config['host'], config['port'], config['database'])\n",
    "\n",
    "dbschema='augur_data'\n",
    "engine = salc.create_engine(\n",
    "    database_connection_string,\n",
    "    connect_args={'options': '-csearch_path={}'.format(dbschema)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declare all repo ids you would like to produce charts for\n",
    "repo_set = {25440, 25448}\n",
    "\n",
    "#can be set as 'competitors' or 'repo'\n",
    "#'competitors' will group graphs by type, so it is easy to compare across repos\n",
    "# 'repo' will group graphs by repo so it is easy to look at all the contributor data for each repo\n",
    "display_grouping = 'repo'\n",
    "\n",
    "#if display_grouping is set to 'competitors', enter the repo ids you do no want to alias, if 'display_grouping' is set to repo the list will not effect anything\n",
    "not_aliased_repos = [25502, 25583]\n",
    "\n",
    "begin_date = '2018-01-01'\n",
    "end_date = '2020-07-30'\n",
    "\n",
    "#specify number of outliers for removal in scatter plot\n",
    "scatter_plot_outliers_removed = 5\n",
    "save_files = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying the Longest Running Pull Requests\n",
    "\n",
    "## Getting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_all = pd.DataFrame()\n",
    "\n",
    "for repo_id in repo_set: \n",
    "\n",
    "    pr_query = salc.sql.text(f\"\"\"\n",
    "                    SELECT\n",
    "                        repo.repo_id AS repo_id,\n",
    "                        pull_requests.pr_src_id AS pr_src_id,\n",
    "                        repo.repo_name AS repo_name,\n",
    "                        pr_src_author_association,\n",
    "                        repo_groups.rg_name AS repo_group,\n",
    "                        pull_requests.pr_src_state,\n",
    "                        pull_requests.pr_merged_at,\n",
    "                        pull_requests.pr_created_at AS pr_created_at,\n",
    "                        pull_requests.pr_closed_at AS pr_closed_at,\n",
    "                        date_part( 'year', pr_created_at :: DATE ) AS CREATED_YEAR,\n",
    "                        date_part( 'month', pr_created_at :: DATE ) AS CREATED_MONTH,\n",
    "                        date_part( 'year', pr_closed_at :: DATE ) AS CLOSED_YEAR,\n",
    "                        date_part( 'month', pr_closed_at :: DATE ) AS CLOSED_MONTH,\n",
    "                        pr_src_meta_label,\n",
    "                        pr_head_or_base,\n",
    "                        ( EXTRACT ( EPOCH FROM pull_requests.pr_closed_at ) - EXTRACT ( EPOCH FROM pull_requests.pr_created_at ) ) / 3600 AS hours_to_close,\n",
    "                        ( EXTRACT ( EPOCH FROM pull_requests.pr_closed_at ) - EXTRACT ( EPOCH FROM pull_requests.pr_created_at ) ) / 86400 AS days_to_close, \n",
    "                        ( EXTRACT ( EPOCH FROM first_response_time ) - EXTRACT ( EPOCH FROM pull_requests.pr_created_at ) ) / 3600 AS hours_to_first_response,\n",
    "                        ( EXTRACT ( EPOCH FROM first_response_time ) - EXTRACT ( EPOCH FROM pull_requests.pr_created_at ) ) / 86400 AS days_to_first_response, \n",
    "                        ( EXTRACT ( EPOCH FROM last_response_time ) - EXTRACT ( EPOCH FROM pull_requests.pr_created_at ) ) / 3600 AS hours_to_last_response,\n",
    "                        ( EXTRACT ( EPOCH FROM last_response_time ) - EXTRACT ( EPOCH FROM pull_requests.pr_created_at ) ) / 86400 AS days_to_last_response, \n",
    "                        first_response_time,\n",
    "                        last_response_time,\n",
    "                        average_time_between_responses,\n",
    "                        assigned_count,\n",
    "                        review_requested_count,\n",
    "                        labeled_count,\n",
    "                        subscribed_count,\n",
    "                        mentioned_count,\n",
    "                        referenced_count,\n",
    "                        closed_count,\n",
    "                        head_ref_force_pushed_count,\n",
    "                        merged_count,\n",
    "                        milestoned_count,\n",
    "                        unlabeled_count,\n",
    "                        head_ref_deleted_count,\n",
    "                        comment_count,\n",
    "                        lines_added, \n",
    "                        lines_removed,\n",
    "                        commit_count, \n",
    "                        file_count\n",
    "                    FROM\n",
    "                        repo,\n",
    "                        repo_groups,\n",
    "                        pull_requests LEFT OUTER JOIN ( \n",
    "                            SELECT pull_requests.pull_request_id,\n",
    "                            count(*) FILTER (WHERE action = 'assigned') AS assigned_count,\n",
    "                            count(*) FILTER (WHERE action = 'review_requested') AS review_requested_count,\n",
    "                            count(*) FILTER (WHERE action = 'labeled') AS labeled_count,\n",
    "                            count(*) FILTER (WHERE action = 'unlabeled') AS unlabeled_count,\n",
    "                            count(*) FILTER (WHERE action = 'subscribed') AS subscribed_count,\n",
    "                            count(*) FILTER (WHERE action = 'mentioned') AS mentioned_count,\n",
    "                            count(*) FILTER (WHERE action = 'referenced') AS referenced_count,\n",
    "                            count(*) FILTER (WHERE action = 'closed') AS closed_count,\n",
    "                            count(*) FILTER (WHERE action = 'head_ref_force_pushed') AS head_ref_force_pushed_count,\n",
    "                            count(*) FILTER (WHERE action = 'head_ref_deleted') AS head_ref_deleted_count,\n",
    "                            count(*) FILTER (WHERE action = 'milestoned') AS milestoned_count,\n",
    "                            count(*) FILTER (WHERE action = 'merged') AS merged_count,\n",
    "                            MIN(message.msg_timestamp) AS first_response_time,\n",
    "                            COUNT(DISTINCT message.msg_timestamp) AS comment_count,\n",
    "                            MAX(message.msg_timestamp) AS last_response_time,\n",
    "                            (MAX(message.msg_timestamp) - MIN(message.msg_timestamp)) / COUNT(DISTINCT message.msg_timestamp) AS average_time_between_responses\n",
    "                            FROM pull_request_events, pull_requests, repo, pull_request_message_ref, message\n",
    "                            WHERE repo.repo_id = {repo_id}\n",
    "                            AND repo.repo_id = pull_requests.repo_id\n",
    "                            AND pull_requests.pull_request_id = pull_request_events.pull_request_id\n",
    "                            AND pull_requests.pull_request_id = pull_request_message_ref.pull_request_id\n",
    "                            AND pull_request_message_ref.msg_id = message.msg_id\n",
    "                            GROUP BY pull_requests.pull_request_id\n",
    "                        ) response_times\n",
    "                        ON pull_requests.pull_request_id = response_times.pull_request_id\n",
    "                        LEFT OUTER JOIN (\n",
    "                            SELECT pull_request_commits.pull_request_id, count(DISTINCT pr_cmt_sha) AS commit_count                                FROM pull_request_commits, pull_requests, pull_request_meta\n",
    "                            WHERE pull_requests.pull_request_id = pull_request_commits.pull_request_id\n",
    "                            AND pull_requests.pull_request_id = pull_request_meta.pull_request_id\n",
    "                            AND pull_requests.repo_id = {repo_id}\n",
    "                            AND pr_cmt_sha <> pull_requests.pr_merge_commit_sha\n",
    "                            AND pr_cmt_sha <> pull_request_meta.pr_sha\n",
    "                            GROUP BY pull_request_commits.pull_request_id\n",
    "                        ) all_commit_counts\n",
    "                        ON pull_requests.pull_request_id = all_commit_counts.pull_request_id\n",
    "                        LEFT OUTER JOIN (\n",
    "                            SELECT MAX(pr_repo_meta_id), pull_request_meta.pull_request_id, pr_head_or_base, pr_src_meta_label\n",
    "                            FROM pull_requests, pull_request_meta\n",
    "                            WHERE pull_requests.pull_request_id = pull_request_meta.pull_request_id\n",
    "                            AND pull_requests.repo_id = {repo_id}\n",
    "                            AND pr_head_or_base = 'base'\n",
    "                            GROUP BY pull_request_meta.pull_request_id, pr_head_or_base, pr_src_meta_label\n",
    "                        ) base_labels\n",
    "                        ON base_labels.pull_request_id = all_commit_counts.pull_request_id\n",
    "                        LEFT OUTER JOIN (\n",
    "                            SELECT sum(cmt_added) AS lines_added, sum(cmt_removed) AS lines_removed, pull_request_commits.pull_request_id, count(DISTINCT cmt_filename) AS file_count\n",
    "                            FROM pull_request_commits, commits, pull_requests, pull_request_meta\n",
    "                            WHERE cmt_commit_hash = pr_cmt_sha\n",
    "                            AND pull_requests.pull_request_id = pull_request_commits.pull_request_id\n",
    "                            AND pull_requests.pull_request_id = pull_request_meta.pull_request_id\n",
    "                            AND pull_requests.repo_id = {repo_id}\n",
    "                            AND commits.repo_id = pull_requests.repo_id\n",
    "                            AND commits.cmt_commit_hash <> pull_requests.pr_merge_commit_sha\n",
    "                            AND commits.cmt_commit_hash <> pull_request_meta.pr_sha\n",
    "                            GROUP BY pull_request_commits.pull_request_id\n",
    "                        ) master_merged_counts \n",
    "                        ON base_labels.pull_request_id = master_merged_counts.pull_request_id                    \n",
    "                    WHERE \n",
    "                        repo.repo_group_id = repo_groups.repo_group_id \n",
    "                        AND repo.repo_id = pull_requests.repo_id \n",
    "                        AND repo.repo_id = {repo_id} \n",
    "                    ORDER BY\n",
    "                       merged_count DESC\n",
    "        \"\"\")\n",
    "    pr_a = pd.read_sql(pr_query, con=engine)\n",
    "    if not pr_all.empty: \n",
    "        pr_all = pd.concat([pr_all, pr_a]) \n",
    "    else: \n",
    "        # first repo\n",
    "        pr_all = pr_a\n",
    "display(pr_all.head())\n",
    "pr_all.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin data pre-processing and adding columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data type changing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change count columns from float datatype to integer\n",
    "pr_all[['assigned_count',\n",
    "          'review_requested_count',\n",
    "          'labeled_count',\n",
    "          'subscribed_count',\n",
    "          'mentioned_count',\n",
    "          'referenced_count',\n",
    "          'closed_count',\n",
    "          'head_ref_force_pushed_count',\n",
    "          'merged_count',\n",
    "          'milestoned_count',\n",
    "          'unlabeled_count',\n",
    "          'head_ref_deleted_count',\n",
    "          'comment_count',\n",
    "        'commit_count',\n",
    "        'file_count',\n",
    "        'lines_added',\n",
    "        'lines_removed'\n",
    "       ]] = pr_all[['assigned_count',\n",
    "                                      'review_requested_count',\n",
    "                                      'labeled_count',\n",
    "                                      'subscribed_count',\n",
    "                                      'mentioned_count',\n",
    "                                      'referenced_count',\n",
    "                                      'closed_count',\n",
    "                                        'head_ref_force_pushed_count',\n",
    "                                    'merged_count',\n",
    "                                      'milestoned_count',          \n",
    "                                      'unlabeled_count',\n",
    "                                      'head_ref_deleted_count',\n",
    "                                      'comment_count',\n",
    "                                        'commit_count',\n",
    "                                        'file_count',\n",
    "                                        'lines_added',\n",
    "                                        'lines_removed'\n",
    "                   ]].astype(float)\n",
    "# Change years to int so that doesn't display as 2019.0 for example\n",
    "pr_all[[\n",
    "            'created_year',\n",
    "           'closed_year']] = pr_all[['created_year',\n",
    "                                       'closed_year']].fillna(-1).astype(int).astype(str)\n",
    "pr_all.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pr_all['repo_name'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add `average_days_between_responses` and `average_hours_between_responses` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get days for average_time_between_responses time delta\n",
    "\n",
    "pr_all['average_days_between_responses'] = pr_all['average_time_between_responses'].map(lambda x: x.days).astype(float)\n",
    "pr_all['average_hours_between_responses'] = pr_all['average_time_between_responses'].map(lambda x: x.days * 24).astype(float)\n",
    "\n",
    "pr_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date filtering entire dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = pd.to_datetime(begin_date)\n",
    "# end_date = pd.to_datetime('2020-02-01 09:00:00')\n",
    "end_date = pd.to_datetime(end_date)\n",
    "pr_all = pr_all[(pr_all['pr_created_at'] > start_date) & (pr_all['pr_closed_at'] < end_date)]\n",
    "\n",
    "pr_all['created_year'] = pr_all['created_year'].map(int)\n",
    "pr_all['created_month'] = pr_all['created_month'].map(int)\n",
    "pr_all['created_month'] = pr_all['created_month'].map(lambda x: '{0:0>2}'.format(x))\n",
    "pr_all['created_yearmonth'] = pd.to_datetime(pr_all['created_year'].map(str) + '-' + pr_all['created_month'].map(str) + '-01')\n",
    "pr_all.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add `days_to_close` column for pull requests that are still open (closed pull requests already have this column filled from the query)\n",
    "\n",
    "Note: there will be no pull requests that are still open in the dataframe if you filtered by an end date in the above cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "# getting the number of days of (today - created at) for the PRs that are still open\n",
    "# and putting this in the days_to_close column\n",
    "\n",
    "# get timedeltas of creation time to todays date/time\n",
    "days_to_close_open_pr = datetime.datetime.now() - pr_all.loc[pr_all['pr_src_state'] == 'open']['pr_created_at']\n",
    "\n",
    "# get num days from above timedelta\n",
    "days_to_close_open_pr = days_to_close_open_pr.apply(lambda x: x.days).astype(int)\n",
    "\n",
    "# for only OPEN pr's, set the days_to_close column equal to above dataframe\n",
    "pr_all.loc[pr_all['pr_src_state'] == 'open'] = pr_all.loc[pr_all['pr_src_state'] == 'open'].assign(days_to_close=days_to_close_open_pr)\n",
    "\n",
    "pr_all.loc[pr_all['pr_src_state'] == 'open'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add `closed_yearmonth` column for only CLOSED pull requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate column by setting all null datetimes\n",
    "pr_all['closed_yearmonth'] = pd.to_datetime(np.nan)\n",
    "\n",
    "# Fill column with prettified string of year/month closed that looks like: 2019-07-01\n",
    "pr_all.loc[pr_all['pr_src_state'] == 'closed'] = pr_all.loc[pr_all['pr_src_state'] == 'closed'].assign(\n",
    "    closed_yearmonth = pd.to_datetime(pr_all.loc[pr_all['pr_src_state'] == 'closed']['closed_year'].astype(int\n",
    "        ).map(str) + '-' + pr_all.loc[pr_all['pr_src_state'] == 'closed']['closed_month'].astype(int).map(str) + '-01'))\n",
    "\n",
    "pr_all.loc[pr_all['pr_src_state'] == 'closed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add `merged_flag` column which is just prettified strings based off of if the `pr_merged_at` column is null or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Merged flag \"\"\"\n",
    "if 'pr_merged_at' in pr_all.columns.values:\n",
    "    pr_all['pr_merged_at'] = pr_all['pr_merged_at'].fillna(0)\n",
    "    pr_all['merged_flag'] = 'Not Merged / Rejected'\n",
    "    pr_all['merged_flag'].loc[pr_all['pr_merged_at'] != 0] = 'Merged / Accepted'\n",
    "    pr_all['merged_flag'].loc[pr_all['pr_src_state'] == 'open'] = 'Still Open'\n",
    "    del pr_all['pr_merged_at']\n",
    "pr_all['merged_flag']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into different dataframes\n",
    "### All, open, closed, and slowest 20% of these 3 categories (6 dataframes total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate the different state PRs for now\n",
    "pr_open = pr_all.loc[pr_all['pr_src_state'] == 'open']\n",
    "pr_closed = pr_all.loc[pr_all['pr_src_state'] == 'closed']\n",
    "pr_merged = pr_all.loc[pr_all['merged_flag'] == 'Merged / Accepted']\n",
    "pr_not_merged = pr_all.loc[pr_all['merged_flag'] == 'Not Merged / Rejected']\n",
    "pr_closed['merged_flag']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframes that contain the slowest 20% pull requests of each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the 80th percentile slowest PRs\n",
    "\n",
    "def filter_20_per_slowest(input_df):\n",
    "    pr_slow20_filtered = pd.DataFrame()\n",
    "    pr_slow20_x = pd.DataFrame()\n",
    "    for value in repo_set: \n",
    "        if not pr_slow20_filtered.empty: \n",
    "            pr_slow20x = input_df.query('repo_id==@value')\n",
    "            pr_slow20x['percentile_rank_local'] = pr_slow20x.days_to_close.rank(pct=True)\n",
    "            pr_slow20x = pr_slow20x.query('percentile_rank_local >= .8', )\n",
    "            pr_slow20_filtered = pd.concat([pr_slow20x, pr_slow20_filtered]) \n",
    "            reponame = str(value)\n",
    "            filename = ''.join(['output/pr_slowest20pct', reponame, '.csv'])\n",
    "            pr_slow20x.to_csv(filename)\n",
    "        else: \n",
    "            # first time\n",
    "            pr_slow20_filtered = input_df.copy()\n",
    "            pr_slow20_filtered['percentile_rank_local'] = pr_slow20_filtered.days_to_close.rank(pct=True)\n",
    "            pr_slow20_filtered = pr_slow20_filtered.query('percentile_rank_local >= .8', )\n",
    "#     print(pr_slow20_filtered.describe())\n",
    "    return pr_slow20_filtered\n",
    "\n",
    "pr_slow20_open = filter_20_per_slowest(pr_open)\n",
    "pr_slow20_closed = filter_20_per_slowest(pr_closed)\n",
    "pr_slow20_merged = filter_20_per_slowest(pr_merged)\n",
    "pr_slow20_not_merged = filter_20_per_slowest(pr_not_merged)\n",
    "pr_slow20_all = filter_20_per_slowest(pr_all)\n",
    "pr_slow20_merged#.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionairy with number as the key and a letter as the value\n",
    "#this is used to alias repos when using 'compeitor' display grouping\n",
    "letters = []\n",
    "nums = []\n",
    "alpha = 'a'\n",
    "for i in range(0, 26): \n",
    "    letters.append(alpha) \n",
    "    alpha = chr(ord(alpha) + 1)\n",
    "    nums.append(i)\n",
    "letters = [x.upper() for x in letters]\n",
    "\n",
    "#create dict out of list of numbers and letters\n",
    "repo_alias_dict = {nums[i]: letters[i] for i in range(len(nums))}\n",
    "\n",
    "# create dict in the form {repo_id : repo_name}\n",
    "aliased_repos = []\n",
    "repo_dict = {}\n",
    "count = 0\n",
    "for repo_id in repo_set:\n",
    "    \n",
    "    #find corresponding repo name from each repo_id \n",
    "    repo_name = pr_all.loc[pr_all['repo_id'] == repo_id].iloc[0]['repo_name']\n",
    "    \n",
    "    #if competitor grouping is enabled turn all repo names, other than the ones in the 'not_aliased_repos' into an alias\n",
    "    if display_grouping == 'competitors' and not repo_id in not_aliased_repos:\n",
    "        repo_name =  'Repo ' + repo_alias_dict[count]\n",
    "        \n",
    "        #add repo_id to list of aliased repos, this is used for ordering\n",
    "        aliased_repos.append(repo_id)\n",
    "        count += 1\n",
    "        \n",
    "    #add repo_id and repo names as key value pairs into a dict, this is used to label the title of the visualizations\n",
    "    repo_dict.update({repo_id : repo_name})\n",
    "\n",
    "#gurantees that the non_aliased repos come first when display grouping is set as 'competitors'\n",
    "repo_list = not_aliased_repos + aliased_repos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Visualization Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.palettes import Colorblind, mpl, Category20\n",
    "from bokeh.layouts import gridplot\n",
    "from bokeh.models.annotations import Title\n",
    "from bokeh.io import export_png\n",
    "from bokeh.io import show, output_notebook\n",
    "from bokeh.models import ColumnDataSource, Legend, LabelSet, Range1d, LinearAxis, Label\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models.glyphs import Rect\n",
    "from bokeh.transform import dodge\n",
    "\n",
    "try:\n",
    "    colors = Colorblind[len(repo_set)]\n",
    "except:\n",
    "    colors = Colorblind[3]\n",
    "#mpl['Plasma'][len(repo_set)]\n",
    "#['A6CEE3','B2DF8A','33A02C','FB9A99']\n",
    "\n",
    "def remove_outliers(input_df, field, num_outliers_repo_map):\n",
    "    df_no_outliers = input_df.copy()\n",
    "    for repo_name, num_outliers in num_outliers_repo_map.items():\n",
    "        indices_to_drop = input_df.loc[input_df['repo_name'] == repo_name].nlargest(num_outliers, field).index\n",
    "        df_no_outliers = df_no_outliers.drop(index=indices_to_drop)\n",
    "    return df_no_outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.dates import DateFormatter\n",
    "import datetime as dt\n",
    "\n",
    "def visualize_mean_days_to_close(input_df, x_axis='closed_yearmonth', description='Closed', save_file=False, num_remove_outliers=0, drop_outliers_repo=None):\n",
    "\n",
    "    # Set the df you want to build the viz's for\n",
    "    driver_df = input_df.copy()\n",
    "    \n",
    "    driver_df = driver_df[['repo_id', 'repo_name', 'pr_src_id', 'created_yearmonth', 'closed_yearmonth', 'days_to_close']]\n",
    "\n",
    "    if save_file:\n",
    "        driver_df.to_csv('output/c.westw20small {}.csv'.format(description))\n",
    "    \n",
    "    driver_df_mean = driver_df.groupby(['repo_id', x_axis, 'repo_name'],as_index=False).mean()\n",
    "        \n",
    "    # Total PRS Closed\n",
    "    fig, ax = plt.subplots()\n",
    "    # the size of A4 paper\n",
    "    fig.set_size_inches(16, 8)\n",
    "    plotter = sns.lineplot(x=x_axis, y='days_to_close', style='repo_name', data=driver_df_mean, sort=True, legend='full', linewidth=2.5, hue='repo_name').set_title(\"Average Days to Close of {} Pull Requests, July 2017-January 2020\".format(description))  \n",
    "    if save_file:\n",
    "        fig.savefig('images/slow_20_mean {}.png'.format(description))\n",
    "    \n",
    "    # Copying array and deleting the outlier in the copy to re-visualize\n",
    "    def drop_n_largest(input_df, n, repo_name):\n",
    "        input_df_copy = input_df.copy()\n",
    "        indices_to_drop = input_df.loc[input_df['repo_name'] == 'amazon-freertos'].nlargest(n,'days_to_close').index\n",
    "        print(\"Indices to drop: {}\".format(indices_to_drop))\n",
    "        input_df_copy = input_df_copy.drop(index=indices_to_drop)\n",
    "        input_df_copy.loc[input_df['repo_name'] == repo_name]\n",
    "        return input_df_copy\n",
    "\n",
    "    if num_remove_outliers > 0 and drop_outliers_repo:\n",
    "        driver_df_mean_no_outliers = drop_n_largest(driver_df_mean, num_remove_outliers, drop_outliers_repo)\n",
    "    \n",
    "        # Total PRS Closed without outlier\n",
    "        fig, ax = plt.subplots()\n",
    "        # the size of A4 paper\n",
    "        fig.set_size_inches(16, 8)\n",
    "        plotter = sns.lineplot(x=x_axis, y='days_to_close', style='repo_name', data=driver_df_mean_no_outliers, sort=False, legend='full', linewidth=2.5, hue='repo_name').set_title(\"Average Days to Close among {} Pull Requests Without Outlier, July 2017-January 2020\".format(description))\n",
    "        plotterlabels = ax.set_xticklabels(driver_df_mean_no_outliers[x_axis], rotation=90, fontsize=8)\n",
    "        if save_file:\n",
    "            fig.savefig('images/slow_20_mean_no_outlier {}.png'.format(description))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize_mean_days_to_close(pr_closed, description='All Closed', save_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import ColumnDataSource, FactorRange\n",
    "from bokeh.transform import factor_cmap\n",
    "\n",
    "def vertical_grouped_bar(input_df, repo_id, group_by = 'merged_flag', x_axis='closed_year', y_axis='num_commits', description='All', title=\"{}Average Commit Counts Per Year for {} Pull Requests\"):\n",
    "    \n",
    "    if type(repo_id) == type(repo_list):\n",
    "        repo_ids = repo_id\n",
    "    else:\n",
    "        repo_ids = [repo_id]\n",
    "    \n",
    "    for repo_id in repo_ids:\n",
    "    \n",
    "        output_notebook() # let bokeh display plot in jupyter cell output\n",
    "\n",
    "        driver_df = input_df.copy() # deep copy input data so we do not change the external dataframe \n",
    "\n",
    "        # Filter df by passed *repo_id* param\n",
    "        driver_df = driver_df.loc[driver_df['repo_id'] == repo_id]\n",
    "\n",
    "        # Change closed year to int so that doesn't display as 2019.0 for example\n",
    "        driver_df['closed_year'] = driver_df['closed_year'].astype(int).astype(str)\n",
    "\n",
    "        # contains the closed years\n",
    "        x_groups = sorted(list(driver_df[x_axis].unique()))\n",
    "\n",
    "        # inner groups on x_axis they are merged and not_merged\n",
    "        groups = list(driver_df[group_by].unique())\n",
    "\n",
    "        # setup color pallete\n",
    "        try:\n",
    "            colors = mpl['Plasma'][len(groups)]\n",
    "        except:\n",
    "            colors = [mpl['Plasma'][3][0]] + [mpl['Plasma'][3][1]]\n",
    "\n",
    "        merged_avg_values = list(driver_df.loc[driver_df[group_by] == 'Merged / Accepted'].groupby([x_axis],as_index=False).mean().round(1)['commit_count'])\n",
    "        not_merged_avg_values = list(driver_df.loc[driver_df[group_by] == 'Not Merged / Rejected'].groupby([x_axis],as_index=False).mean().round(1)['commit_count'])\n",
    "\n",
    "\n",
    "        # Setup data in format for grouped bar chart\n",
    "        data = {\n",
    "                'years'                   : x_groups,\n",
    "                'Merged / Accepted'       : merged_avg_values,\n",
    "                'Not Merged / Rejected'   : not_merged_avg_values,\n",
    "            }\n",
    "\n",
    "        x = [ (year, pr_state) for year in x_groups for pr_state in groups ]\n",
    "        counts = sum(zip(data['Merged / Accepted'], data['Not Merged / Rejected']), ())\n",
    "\n",
    "        source = ColumnDataSource(data=dict(x=x, counts=counts))\n",
    "\n",
    "        title_beginning = '{}: '.format(repo_dict[repo_id])\n",
    "        title=title.format(title_beginning, description)\n",
    "        \n",
    "        plot_width = len(x_groups) * 300\n",
    "        title_text_font_size = 16 \n",
    "        \n",
    "        if (len(title) * title_text_font_size / 2) > plot_width:\n",
    "            plot_width = int(len(title) * title_text_font_size / 2) + 40\n",
    "        \n",
    "        p = figure(x_range=FactorRange(*x), plot_height=450, plot_width=plot_width, title=title, y_range=(0, max(merged_avg_values + not_merged_avg_values)*1.15), toolbar_location=None)\n",
    "\n",
    "        # Vertical bar glyph\n",
    "        p.vbar(x='x', top='counts', width=0.9, source=source, line_color=\"white\",\n",
    "               fill_color=factor_cmap('x', palette=colors, factors=groups, start=1, end=2))\n",
    "\n",
    "        # Data label \n",
    "        labels = LabelSet(x='x', y='counts', text='counts',# y_offset=-8, x_offset=34,\n",
    "                  text_font_size=\"12pt\", text_color=\"black\",\n",
    "                  source=source, text_align='center')\n",
    "        p.add_layout(labels)\n",
    "\n",
    "        p.y_range.start = 0\n",
    "        p.x_range.range_padding = 0.1\n",
    "        p.xaxis.major_label_orientation = 1\n",
    "        p.xgrid.grid_line_color = None\n",
    "\n",
    "        p.yaxis.axis_label = 'Average Commits / Pull Request'\n",
    "        p.xaxis.axis_label = 'Year Closed'\n",
    "\n",
    "        p.title.align = \"center\"\n",
    "        p.title.text_font_size = \"{}px\".format(title_text_font_size)\n",
    "\n",
    "        p.xaxis.axis_label_text_font_size = \"16px\"\n",
    "        p.xaxis.major_label_text_font_size = \"15px\"\n",
    "\n",
    "        p.yaxis.axis_label_text_font_size = \"15px\"\n",
    "        p.yaxis.major_label_text_font_size = \"15px\"\n",
    "        \n",
    "        plot = p\n",
    "\n",
    "        p = figure(width = plot_width, height=200, margin = (0, 0, 0, 0))\n",
    "        caption = \"This graph shows the average commits per pull requests over an entire year, for merged and not merged pull requests.\"\n",
    "        p.add_layout(Label(\n",
    "        x = 0, # Change to shift caption left or right\n",
    "        y = 160, \n",
    "        x_units = 'screen',\n",
    "        y_units = 'screen',\n",
    "        text='{}'.format(caption),\n",
    "        text_font = 'times', # Use same font as paper\n",
    "        text_font_size = '15pt',\n",
    "        render_mode='css'\n",
    "        ))\n",
    "        p.outline_line_color = None\n",
    "\n",
    "        caption_plot = p\n",
    "\n",
    "        grid = gridplot([[plot], [caption_plot]])\n",
    "        \n",
    "        show(grid)\n",
    "\n",
    "\n",
    "        #show(p)\n",
    "\n",
    "        if save_files:\n",
    "            export_png(grid, filename=\"./images/v_grouped_bar/v_grouped_bar__{}_PRs__yaxis_{}__repo_{}.png\".format(description, y_axis, repo_dict[repo_id]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vertical_grouped_bar(pr_all, repo_id=repo_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vertical_grouped_bar_line_counts(input_df, repo_id, x_axis='closed_year', y_max1=600000, y_max2=1000, description=\"\", title =\"\", save_file=False):\n",
    "    output_notebook() # let bokeh display plot in jupyter cell output\n",
    "    \n",
    "    driver_df = input_df.copy() # deep copy input data so we do not change the external dataframe \n",
    "    \n",
    "    # Filter df by passed *repo_id* param\n",
    "    driver_df = driver_df.loc[driver_df['repo_id'] == repo_id]\n",
    "        \n",
    "    # Change closed year to int so that doesn't display as 2019.0 for example\n",
    "    driver_df['closed_year'] = driver_df['closed_year'].astype(int).astype(str)\n",
    "    \n",
    "    # contains the closed years\n",
    "    x_groups = sorted(list(driver_df[x_axis].unique()))\n",
    "    \n",
    "    groups = ['Lines Added', 'Lines Removed', 'Files Changed']\n",
    "    \n",
    "    # setup color pallete\n",
    "    colors = mpl['Plasma'][3]\n",
    "    \n",
    "    display(pr_all[pr_all['lines_added'].notna()])#.groupby([x_axis],as_index=False).mean())\n",
    "        \n",
    "    files_avg_values = list(driver_df.groupby([x_axis],as_index=False).mean().round(1)['file_count'])\n",
    "    added_avg_values = list(driver_df.groupby([x_axis],as_index=False).mean().round(1)['lines_added'])\n",
    "    removed_avg_values = list(driver_df.groupby([x_axis],as_index=False).mean().round(1)['lines_removed'])\n",
    "    display(driver_df.groupby([x_axis],as_index=False).mean())\n",
    "    print(files_avg_values)\n",
    "    print(added_avg_values)\n",
    "    print(removed_avg_values)\n",
    "    \n",
    "        \n",
    "    # Setup data in format for grouped bar chart\n",
    "    data = {\n",
    "            'years' : x_groups,\n",
    "            'Lines Added'   : added_avg_values,\n",
    "            'Lines Removed' : removed_avg_values,\n",
    "            'Files Changed' : files_avg_values\n",
    "        }\n",
    "\n",
    "    x = [ (year, pr_state) for year in x_groups for pr_state in groups ]\n",
    "    line_counts = sum(zip(data['Lines Added'], data['Lines Removed'], [0]*len(x_groups)), ())\n",
    "    file_counts = sum(zip([0]*len(x_groups),[0]*len(x_groups),data['Files Changed']), ())\n",
    "    print(line_counts)\n",
    "    print(file_counts)\n",
    "    \n",
    "    source = ColumnDataSource(data=dict(x=x, line_counts=line_counts, file_counts=file_counts))\n",
    "\n",
    "    if y_max1:\n",
    "        p = figure(x_range=FactorRange(*x), plot_height=450, plot_width=700, title=title.format(description), y_range=(0,y_max1))\n",
    "    else:\n",
    "        p = figure(x_range=FactorRange(*x), plot_height=450, plot_width=700, title=title.format(description))\n",
    "                \n",
    "    # Setting the second y axis range name and range\n",
    "    p.extra_y_ranges = {\"file_counts\": Range1d(start=0, end=y_max2)}\n",
    "    \n",
    "    # Adding the second axis to the plot.  \n",
    "    p.add_layout(LinearAxis(y_range_name=\"file_counts\"), 'right')\n",
    "    \n",
    "    # Data label for line counts\n",
    "    labels = LabelSet(x='x', y='line_counts', text='line_counts',y_offset=8,# x_offset=34,\n",
    "              text_font_size=\"10pt\", text_color=\"black\",\n",
    "              source=source, text_align='center')\n",
    "    p.add_layout(labels)\n",
    "\n",
    "    # Vertical bar glyph for line counts\n",
    "    p.vbar(x='x', top='line_counts', width=0.9, source=source, line_color=\"white\",\n",
    "           fill_color=factor_cmap('x', palette=colors, factors=groups, start=1, end=2))\n",
    "    \n",
    "    # Data label for file counts\n",
    "    labels = LabelSet(x='x', y='file_counts', text='file_counts', y_offset=0, #x_offset=34,\n",
    "              text_font_size=\"10pt\", text_color=\"black\",\n",
    "              source=source, text_align='center', y_range_name=\"file_counts\")\n",
    "    p.add_layout(labels)\n",
    "    \n",
    "    # Vertical bar glyph for file counts\n",
    "    p.vbar(x='x', top='file_counts', width=0.9, source=source, line_color=\"white\",\n",
    "           fill_color=factor_cmap('x', palette=colors, factors=groups, start=1, end=2), y_range_name=\"file_counts\")\n",
    "\n",
    "    p.left[0].formatter.use_scientific = False\n",
    "    p.y_range.start = 0\n",
    "    p.x_range.range_padding = 0.1\n",
    "    p.xaxis.major_label_orientation = 1\n",
    "    p.xgrid.grid_line_color = None\n",
    "    \n",
    "    p.yaxis.axis_label = 'Average Commits / Pull Request'\n",
    "    p.xaxis.axis_label = 'Year Closed'\n",
    "    \n",
    "    p.title.align = \"center\"\n",
    "    p.title.text_font_size = \"16px\"\n",
    "    \n",
    "    p.xaxis.axis_label_text_font_size = \"16px\"\n",
    "    p.xaxis.major_label_text_font_size = \"16px\"\n",
    "    \n",
    "    p.yaxis.axis_label_text_font_size = \"16px\"\n",
    "    p.yaxis.major_label_text_font_size = \"16px\"\n",
    "    \n",
    "    show(p)\n",
    "    \n",
    "    if save_files:\n",
    "        export_png(p, filename=\"./images/v_grouped_bar/v_grouped_bar__{}_PRs__yaxis_{}__repo_{}.png\".format(description, y_axis, repo_dict[repo_id]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" THIS VIZ IS NOT READY YET , BUT UNCOMMENT LINE BELOW IF YOU WANT TO SEE\"\"\"\n",
    "# vertical_grouped_bar_line_counts(pr_all, description='All', title=\"Average Size Metrics Per Year for {} Merged Pull Requests in Master\", save_file=False, y_max1=580000, y_max2=1100)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def horizontal_stacked_bar(input_df, repo_id, group_by='merged_flag', x_axis='comment_count', description=\"All Closed\", y_axis='closed_year', title=\"Mean Comments for {} Pull Requests\"):\n",
    "    \n",
    "    if type(repo_id) == type(repo_list):\n",
    "        repo_ids = repo_id\n",
    "    else:\n",
    "        repo_ids = [repo_id]\n",
    "    \n",
    "    for repo_id in repo_ids:\n",
    "    \n",
    "        driver_df = input_df.copy()\n",
    "\n",
    "        driver_df = driver_df.loc[driver_df['repo_id'] == repo_id]\n",
    "\n",
    "        output_notebook()\n",
    "\n",
    "        try:\n",
    "            y_groups = sorted(list(driver_df[y_axis].unique()))\n",
    "        except:\n",
    "            y_groups = [repo_id]\n",
    "\n",
    "        groups = driver_df[group_by].unique()\n",
    "        try:\n",
    "            colors = mpl['Plasma'][len(groups)]\n",
    "        except:\n",
    "            colors = [mpl['Plasma'][3][0]] + [mpl['Plasma'][3][1]]\n",
    "\n",
    "        len_not_merged = len(driver_df.loc[driver_df['merged_flag'] == 'Not Merged / Rejected'])\n",
    "        len_merged = len(driver_df.loc[driver_df['merged_flag'] == 'Merged / Accepted'])\n",
    "\n",
    "        title_beginning = '{}: '.format(repo_dict[repo_id]) \n",
    "        plot_width = 650\n",
    "        p = figure(y_range=y_groups, plot_height=450, plot_width=plot_width, # y_range=y_groups,#(pr_all[y_axis].min(),pr_all[y_axis].max()) #y_axis_type=\"datetime\",\n",
    "                   title='{} {}'.format(title_beginning, title.format(description)), toolbar_location=None)\n",
    "\n",
    "        possible_maximums= []\n",
    "        for y_value in y_groups:\n",
    "\n",
    "            y_merged_data = driver_df.loc[(driver_df[y_axis] == y_value) & (driver_df['merged_flag'] == 'Merged / Accepted')]\n",
    "            y_not_merged_data = driver_df.loc[(driver_df[y_axis] == y_value) & (driver_df['merged_flag'] == 'Not Merged / Rejected')]\n",
    "\n",
    "            if len(y_merged_data) > 0:\n",
    "                y_merged_data[x_axis + '_mean'] = y_merged_data[x_axis].mean().round(1)\n",
    "            else:\n",
    "                y_merged_data[x_axis + '_mean'] = 0.00\n",
    "\n",
    "            if len(y_not_merged_data) > 0:\n",
    "                y_not_merged_data[x_axis + '_mean'] = y_not_merged_data[x_axis].mean().round(1)\n",
    "            else:\n",
    "                y_not_merged_data[x_axis + '_mean'] = 0\n",
    "\n",
    "            not_merged_source = ColumnDataSource(y_not_merged_data)\n",
    "            merged_source = ColumnDataSource(y_merged_data)\n",
    "\n",
    "            possible_maximums.append(max(y_not_merged_data[x_axis + '_mean']))\n",
    "            possible_maximums.append(max(y_merged_data[x_axis + '_mean']))\n",
    "\n",
    "            # mean comment count for merged\n",
    "            merged_comment_count_glyph = p.hbar(y=dodge(y_axis, -0.1, range=p.y_range), left=0, right=x_axis + '_mean', height=0.04*len(driver_df[y_axis].unique()), \n",
    "                                         source=merged_source, fill_color=\"black\")#,legend_label=\"Mean Days to Close\",\n",
    "            # Data label \n",
    "            labels = LabelSet(x=x_axis + '_mean', y=dodge(y_axis, -0.1, range=p.y_range), text=x_axis + '_mean', y_offset=-8, x_offset=34,\n",
    "                      text_font_size=\"12pt\", text_color=\"black\",\n",
    "                      source=merged_source, text_align='center')\n",
    "            p.add_layout(labels)\n",
    "            # mean comment count For nonmerged\n",
    "            not_merged_comment_count_glyph = p.hbar(y=dodge(y_axis, 0.1, range=p.y_range), left=0, right=x_axis + '_mean', \n",
    "                                         height=0.04*len(driver_df[y_axis].unique()), source=not_merged_source, fill_color=\"#e84d60\")#legend_label=\"Mean Days to Close\",\n",
    "            # Data label \n",
    "            labels = LabelSet(x=x_axis + '_mean', y=dodge(y_axis, 0.1, range=p.y_range), text=x_axis + '_mean', y_offset=-8, x_offset=34,\n",
    "                      text_font_size=\"12pt\", text_color=\"#e84d60\",\n",
    "                      source=not_merged_source, text_align='center')\n",
    "            p.add_layout(labels)\n",
    "\n",
    "    #         p.y_range.range_padding = 0.1\n",
    "        p.ygrid.grid_line_color = None\n",
    "        p.legend.location = \"bottom_right\"\n",
    "        p.axis.minor_tick_line_color = None\n",
    "        p.outline_line_color = None\n",
    "        p.xaxis.axis_label = 'Average Comments / Pull Request'\n",
    "        p.yaxis.axis_label = 'Repository' if y_axis == 'repo_name' else 'Year Closed' if y_axis == 'closed_year' else ''\n",
    "\n",
    "        legend = Legend(\n",
    "                items=[\n",
    "                    (\"Merged Pull Request Mean Comment Count\", [merged_comment_count_glyph]),\n",
    "                    (\"Rejected Pull Request Mean Comment Count\", [not_merged_comment_count_glyph])\n",
    "                ],\n",
    "\n",
    "                location='center', \n",
    "                orientation='vertical',\n",
    "                border_line_color=\"black\"\n",
    "            )\n",
    "        p.add_layout(legend, \"below\")\n",
    "\n",
    "        p.title.text_font_size = \"16px\"\n",
    "        p.title.align = \"center\"\n",
    "\n",
    "        p.xaxis.axis_label_text_font_size = \"16px\"\n",
    "        p.xaxis.major_label_text_font_size = \"16px\"\n",
    "\n",
    "        p.yaxis.axis_label_text_font_size = \"16px\"\n",
    "        p.yaxis.major_label_text_font_size = \"16px\"\n",
    "\n",
    "        p.x_range = Range1d(0, max(possible_maximums)*1.15)\n",
    "        \n",
    "        plot = p\n",
    "        \n",
    "        p = figure(width = plot_width, height=200, margin = (0, 0, 0, 0))\n",
    "        caption = \"This graph shows the average number of comments per merged or not merged pull request.\"\n",
    "        p.add_layout(Label(\n",
    "        x = 0, # Change to shift caption left or right\n",
    "        y = 160, \n",
    "        x_units = 'screen',\n",
    "        y_units = 'screen',\n",
    "        text='{}'.format(caption),\n",
    "        text_font = 'times', # Use same font as paper\n",
    "        text_font_size = '15pt',\n",
    "        render_mode='css'\n",
    "        ))\n",
    "        p.outline_line_color = None\n",
    "\n",
    "        caption_plot = p\n",
    "\n",
    "        grid = gridplot([[plot], [caption_plot]])\n",
    "\n",
    "        show(grid)\n",
    "\n",
    "        #show(p, plot_width=1200, plot_height=300*len(y_groups) + 300)\n",
    "\n",
    "        if save_files:\n",
    "            repo_extension = 'All' if not repo_name else repo_name\n",
    "            export_png(grid, filename=\"./images/h_stacked_bar_mean_comments_merged_status/mean_comments_merged_status__{}_PRs__yaxis_{}__repo_{}.png\".format(description, y_axis, repo_dict[repo_id]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#horizontal_stacked_bar(pr_closed, repo_id=repo_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merged_ratio_vertical_grouped_bar(data_dict, repo_id, x_axis='closed_year', description=\"All Closed\", title=\"Count of {} Pull Requests by Merged Status\"):\n",
    "    \n",
    "    if type(repo_id) == type(repo_list):\n",
    "        repo_ids = repo_id\n",
    "    else:\n",
    "        repo_ids = [repo_id]\n",
    "    \n",
    "    for repo_id in repo_ids:\n",
    "        \n",
    "        output_notebook()\n",
    "\n",
    "        colors = mpl['Plasma'][6]\n",
    "\n",
    "        #if repo_name == 'mbed-os':\n",
    "            #colors = colors[::-1]\n",
    "\n",
    "        for data_desc, input_df in data_dict.items():\n",
    "            x_groups = sorted(list(input_df[x_axis].astype(str).unique()))\n",
    "            break\n",
    "\n",
    "        plot_width = 315 * len(x_groups)\n",
    "        title_beginning = repo_dict[repo_id] \n",
    "        p = figure(x_range=x_groups, plot_height=350, plot_width=plot_width,  \n",
    "                   title='{}: {}'.format(title_beginning, title.format(description)), toolbar_location=None)\n",
    "\n",
    "        dodge_amount = 0.12\n",
    "        color_index = 0\n",
    "        x_offset = 50\n",
    "\n",
    "        all_totals = []\n",
    "        for data_desc, input_df in data_dict.items():\n",
    "            driver_df = input_df.copy()\n",
    "\n",
    "            driver_df[x_axis] = driver_df[x_axis].astype(str)\n",
    "\n",
    "            groups = sorted(list(driver_df['merged_flag'].unique()))\n",
    "\n",
    "            driver_df = driver_df.loc[driver_df['repo_id'] == repo_id]\n",
    "\n",
    "            len_merged = []\n",
    "            zeros = []\n",
    "            len_not_merged = []\n",
    "            totals = []\n",
    "\n",
    "            for x_group in x_groups:\n",
    "\n",
    "                len_merged_entry = len(driver_df.loc[(driver_df['merged_flag'] == 'Merged / Accepted') & (driver_df[x_axis] == x_group)])\n",
    "                totals += [len(driver_df.loc[(driver_df['merged_flag'] == 'Not Merged / Rejected') & (driver_df[x_axis] == x_group)]) + len_merged_entry]\n",
    "                len_not_merged += [len(driver_df.loc[(driver_df['merged_flag'] == 'Not Merged / Rejected') & (driver_df[x_axis] == x_group)])]\n",
    "                len_merged += [len_merged_entry]\n",
    "                zeros.append(0)\n",
    "\n",
    "            data = {'X': x_groups}\n",
    "            for group in groups:\n",
    "                data[group] = []\n",
    "                for x_group in x_groups:\n",
    "                    data[group] += [len(driver_df.loc[(driver_df['merged_flag'] == group) & (driver_df[x_axis] == x_group)])]\n",
    "\n",
    "            data['len_merged'] = len_merged\n",
    "            data['len_not_merged'] = len_not_merged\n",
    "            data['totals'] = totals\n",
    "            data['zeros'] = zeros\n",
    "\n",
    "            if data_desc == \"All\":\n",
    "                all_totals = totals\n",
    "\n",
    "            source = ColumnDataSource(data)\n",
    "\n",
    "            stacked_bar = p.vbar_stack(groups, x=dodge('X', dodge_amount, range=p.x_range), width=0.2, source=source, color=colors[1:3], legend_label=[f\"{data_desc} \" + \"%s\" % x for x in groups])\n",
    "            # Data label for merged\n",
    "\n",
    "            p.add_layout(\n",
    "                LabelSet(x=dodge('X', dodge_amount, range=p.x_range), y='zeros', text='len_merged', y_offset=2, x_offset=x_offset,\n",
    "                      text_font_size=\"12pt\", text_color=colors[1:3][0],\n",
    "                      source=source, text_align='center')\n",
    "            )\n",
    "            if min(data['totals']) < 400:\n",
    "                y_offset = 15\n",
    "            else:\n",
    "                y_offset = 0\n",
    "            # Data label for not merged\n",
    "            p.add_layout(\n",
    "                LabelSet(x=dodge('X', dodge_amount, range=p.x_range), y='totals', text='len_not_merged', y_offset=y_offset, x_offset=x_offset,\n",
    "                      text_font_size=\"12pt\", text_color=colors[1:3][1],\n",
    "                      source=source, text_align='center')\n",
    "            )\n",
    "            # Data label for total\n",
    "            p.add_layout(\n",
    "                LabelSet(x=dodge('X', dodge_amount, range=p.x_range), y='totals', text='totals', y_offset=0, x_offset=0,\n",
    "                      text_font_size=\"12pt\", text_color='black',\n",
    "                      source=source, text_align='center')\n",
    "            )\n",
    "            dodge_amount *= -1\n",
    "            colors = colors[::-1]\n",
    "            x_offset *= -1\n",
    "\n",
    "        p.y_range = Range1d(0,  max(all_totals)*1.4)\n",
    "\n",
    "        p.xgrid.grid_line_color = None\n",
    "        p.legend.location = \"top_center\"\n",
    "        p.legend.orientation=\"horizontal\"\n",
    "        p.axis.minor_tick_line_color = None\n",
    "        p.outline_line_color = None\n",
    "        p.yaxis.axis_label = 'Count of Pull Requests'\n",
    "        p.xaxis.axis_label = 'Repository' if x_axis == 'repo_name' else 'Year Closed' if x_axis == 'closed_year' else ''\n",
    "\n",
    "        p.title.align = \"center\"\n",
    "        p.title.text_font_size = \"16px\"\n",
    "\n",
    "        p.xaxis.axis_label_text_font_size = \"16px\"\n",
    "        p.xaxis.major_label_text_font_size = \"16px\"\n",
    "\n",
    "        p.yaxis.axis_label_text_font_size = \"16px\"\n",
    "        p.yaxis.major_label_text_font_size = \"16px\"\n",
    "\n",
    "        p.outline_line_color = None\n",
    "        \n",
    "        plot = p\n",
    "        \n",
    "        p = figure(width = plot_width, height=200, margin = (0, 0, 0, 0))\n",
    "        caption = \"This graph shows the number of closed pull requests per year in four different categories. These four categories are All Merged, All Not Merged, Slowest 20% Merged, and Slowest 20% Not Merged.\"\n",
    "        p.add_layout(Label(\n",
    "        x = 0, # Change to shift caption left or right\n",
    "        y = 160, \n",
    "        x_units = 'screen',\n",
    "        y_units = 'screen',\n",
    "        text='{}'.format(caption),\n",
    "        text_font = 'times', # Use same font as paper\n",
    "        text_font_size = '15pt',\n",
    "        render_mode='css'\n",
    "        ))\n",
    "        p.outline_line_color = None\n",
    "\n",
    "        caption_plot = p\n",
    "\n",
    "        grid = gridplot([[plot], [caption_plot]])\n",
    "\n",
    "        show(grid)\n",
    "        \n",
    "        if save_files:\n",
    "            repo_extension = 'All' if not repo_id else repo_id\n",
    "            export_png(grid, filename=\"./images/v_stacked_bar_merged_status_count/stacked_bar_merged_status_count__{}_PRs__xaxis_{}__repo_{}.png\".format(description, x_axis, repo_dict[repo_id]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for repo_name in pr_all['repo_name'].unique():\n",
    "#erged_ratio_vertical_grouped_bar({'All':pr_closed,'Slowest 20%':pr_slow20_not_merged.append(pr_slow20_merged,ignore_index=True)}, repo_id = repo_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_mean_response_times(input_df, repo_id, time_unit='days', x_max=95,  y_axis='closed_year', description=\"All Closed\", legend_position=(410, 10)):\n",
    "    \n",
    "    if type(repo_id) == type(repo_list):\n",
    "        repo_ids = repo_id\n",
    "    else:\n",
    "        repo_ids = [repo_id]\n",
    "    \n",
    "    for repo_id in repo_ids:\n",
    "\n",
    "        output_notebook() # let bokeh show plot in jupyter cell output\n",
    "\n",
    "        driver_df = input_df.copy()[['repo_name', 'repo_id', 'merged_flag', y_axis, time_unit + '_to_first_response', time_unit + '_to_last_response', \n",
    "                                     time_unit + '_to_close']] # deep copy input data so we do not alter the external dataframe\n",
    "\n",
    "        # filter by repo_id param\n",
    "        driver_df = driver_df.loc[driver_df['repo_id'] == repo_id]\n",
    "\n",
    "        title_beginning = '{}: '.format(repo_dict[repo_id])\n",
    "        plot_width = 950\n",
    "        p = figure(toolbar_location=None, y_range=sorted(driver_df[y_axis].unique()), plot_width=plot_width, \n",
    "                   plot_height=450,#75*len(driver_df[y_axis].unique()),\n",
    "                   title=\"{}Mean Response Times for Pull Requests {}\".format(title_beginning, description))\n",
    "\n",
    "        first_response_glyphs = []\n",
    "        last_response_glyphs = []\n",
    "        merged_days_to_close_glyphs = []\n",
    "        not_merged_days_to_close_glyphs = []\n",
    "\n",
    "        possible_maximums = []\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for y_value in driver_df[y_axis].unique():\n",
    "\n",
    "            y_merged_data = driver_df.loc[(driver_df[y_axis] == y_value) & (driver_df['merged_flag'] == 'Merged / Accepted')]\n",
    "            y_not_merged_data = driver_df.loc[(driver_df[y_axis] == y_value) & (driver_df['merged_flag'] == 'Not Merged / Rejected')]\n",
    "\n",
    "            y_merged_data[time_unit + '_to_first_response_mean'] = y_merged_data[time_unit + '_to_first_response'].mean().round(1) if len(y_merged_data) > 0 else 0.00\n",
    "            y_merged_data[time_unit + '_to_last_response_mean'] = y_merged_data[time_unit + '_to_last_response'].mean().round(1) if len(y_merged_data) > 0 else 0.00\n",
    "            y_merged_data[time_unit + '_to_close_mean'] = y_merged_data[time_unit + '_to_close'].mean().round(1) if len(y_merged_data) > 0 else 0.00\n",
    "\n",
    "            y_not_merged_data[time_unit + '_to_first_response_mean'] = y_not_merged_data[time_unit + '_to_first_response'].mean().round(1) if len(y_not_merged_data) > 0 else 0.00\n",
    "            y_not_merged_data[time_unit + '_to_last_response_mean'] = y_not_merged_data[time_unit + '_to_last_response'].mean().round(1) if len(y_not_merged_data) > 0 else 0.00\n",
    "            y_not_merged_data[time_unit + '_to_close_mean'] = y_not_merged_data[time_unit + '_to_close'].mean().round(1) if len(y_not_merged_data) > 0 else 0.00\n",
    "\n",
    "            possible_maximums.append(max(y_merged_data[time_unit + '_to_close_mean']))\n",
    "            possible_maximums.append(max(y_not_merged_data[time_unit + '_to_close_mean']))\n",
    "            \n",
    "            maximum = max(possible_maximums)*1.15\n",
    "            ideal_difference = maximum*0.064\n",
    "            \n",
    "        for y_value in driver_df[y_axis].unique():\n",
    "\n",
    "            y_merged_data = driver_df.loc[(driver_df[y_axis] == y_value) & (driver_df['merged_flag'] == 'Merged / Accepted')]\n",
    "            y_not_merged_data = driver_df.loc[(driver_df[y_axis] == y_value) & (driver_df['merged_flag'] == 'Not Merged / Rejected')]\n",
    "\n",
    "            y_merged_data[time_unit + '_to_first_response_mean'] = y_merged_data[time_unit + '_to_first_response'].mean().round(1) if len(y_merged_data) > 0 else 0.00\n",
    "            y_merged_data[time_unit + '_to_last_response_mean'] = y_merged_data[time_unit + '_to_last_response'].mean().round(1) if len(y_merged_data) > 0 else 0.00\n",
    "            y_merged_data[time_unit + '_to_close_mean'] = y_merged_data[time_unit + '_to_close'].mean().round(1) if len(y_merged_data) > 0 else 0.00\n",
    "\n",
    "            y_not_merged_data[time_unit + '_to_first_response_mean'] = y_not_merged_data[time_unit + '_to_first_response'].mean().round(1) if len(y_not_merged_data) > 0 else 0.00\n",
    "            y_not_merged_data[time_unit + '_to_last_response_mean'] = y_not_merged_data[time_unit + '_to_last_response'].mean().round(1) if len(y_not_merged_data) > 0 else 0.00\n",
    "            y_not_merged_data[time_unit + '_to_close_mean'] = y_not_merged_data[time_unit + '_to_close'].mean().round(1) if len(y_not_merged_data) > 0 else 0.00\n",
    "\n",
    "            not_merged_source = ColumnDataSource(y_not_merged_data)\n",
    "            merged_source = ColumnDataSource(y_merged_data)\n",
    "\n",
    "            # mean PR length for merged\n",
    "            merged_days_to_close_glyph = p.hbar(y=dodge(y_axis, -0.1, range=p.y_range), left=0, right=time_unit + '_to_close_mean', height=0.04*len(driver_df[y_axis].unique()), \n",
    "                                         source=merged_source, fill_color=\"black\")#,legend_label=\"Mean Days to Close\",\n",
    "            merged_days_to_close_glyphs.append(merged_days_to_close_glyph)\n",
    "            # Data label \n",
    "            labels = LabelSet(x=time_unit + '_to_close_mean', y=dodge(y_axis, -0.1, range=p.y_range), text=time_unit + '_to_close_mean', y_offset=-8, x_offset=34, #34\n",
    "                      text_font_size=\"12pt\", text_color=\"black\",\n",
    "                      source=merged_source, text_align='center')\n",
    "            p.add_layout(labels)\n",
    "\n",
    "\n",
    "            # mean PR length For nonmerged\n",
    "            not_merged_days_to_close_glyph = p.hbar(y=dodge(y_axis, 0.1, range=p.y_range), left=0, right=time_unit + '_to_close_mean', \n",
    "                                         height=0.04*len(driver_df[y_axis].unique()), source=not_merged_source, fill_color=\"#e84d60\")#legend_label=\"Mean Days to Close\",\n",
    "            not_merged_days_to_close_glyphs.append(not_merged_days_to_close_glyph)\n",
    "            # Data label \n",
    "            labels = LabelSet(x=time_unit + '_to_close_mean', y=dodge(y_axis, 0.1, range=p.y_range), text=time_unit + '_to_close_mean', y_offset=-8, x_offset=44,\n",
    "                      text_font_size=\"12pt\", text_color=\"#e84d60\",\n",
    "                      source=not_merged_source, text_align='center')\n",
    "            p.add_layout(labels)\n",
    "\n",
    "            \n",
    "            #if the difference between two values is less than 6.4 percent move the second one to the right 30 pixels\n",
    "            if (max(y_merged_data[time_unit + '_to_last_response_mean']) - max(y_merged_data[time_unit + '_to_first_response_mean'])) < ideal_difference:\n",
    "                merged_x_offset = 30\n",
    "            else:\n",
    "                merged_x_offset = 0\n",
    "                \n",
    "            #if the difference between two values is less than 6.4 percent move the second one to the right 30 pixels\n",
    "            if (max(y_not_merged_data[time_unit + '_to_last_response_mean']) - max(y_not_merged_data[time_unit + '_to_first_response_mean'])) < ideal_difference:\n",
    "                not_merged_x_offset = 30\n",
    "            else:\n",
    "                not_merged_x_offset = 0\n",
    "                \n",
    "            #if there is only one bar set the y_offsets so the labels will not overlap the bars\n",
    "            if len(driver_df[y_axis].unique()) == 1:\n",
    "                merged_y_offset = -65\n",
    "                not_merged_y_offset = 45\n",
    "            else:\n",
    "                merged_y_offset = -45\n",
    "                not_merged_y_offset = 25\n",
    "            \n",
    "            \n",
    "            # mean time to first response\n",
    "            glyph = Rect(x=time_unit + '_to_first_response_mean', y=dodge(y_axis, -0.1, range=p.y_range), width=x_max/100, height=0.08*len(driver_df[y_axis].unique()), fill_color=colors[0])\n",
    "            first_response_glyph = p.add_glyph(merged_source, glyph)\n",
    "            first_response_glyphs.append(first_response_glyph)\n",
    "            # Data label \n",
    "            labels = LabelSet(x=time_unit + '_to_first_response_mean', y=dodge(y_axis, 0, range=p.y_range),text=time_unit + '_to_first_response_mean',x_offset = 0, y_offset=merged_y_offset,#-60,\n",
    "                      text_font_size=\"12pt\", text_color=colors[0],\n",
    "                      source=merged_source, text_align='center')\n",
    "            p.add_layout(labels)\n",
    "\n",
    "            #for nonmerged\n",
    "            glyph = Rect(x=time_unit + '_to_first_response_mean', y=dodge(y_axis, 0.1, range=p.y_range), width=x_max/100, height=0.08*len(driver_df[y_axis].unique()), fill_color=colors[0])\n",
    "            first_response_glyph = p.add_glyph(not_merged_source, glyph)\n",
    "            first_response_glyphs.append(first_response_glyph)\n",
    "            # Data label \n",
    "            labels = LabelSet(x=time_unit + '_to_first_response_mean', y=dodge(y_axis, 0, range=p.y_range),text=time_unit + '_to_first_response_mean',x_offset = 0, y_offset=not_merged_y_offset,#40,\n",
    "                              text_font_size=\"12pt\", text_color=colors[0],\n",
    "                      source=not_merged_source, text_align='center')\n",
    "            p.add_layout(labels)\n",
    "\n",
    "\n",
    "            # mean time to last response\n",
    "            glyph = Rect(x=time_unit + '_to_last_response_mean', y=dodge(y_axis, -0.1, range=p.y_range), width=x_max/100, height=0.08*len(driver_df[y_axis].unique()), fill_color=colors[1])\n",
    "            last_response_glyph = p.add_glyph(merged_source, glyph)\n",
    "            last_response_glyphs.append(last_response_glyph)\n",
    "            # Data label \n",
    "            labels = LabelSet(x=time_unit + '_to_last_response_mean', y=dodge(y_axis, 0, range=p.y_range), text=time_unit + '_to_last_response_mean', x_offset=merged_x_offset, y_offset=merged_y_offset,#-60,\n",
    "                      text_font_size=\"12pt\", text_color=colors[1],\n",
    "                      source=merged_source, text_align='center')\n",
    "            p.add_layout(labels)\n",
    "            \n",
    "\n",
    "            #for nonmerged\n",
    "            glyph = Rect(x=time_unit + '_to_last_response_mean', y=dodge(y_axis, 0.1, range=p.y_range), width=x_max/100, height=0.08*len(driver_df[y_axis].unique()), fill_color=colors[1])\n",
    "            last_response_glyph = p.add_glyph(not_merged_source, glyph)\n",
    "            last_response_glyphs.append(last_response_glyph)\n",
    "            # Data label \n",
    "            labels = LabelSet(x=time_unit + '_to_last_response_mean', y=dodge(y_axis, 0, range=p.y_range), text=time_unit + '_to_last_response_mean', x_offset = not_merged_x_offset, y_offset=not_merged_y_offset,#40,\n",
    "                      text_font_size=\"12pt\", text_color=colors[1],\n",
    "                      source=not_merged_source, text_align='center')\n",
    "            p.add_layout(labels)\n",
    "\n",
    "        p.title.align = \"center\"\n",
    "        p.title.text_font_size = \"16px\"\n",
    "\n",
    "        p.xaxis.axis_label = \"Days to Close\"\n",
    "        p.xaxis.axis_label_text_font_size = \"16px\"\n",
    "        p.xaxis.major_label_text_font_size = \"16px\"\n",
    "        \n",
    "        #adjust the starting point and ending point based on the maximum of maximum of the graph\n",
    "        p.x_range = Range1d(maximum/30 * -1, maximum*1.15)\n",
    "\n",
    "        p.yaxis.axis_label = \"Repository\" if y_axis == 'repo_name' else 'Year Closed' if y_axis == 'closed_year' else ''\n",
    "        p.yaxis.axis_label_text_font_size = \"16px\"\n",
    "        p.yaxis.major_label_text_font_size = \"16px\"\n",
    "        p.ygrid.grid_line_color = None\n",
    "        p.y_range.range_padding = 0.15\n",
    "\n",
    "        p.outline_line_color = None\n",
    "        p.toolbar.logo = None\n",
    "        p.toolbar_location = None\n",
    "\n",
    "        def add_legend(location, orientation, side):\n",
    "            legend = Legend(\n",
    "                items=[\n",
    "                    (\"Mean Days to First Response\", first_response_glyphs),\n",
    "                    (\"Mean Days to Last Response\", last_response_glyphs),\n",
    "                    (\"Merged Mean Days to Close\", merged_days_to_close_glyphs),\n",
    "                    (\"Not Merged Mean Days to Close\", not_merged_days_to_close_glyphs)\n",
    "                ],\n",
    "\n",
    "                location=location, \n",
    "                orientation=orientation,\n",
    "                border_line_color=\"black\"\n",
    "        #         title='Example Title'\n",
    "            )\n",
    "            p.add_layout(legend, side)\n",
    "\n",
    "    #     add_legend((150, 50), \"horizontal\", \"center\")\n",
    "        add_legend(legend_position, \"vertical\", \"right\")\n",
    "        \n",
    "        plot = p\n",
    "        \n",
    "        p = figure(width = plot_width, height = 200, margin = (0, 0, 0, 0))\n",
    "        caption = \"Caption Here\"\n",
    "        p.add_layout(Label(\n",
    "        x = 0, # Change to shift caption left or right\n",
    "        y = 160, \n",
    "        x_units = 'screen',\n",
    "        y_units = 'screen',\n",
    "        text='{}'.format(caption),\n",
    "        text_font = 'times', # Use same font as paper\n",
    "        text_font_size = '15pt',\n",
    "        render_mode='css'\n",
    "        ))\n",
    "        p.outline_line_color = None\n",
    "\n",
    "        caption_plot = p\n",
    "\n",
    "        grid = gridplot([[plot], [caption_plot]])\n",
    "\n",
    "        show(grid)\n",
    "\n",
    "        if save_files:\n",
    "            export_png(grid, filename=\"./images/hbar_response_times/mean_response_times__{}_PRs__yaxis_{}__repo_{}.png\".format(description, y_axis, repo_dict[repo_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for repo_name in pr_closed['repo_name'].unique():\n",
    "#visualize_mean_response_times(pr_closed, repo_id=repo_list, legend_position='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_mean_time_between_responses(data_dict, repo_id, time_unit='Days', x_axis='closed_yearmonth', description=\"All Closed\", line_group='merged_flag', y_axis='average_days_between_responses', num_outliers_repo_map={}):\n",
    "    \n",
    "    if type(repo_id) == type(repo_list):\n",
    "        repo_ids = repo_id\n",
    "    else:\n",
    "        repo_ids = [repo_id]\n",
    "    \n",
    "    for repo_id in repo_ids:\n",
    "    \n",
    "        output_notebook()\n",
    "        plot_width = 950\n",
    "        p1 = figure(x_axis_type=\"datetime\", title=\"{}: Mean {} Between Comments by Month Closed for {} Pull Requests\".format(repo_dict[repo_id], time_unit, description), plot_width=plot_width, x_range=(pr_all[x_axis].min(),pr_all[x_axis].max()), plot_height=500, toolbar_location=None)\n",
    "        colors = Category20[10][6:]\n",
    "        color_index = 0\n",
    "\n",
    "        glyphs = []\n",
    "\n",
    "        possible_maximums = []\n",
    "        for data_desc, input_df in data_dict.items():\n",
    "\n",
    "            driver_df = input_df.copy()\n",
    "\n",
    "            driver_df = remove_outliers(driver_df, y_axis, num_outliers_repo_map)\n",
    "\n",
    "            driver_df = driver_df.loc[driver_df['repo_id'] == repo_id]\n",
    "            index = 0\n",
    "\n",
    "            driver_df_mean = driver_df.groupby(['repo_id', line_group, x_axis],as_index=False).mean()\n",
    "\n",
    "            title_ending = ''\n",
    "            if repo_id:\n",
    "                title_ending += ' for Repo: {}'.format(repo_id)\n",
    "\n",
    "            for group_num, line_group_value in enumerate(driver_df[line_group].unique(), color_index):\n",
    "                glyphs.append(p1.line(driver_df_mean.loc[driver_df_mean[line_group] == line_group_value][x_axis], driver_df_mean.loc[driver_df_mean[line_group] == line_group_value][y_axis], color=colors[group_num], line_width = 3))\n",
    "                color_index += 1\n",
    "                possible_maximums.append(max(driver_df_mean.loc[driver_df_mean[line_group] == line_group_value][y_axis].dropna()))\n",
    "        for repo, num_outliers in num_outliers_repo_map.items():\n",
    "            if repo_name == repo:\n",
    "                p1.add_layout(Title(text=\"** {} outliers for {} were removed\".format(num_outliers, repo), align=\"center\"), \"below\")\n",
    "\n",
    "        p1.grid.grid_line_alpha = 0.3\n",
    "        p1.xaxis.axis_label = 'Month Closed'\n",
    "        p1.xaxis.ticker.desired_num_ticks = 15\n",
    "        p1.yaxis.axis_label = 'Mean {} Between Responses'.format(time_unit)\n",
    "        p1.legend.location = \"top_left\"\n",
    "\n",
    "        legend = Legend(\n",
    "            items=[\n",
    "                (\"All Not Merged / Rejected\", [glyphs[0]]),\n",
    "                (\"All Merged / Accepted\", [glyphs[1]]),\n",
    "                (\"Slowest 20% Not Merged / Rejected\", [glyphs[2]]),\n",
    "                (\"Slowest 20% Merged / Accepted\", [glyphs[3]])\n",
    "            ],\n",
    "\n",
    "            location='center_right', \n",
    "            orientation='vertical',\n",
    "            border_line_color=\"black\"\n",
    "        )\n",
    "\n",
    "        p1.add_layout(legend, 'right')\n",
    "\n",
    "        p1.title.text_font_size = \"16px\"\n",
    "\n",
    "        p1.xaxis.axis_label_text_font_size = \"16px\"\n",
    "        p1.xaxis.major_label_text_font_size = \"16px\"\n",
    "\n",
    "        p1.yaxis.axis_label_text_font_size = \"16px\"\n",
    "        p1.yaxis.major_label_text_font_size = \"16px\"\n",
    "        p1.xaxis.major_label_orientation = 45.0\n",
    "        \n",
    "        p1.y_range = Range1d(0,  max(possible_maximums)*1.15)\n",
    "        \n",
    "        plot = p1\n",
    "        \n",
    "        p = figure(width = plot_width, height=200, margin = (0, 0, 0, 0))\n",
    "        caption = \"This graph shows the average number of days between comments for all closed pull requests per month in four categories. These four categories are All Merged, All Not Merged, Slowest 20% Merged, and Slowest 20% Not Merged.\"\n",
    "        p.add_layout(Label(\n",
    "        x = 0, # Change to shift caption left or right\n",
    "        y = 160, \n",
    "        x_units = 'screen',\n",
    "        y_units = 'screen',\n",
    "        text='{}'.format(caption),\n",
    "        text_font = 'times', # Use same font as paper\n",
    "        text_font_size = '15pt',\n",
    "        render_mode='css'\n",
    "        ))\n",
    "        p.outline_line_color = None\n",
    "\n",
    "        caption_plot = p\n",
    "\n",
    "        grid = gridplot([[plot], [caption_plot]])\n",
    "\n",
    "        show(grid)\n",
    "\n",
    "        if save_files:\n",
    "            repo_extension = 'All' if not repo_name else repo_name\n",
    "            export_png(grid, filename=\"./images/line_mean_time_between_comments/line_mean_time_between_comments__{}_PRs__yaxis_{}__repo_{}.png\".format(description, y_axis, repo_dict[repo_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for repo_name in pr_all['repo_name'].unique():\n",
    "#visualize_mean_time_between_responses({'All':pr_closed,'Slowest 20%':pr_slow20_not_merged.append(pr_slow20_merged,ignore_index=True)}, repo_id = repo_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_time_to_first_comment(input_df, repo_id, x_axis='pr_closed_at', y_axis='days_to_first_response', description='All', num_outliers_repo_map={}, group_by='merged_flag', same_scales=True, columns=2, legend_position='top_right', remove_outliers = 0):\n",
    "    if type(repo_id) == type(repo_list):\n",
    "        repo_ids = repo_id\n",
    "    else:\n",
    "        repo_ids = [repo_id]\n",
    "    \n",
    "    for repo_id in repo_ids:\n",
    "    \n",
    "        output_notebook()\n",
    "\n",
    "        driver_df = input_df.copy()\n",
    "\n",
    "        driver_df = driver_df.loc[driver_df['repo_id'] == repo_id]\n",
    "\n",
    "        group_by_groups = sorted(driver_df[group_by].unique())\n",
    "\n",
    "        seconds = ((driver_df[x_axis].max() + datetime.timedelta(days=25))- (driver_df[x_axis].min() - datetime.timedelta(days=30))).total_seconds()\n",
    "        quarter_years = seconds / 10506240\n",
    "        quarter_years = round(quarter_years)\n",
    "\n",
    "        title_beginning = '{}: '.format(repo_dict[repo_id]) \n",
    "        plot_width = 180 * 5\n",
    "        p = figure(x_range=(driver_df[x_axis].min() - datetime.timedelta(days=30), driver_df[x_axis].max() + datetime.timedelta(days=25)), \n",
    "                  #(driver_df[y_axis].min(), driver_df[y_axis].max()), \n",
    "                   toolbar_location=None,\n",
    "                   title='{}Days to First Response for {} Closed Pull Requests'.format(title_beginning, description), plot_width=plot_width, \n",
    "                   plot_height=400, x_axis_type='datetime')\n",
    "\n",
    "        for index, group_by_group in enumerate(group_by_groups):\n",
    "            p.scatter(x_axis, y_axis, color=colors[index], marker=\"square\", source=driver_df.loc[driver_df[group_by] == group_by_group], legend_label=group_by_group)\n",
    "\n",
    "            if group_by_group == \"Merged / Accepted\":\n",
    "                merged_values = driver_df.loc[driver_df[group_by] == group_by_group][y_axis].dropna().values.tolist()\n",
    "            else:\n",
    "                not_merged_values = driver_df.loc[driver_df[group_by] == group_by_group][y_axis].dropna().values.tolist()\n",
    "\n",
    "        values = not_merged_values + merged_values\n",
    "        #values.fillna(0)\n",
    "\n",
    "        for value in range(0, remove_outliers):\n",
    "            values.remove(max(values))\n",
    " \n",
    "        #determine y_max by finding the max of the values and scaling it up a small amoutn\n",
    "        y_max = max(values)*1.0111\n",
    "        outliers = driver_df.loc[driver_df[y_axis] > y_max]\n",
    "        if len(outliers) > 0:\n",
    "            if repo_id:\n",
    "                p.add_layout(Title(text=\"** Outliers cut off at {} days: {} outlier(s) for {} were removed **\".format(y_max, len(outliers), repo_name), align=\"center\"), \"below\")\n",
    "            else:\n",
    "                p.add_layout(Title(text=\"** Outliers cut off at {} days: {} outlier(s) were removed **\".format(y_max, len(outliers)), align=\"center\"), \"below\")\n",
    "\n",
    "        p.xaxis.axis_label = 'Date Closed' if x_axis == 'pr_closed_at' else 'Date Created' if x_axis == 'pr_created_at' else 'Date'\n",
    "        p.yaxis.axis_label = 'Days to First Response'\n",
    "        p.legend.location = legend_position\n",
    "\n",
    "        p.title.align = \"center\"\n",
    "        p.title.text_font_size = \"16px\"\n",
    "\n",
    "        p.xaxis.axis_label_text_font_size = \"16px\"\n",
    "        p.xaxis.major_label_text_font_size = \"16px\"\n",
    "\n",
    "        p.yaxis.axis_label_text_font_size = \"16px\"\n",
    "        p.yaxis.major_label_text_font_size = \"16px\"\n",
    "\n",
    "        p.y_range = Range1d(0, y_max)\n",
    "        \n",
    "        plot = p\n",
    "        \n",
    "        p = figure(width = plot_width, height=200, margin = (0, 0, 0, 0))\n",
    "        caption = \"This graph shows the days to first reponse for individual pull requests, either Merged or Not Merged.\"\n",
    "        p.add_layout(Label(\n",
    "        x = 0, # Change to shift caption left or right\n",
    "        y = 160, \n",
    "        x_units = 'screen',\n",
    "        y_units = 'screen',\n",
    "        text='{}'.format(caption),\n",
    "        text_font = 'times', # Use same font as paper\n",
    "        text_font_size = '15pt',\n",
    "        render_mode='css'\n",
    "        ))\n",
    "        p.outline_line_color = None\n",
    "\n",
    "        caption_plot = p\n",
    "\n",
    "        grid = gridplot([[plot], [caption_plot]])\n",
    "\n",
    "        show(grid)\n",
    "\n",
    "        if save_files:\n",
    "            repo_extension = repo_ids\n",
    "            export_png(grid, filename=\"./images/first_comment_times/scatter_first_comment_times__{}_PRs__xaxis_{}__repo_{}.png\".format(description, x_axis, repo_dict[repo_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for repo_name in pr_all['repo_name'].unique():\n",
    "#visualize_time_to_first_comment(pr_closed, repo_id= repo_list, legend_position='top_right', remove_outliers = scatter_plot_outliers_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hex_to_RGB(hex):\n",
    "    ''' \"#FFFFFF\" -> [255,255,255] '''\n",
    "    # Pass 16 to the integer function for change of base\n",
    "    return [int(hex[i:i+2], 16) for i in range(1,6,2)]\n",
    "\n",
    "def color_dict(gradient):\n",
    "    ''' Takes in a list of RGB sub-lists and returns dictionary of\n",
    "    colors in RGB and hex form for use in a graphing function\n",
    "    defined later on '''\n",
    "    return {\"hex\":[RGB_to_hex(RGB) for RGB in gradient],\n",
    "      \"r\":[RGB[0] for RGB in gradient],\n",
    "      \"g\":[RGB[1] for RGB in gradient],\n",
    "      \"b\":[RGB[2] for RGB in gradient]}\n",
    "\n",
    "def RGB_to_hex(RGB):\n",
    "    ''' [255,255,255] -> \"#FFFFFF\" '''\n",
    "    # Components need to be integers for hex to make sense\n",
    "    RGB = [int(x) for x in RGB]\n",
    "    return \"#\"+\"\".join([\"0{0:x}\".format(v) if v < 16 else\n",
    "            \"{0:x}\".format(v) for v in RGB])\n",
    "\n",
    "def linear_gradient(start_hex, finish_hex=\"#FFFFFF\", n=10):\n",
    "    ''' returns a gradient list of (n) colors between\n",
    "    two hex colors. start_hex and finish_hex\n",
    "    should be the full six-digit color string,\n",
    "    inlcuding the number sign (\"#FFFFFF\") '''\n",
    "    # Starting and ending colors in RGB form\n",
    "    s = hex_to_RGB(start_hex)\n",
    "    f = hex_to_RGB(finish_hex)\n",
    "    # Initilize a list of the output colors with the starting color\n",
    "    RGB_list = [s]\n",
    "    # Calcuate a color at each evenly spaced value of t from 1 to n\n",
    "    for t in range(1, n):\n",
    "        # Interpolate RGB vector for color at the current value of t\n",
    "        curr_vector = [\n",
    "          int(s[j] + (float(t)/(n-1))*(f[j]-s[j]))\n",
    "          for j in range(3)\n",
    "        ]\n",
    "        # Add it to our list of output colors\n",
    "        RGB_list.append(curr_vector)\n",
    "\n",
    "    return color_dict(RGB_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import BasicTicker, ColorBar, LinearColorMapper, PrintfTickFormatter, LogTicker, Label\n",
    "from bokeh.transform import transform\n",
    "        \n",
    "def events_types_heat_map(input_df, repo_id, include_comments=True, x_axis='closed_year', facet=\"merged_flag\",columns=2, x_max=1100, same_scales=True, y_axis='repo_name', description=\"All Closed\", title=\"Average Pull Request Event Types for {} Pull Requests\"):\n",
    "    if type(repo_id) == type(repo_list):\n",
    "        repo_ids = repo_id\n",
    "    else:\n",
    "        repo_ids = [repo_id]\n",
    "    \n",
    "    for repo_id in repo_ids:\n",
    "    \n",
    "        colors = linear_gradient('#f5f5dc', '#fff44f', 150)['hex']\n",
    "\n",
    "        driver_df = input_df.copy()\n",
    "        driver_df[x_axis] = driver_df[x_axis].astype(str)\n",
    "\n",
    "        driver_df = driver_df.loc[driver_df['repo_id'] == repo_id]\n",
    "\n",
    "        if facet == 'closed_year' or y_axis == 'closed_year':\n",
    "            driver_df['closed_year'] = driver_df['closed_year'].astype(int).astype(str)\n",
    "\n",
    "        optional_comments = ['comment_count'] if include_comments else []\n",
    "        driver_df = driver_df[['repo_id', 'repo_name',x_axis, 'assigned_count',\n",
    "              'review_requested_count',\n",
    "              'labeled_count',\n",
    "              'subscribed_count',\n",
    "              'mentioned_count',\n",
    "              'referenced_count',\n",
    "              'closed_count',\n",
    "              'head_ref_force_pushed_count',\n",
    "              'merged_count',\n",
    "              'milestoned_count',\n",
    "              'unlabeled_count',\n",
    "              'head_ref_deleted_count', facet ] + optional_comments]\n",
    "        y_groups = [\n",
    "              'review_requested_count',\n",
    "              'labeled_count',\n",
    "              'subscribed_count',\n",
    "              'referenced_count',\n",
    "              'closed_count',\n",
    "    #           'milestoned_count',\n",
    "              ] + optional_comments\n",
    "        output_notebook()\n",
    "        optional_group_comments = ['comment'] if include_comments else []\n",
    "    #     y_groups = ['subscribed', 'mentioned', 'labeled', 'review_requested', 'head_ref_force_pushed', 'referenced', 'closed', 'merged', 'unlabeled', 'head_ref_deleted', 'milestoned', 'assigned'] + optional_group_comments\n",
    "\n",
    "        x_groups = sorted(list(driver_df[x_axis].unique()))\n",
    "\n",
    "        grid_array = []\n",
    "        grid_row = []  \n",
    "\n",
    "        for index, facet_group in enumerate(sorted(driver_df[facet].unique())):\n",
    "\n",
    "            facet_data = driver_df.loc[driver_df[facet] == facet_group]\n",
    "    #         display(facet_data.sort_values('merged_count', ascending=False).head(50))\n",
    "            driver_df_mean = facet_data.groupby(['repo_id', 'repo_name', x_axis], as_index=False).mean().round(1)\n",
    "    #         data = {'Y' : y_groups}\n",
    "    #         for group in y_groups:\n",
    "    #             data[group] = driver_df_mean[group].tolist()\n",
    "            plot_width = 700\n",
    "            p = figure(y_range=y_groups, plot_height=500, plot_width=plot_width, x_range=x_groups, \n",
    "                       title='{}'.format(format(facet_group)))\n",
    "\n",
    "            for y_group in y_groups:\n",
    "                driver_df_mean['field'] = y_group\n",
    "                source = ColumnDataSource(driver_df_mean)\n",
    "                mapper = LinearColorMapper(palette=colors, low=driver_df_mean[y_group].min(), high=driver_df_mean[y_group].max())\n",
    "\n",
    "                p.rect(y='field', x=x_axis, width=1, height=1, source=source,\n",
    "                       line_color=None, fill_color=transform(y_group, mapper))\n",
    "                # Data label \n",
    "                labels = LabelSet(x=x_axis, y='field', text=y_group, y_offset=-8,\n",
    "                          text_font_size=\"12pt\", text_color='black',\n",
    "                          source=source, text_align='center')\n",
    "                p.add_layout(labels)\n",
    "\n",
    "                color_bar = ColorBar(color_mapper=mapper, location=(0, 0),\n",
    "                                     ticker=BasicTicker(desired_num_ticks=9),\n",
    "                                     formatter=PrintfTickFormatter(format=\"%d\"))\n",
    "    #         p.add_layout(color_bar, 'right')\n",
    "\n",
    "\n",
    "            p.y_range.range_padding = 0.1\n",
    "            p.ygrid.grid_line_color = None\n",
    "\n",
    "            p.legend.location = \"bottom_right\"\n",
    "            p.axis.minor_tick_line_color = None\n",
    "            p.outline_line_color = None\n",
    "\n",
    "            p.xaxis.axis_label = 'Year Closed'\n",
    "            p.yaxis.axis_label = 'Event Type'\n",
    "\n",
    "            p.title.align = \"center\"\n",
    "            p.title.text_font_size = \"15px\"\n",
    "\n",
    "            p.xaxis.axis_label_text_font_size = \"16px\"\n",
    "            p.xaxis.major_label_text_font_size = \"16px\"\n",
    "\n",
    "            p.yaxis.axis_label_text_font_size = \"16px\"\n",
    "            p.yaxis.major_label_text_font_size = \"16px\"\n",
    "\n",
    "            grid_row.append(p)\n",
    "            if index % columns == columns - 1:\n",
    "                grid_array.append(grid_row)\n",
    "                grid_row = []\n",
    "        grid = gridplot(grid_array)\n",
    "        \n",
    "        #add title, the title changes its x value based on the number of x_groups so that it stays centered\n",
    "        label=Label(x=-len(x_groups), y=6.9, text='{}: Average Pull Request Event Types for {} Closed Pull Requests'.format(repo_dict[repo_id], description), render_mode='css', text_font_size = '17px', text_font_style= 'bold')\n",
    "        p.add_layout(label)\n",
    "\n",
    "        show(grid, plot_width=1200, plot_height=1200)\n",
    "        if save_files:\n",
    "            comments_included = 'comments_included' if include_comments else 'comments_not_included'\n",
    "            repo_extension = 'All' if not repo_id else repo_id\n",
    "            export_png(grid, filename=\"./images/h_stacked_bar_mean_event_types/mean_event_types__facet_{}__{}_PRs__yaxis_{}__{}__repo_{}.png\".format(facet, description, y_axis, comments_included, repo_dict[repo_id]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for repo_name in pr_all['repo_name'].unique():\n",
    "#events_types_heat_map(pr_closed, repo_id=repo_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "red_green_gradient = linear_gradient('#0080FF', '#DC143C', 150)['hex']\n",
    "    #32CD32\n",
    "def heat_map(input_df, repo_id, x_axis='repo_name', group_by='merged_flag', y_axis='closed_yearmonth', same_scales=True, description=\"All Closed\", heat_field='days_to_first_response', columns=2, remove_outliers = 0):\n",
    "\n",
    "\n",
    "    output_notebook()\n",
    "\n",
    "    driver_df = input_df.copy()[['repo_id', y_axis, group_by, x_axis, heat_field]]\n",
    "\n",
    "    if display_grouping == 'repo':\n",
    "        driver_df = driver_df.loc[driver_df['repo_id'] == repo_id]\n",
    "\n",
    "    driver_df[y_axis] = driver_df[y_axis].astype(str)\n",
    "\n",
    "    # add new group by + xaxis column \n",
    "    driver_df['grouped_x'] = driver_df[x_axis] + ' - ' + driver_df[group_by]\n",
    "\n",
    "    driver_df_mean = driver_df.groupby(['grouped_x', y_axis], as_index=False).mean()\n",
    "\n",
    "    colors = red_green_gradient\n",
    "    y_groups = driver_df_mean[y_axis].unique()\n",
    "    x_groups = sorted(driver_df[x_axis].unique())\n",
    "    grouped_x_groups = sorted(driver_df_mean['grouped_x'].unique())\n",
    "\n",
    "    values = driver_df_mean['days_to_first_response'].values.tolist()\n",
    "    for i in range(0, remove_outliers):\n",
    "        values.remove(max(values))\n",
    "\n",
    "    heat_max = max(values)* 1.02\n",
    "\n",
    "    mapper = LinearColorMapper(palette=colors, low=driver_df_mean[heat_field].min(), high=heat_max)#driver_df_mean[heat_field].max())\n",
    "\n",
    "    source = ColumnDataSource(driver_df_mean)\n",
    "    title_beginning = repo_dict[repo_id] + ':' if not type(repo_id) == type(repo_list) else ''\n",
    "    plot_width = 1100\n",
    "    p = figure(plot_width=plot_width, plot_height=300, title=\"{} Mean Duration (Days) {} Pull Requests\".format(title_beginning,description),\n",
    "               y_range=grouped_x_groups[::-1], x_range=y_groups,\n",
    "               toolbar_location=None, tools=\"\")#, x_axis_location=\"above\")\n",
    "\n",
    "    for x_group in x_groups:\n",
    "        outliers = driver_df_mean.loc[(driver_df_mean[heat_field] > heat_max) & (driver_df_mean['grouped_x'].str.contains(x_group))]\n",
    "\n",
    "        if len(outliers) > 0:\n",
    "            p.add_layout(Title(text=\"** Outliers capped at {} days: {} outlier(s) for {} were capped at {} **\".format(heat_max, len(outliers), x_group, heat_max), align=\"center\"), \"below\")\n",
    "\n",
    "    p.rect(x=y_axis, y='grouped_x', width=1, height=1, source=source,\n",
    "           line_color=None, fill_color=transform(heat_field, mapper))\n",
    "\n",
    "    color_bar = ColorBar(color_mapper=mapper, location=(0, 0),\n",
    "                         ticker=BasicTicker(desired_num_ticks=9),\n",
    "                         formatter=PrintfTickFormatter(format=\"%d\"))\n",
    "\n",
    "    p.add_layout(color_bar, 'right')\n",
    "\n",
    "    p.title.align = \"center\"\n",
    "    p.title.text_font_size = \"16px\"\n",
    "\n",
    "    p.axis.axis_line_color = None\n",
    "    p.axis.major_tick_line_color = None\n",
    "    p.axis.major_label_text_font_size = \"11pt\"\n",
    "    p.axis.major_label_standoff = 0\n",
    "    p.xaxis.major_label_orientation = 1.0\n",
    "    p.xaxis.axis_label = 'Month Closed' if y_axis[0:6] == 'closed' else 'Date Created' if y_axis[0:7] == 'created' else 'Repository' if y_axis == 'repo_name' else ''\n",
    "#     p.yaxis.axis_label = 'Merged Status'\n",
    "\n",
    "    p.title.text_font_size = \"16px\"\n",
    "\n",
    "    p.xaxis.axis_label_text_font_size = \"16px\"\n",
    "    p.xaxis.major_label_text_font_size = \"14px\"\n",
    "\n",
    "    p.yaxis.major_label_text_font_size = \"15px\"\n",
    "\n",
    "    plot = p\n",
    "\n",
    "    p = figure(width = plot_width, height=200, margin = (0, 0, 0, 0))\n",
    "    caption = \"Caption Here\"\n",
    "    p.add_layout(Label(\n",
    "    x = 0, # Change to shift caption left or right\n",
    "    y = 160, \n",
    "    x_units = 'screen',\n",
    "    y_units = 'screen',\n",
    "    text='{}'.format(caption),\n",
    "    text_font = 'times', # Use same font as paper\n",
    "    text_font_size = '15pt',\n",
    "    render_mode='css'\n",
    "    ))\n",
    "    p.outline_line_color = None\n",
    "\n",
    "    caption_plot = p\n",
    "\n",
    "    grid = gridplot([[plot], [caption_plot]])\n",
    "\n",
    "    show(grid)\n",
    "\n",
    "    if save_files:\n",
    "        repo_extension = 'All' if not repo_id else repo_id\n",
    "        export_png(grid, filename=\"./images/heat_map_pr_duration_merged_status/heat_map_duration_by_merged_status__{}_PRs__yaxis_{}__repo_{}.png\".format(description, y_axis, repo_dict[repo_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#heat_map(pr_closed, repo_id=25502)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if display_grouping == 'repo':\n",
    "    for repo_id in repo_set:\n",
    "        vertical_grouped_bar(pr_all, repo_id=repo_id)\n",
    "        horizontal_stacked_bar(pr_closed, repo_id=repo_id)\n",
    "        merged_ratio_vertical_grouped_bar({'All':pr_closed,'Slowest 20%':pr_slow20_not_merged.append(pr_slow20_merged,ignore_index=True)}, repo_id = repo_id)\n",
    "        visualize_mean_response_times(pr_closed, repo_id=repo_id, legend_position='center')\n",
    "        visualize_mean_time_between_responses({'All':pr_closed,'Slowest 20%':pr_slow20_not_merged.append(pr_slow20_merged,ignore_index=True)}, repo_id = repo_id)\n",
    "        visualize_time_to_first_comment(pr_closed, repo_id= repo_id, legend_position='top_right', remove_outliers = scatter_plot_outliers_removed)\n",
    "        events_types_heat_map(pr_closed, repo_id=repo_id)\n",
    "        heat_map(pr_closed, repo_id=repo_id)\n",
    "\n",
    "elif display_grouping == 'competitors':\n",
    "    vertical_grouped_bar(pr_all, repo_id=repo_list)\n",
    "    horizontal_stacked_bar(pr_closed, repo_id=repo_list)\n",
    "    merged_ratio_vertical_grouped_bar({'All':pr_closed,'Slowest 20%':pr_slow20_not_merged.append(pr_slow20_merged,ignore_index=True)}, repo_id = repo_list)\n",
    "    visualize_mean_response_times(pr_closed, repo_id=repo_list, legend_position='center')\n",
    "    visualize_mean_time_between_responses({'All':pr_closed,'Slowest 20%':pr_slow20_not_merged.append(pr_slow20_merged,ignore_index=True)}, repo_id = repo_list)\n",
    "    visualize_time_to_first_comment(pr_closed, repo_id= repo_list, legend_position='top_right', remove_outliers = scatter_plot_outliers_removed)\n",
    "    events_types_heat_map(pr_closed, repo_id=repo_list)\n",
    "    heat_map(pr_closed, repo_id=repo_list)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
