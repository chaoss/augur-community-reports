{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the Repos Available in your Database, and What Repository Groups They Are In"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to your database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd \n",
    "import sqlalchemy as salc\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import datetime\n",
    "import json\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "with open(\"config.json\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "database_connection_string = 'postgres+psycopg2://{}:{}@{}:{}/{}'.format(config['user'], config['password'], config['host'], config['port'], config['database'])\n",
    "\n",
    "dbschema='augur_data'\n",
    "engine = salc.create_engine(\n",
    "    database_connection_string,\n",
    "    connect_args={'options': '-csearch_path={}'.format(dbschema)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Available Respositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rg_name</th>\n",
       "      <th>repo_group_id</th>\n",
       "      <th>repo_name</th>\n",
       "      <th>repo_id</th>\n",
       "      <th>forked_from</th>\n",
       "      <th>repo_archived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3scale</td>\n",
       "      <td>25433</td>\n",
       "      <td>3scale</td>\n",
       "      <td>25639</td>\n",
       "      <td>Parent not available</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3scale</td>\n",
       "      <td>25433</td>\n",
       "      <td>3scale-amp-openshift-templates</td>\n",
       "      <td>25613</td>\n",
       "      <td>Parent not available</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3scale</td>\n",
       "      <td>25433</td>\n",
       "      <td>3scale-api-python</td>\n",
       "      <td>25662</td>\n",
       "      <td>3scale-qe/3scale-api-python</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3scale</td>\n",
       "      <td>25433</td>\n",
       "      <td>3scale-api-ruby</td>\n",
       "      <td>25607</td>\n",
       "      <td>Parent not available</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3scale</td>\n",
       "      <td>25433</td>\n",
       "      <td>3scale-go-client</td>\n",
       "      <td>25643</td>\n",
       "      <td>Parent not available</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1325</th>\n",
       "      <td>quay</td>\n",
       "      <td>25430</td>\n",
       "      <td>test-cluster</td>\n",
       "      <td>25525</td>\n",
       "      <td>Parent not available</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1326</th>\n",
       "      <td>quay</td>\n",
       "      <td>25430</td>\n",
       "      <td>update-banner-job</td>\n",
       "      <td>25510</td>\n",
       "      <td>Parent not available</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1327</th>\n",
       "      <td>quay</td>\n",
       "      <td>25430</td>\n",
       "      <td>update-py2-db</td>\n",
       "      <td>25517</td>\n",
       "      <td>Parent not available</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1328</th>\n",
       "      <td>quay</td>\n",
       "      <td>25430</td>\n",
       "      <td>update-ro-keys-job</td>\n",
       "      <td>25518</td>\n",
       "      <td>Parent not available</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1329</th>\n",
       "      <td>quay</td>\n",
       "      <td>25430</td>\n",
       "      <td>zlog</td>\n",
       "      <td>25507</td>\n",
       "      <td>Parent not available</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1330 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     rg_name  repo_group_id                       repo_name  repo_id  \\\n",
       "0     3scale          25433                          3scale    25639   \n",
       "1     3scale          25433  3scale-amp-openshift-templates    25613   \n",
       "2     3scale          25433               3scale-api-python    25662   \n",
       "3     3scale          25433                 3scale-api-ruby    25607   \n",
       "4     3scale          25433                3scale-go-client    25643   \n",
       "...      ...            ...                             ...      ...   \n",
       "1325    quay          25430                    test-cluster    25525   \n",
       "1326    quay          25430               update-banner-job    25510   \n",
       "1327    quay          25430                   update-py2-db    25517   \n",
       "1328    quay          25430              update-ro-keys-job    25518   \n",
       "1329    quay          25430                            zlog    25507   \n",
       "\n",
       "                      forked_from  repo_archived  \n",
       "0            Parent not available            0.0  \n",
       "1            Parent not available            0.0  \n",
       "2     3scale-qe/3scale-api-python            0.0  \n",
       "3            Parent not available            0.0  \n",
       "4            Parent not available            0.0  \n",
       "...                           ...            ...  \n",
       "1325         Parent not available            0.0  \n",
       "1326         Parent not available            0.0  \n",
       "1327         Parent not available            0.0  \n",
       "1328         Parent not available            0.0  \n",
       "1329         Parent not available            0.0  \n",
       "\n",
       "[1330 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "rg_name           object\n",
       "repo_group_id      int64\n",
       "repo_name         object\n",
       "repo_id            int64\n",
       "forked_from       object\n",
       "repo_archived    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repolist = pd.DataFrame()\n",
    "\n",
    "repo_query = salc.sql.text(f\"\"\"\n",
    "             SELECT a.rg_name,\n",
    "                a.repo_group_id,\n",
    "                b.repo_name,\n",
    "                b.repo_id,\n",
    "                b.forked_from,\n",
    "                b.repo_archived \n",
    "            FROM\n",
    "                repo_groups a,\n",
    "                repo b \n",
    "            WHERE\n",
    "                a.repo_group_id = b.repo_group_id \n",
    "            ORDER BY\n",
    "                rg_name,\n",
    "                repo_name;   \n",
    "\n",
    "    \"\"\")\n",
    "\n",
    "repolist = pd.read_sql(repo_query, con=engine)\n",
    "\n",
    "display(repolist)\n",
    "\n",
    "repolist.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapted version of clustering-worker-tasks.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 385\u001b[0m\n\u001b[1;32m    382\u001b[0m stemmer \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mstem\u001b[38;5;241m.\u001b[39msnowball\u001b[38;5;241m.\u001b[39mSnowballStemmer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    384\u001b[0m \u001b[38;5;66;03m#clustering_model(\"https://github.com/chaoss/augur\", engine, session)\u001b[39;00m\n\u001b[0;32m--> 385\u001b[0m \u001b[43mclustering_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://github.com/chaoss/augur\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m display(\u001b[38;5;241m7\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 95\u001b[0m, in \u001b[0;36mclustering_model\u001b[0;34m(repo_git, engine)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# check if dumped pickle file exists, if exists no need to train the model\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(MODEL_FILE_NAME):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;66;03m#train_model(engine, session, max_df, min_df, max_features, ngram_range, num_clusters, num_topics, num_words_per_topic, tool_source, tool_version, data_source)\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m     \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mngram_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_clusters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_topics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_words_per_topic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_source\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_version\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_source\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     97\u001b[0m     model_stats \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstat(MODEL_FILE_NAME)\n",
      "Cell \u001b[0;32mIn[3], line 275\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(engine, max_df, min_df, max_features, ngram_range, num_clusters, num_topics, num_words_per_topic, tool_source, tool_version, data_source)\u001b[0m\n\u001b[1;32m    271\u001b[0m message_desc_df \u001b[38;5;241m=\u001b[39m msg_df_all[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_git\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmsg_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mgroupby(\n\u001b[1;32m    272\u001b[0m     [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_git\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39magg(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m    273\u001b[0m message_desc_df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_git\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage_count\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 275\u001b[0m tfidf_matrix, features \u001b[38;5;241m=\u001b[39m \u001b[43mget_tf_idf_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmsg_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mngram_range\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m msg_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m cluster_and_label(tfidf_matrix, num_clusters)\n\u001b[1;32m    279\u001b[0m \u001b[38;5;66;03m# LDA - Topic Modeling\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 190\u001b[0m, in \u001b[0;36mget_tf_idf_matrix\u001b[0;34m(text_list, max_df, max_features, min_df, ngram_range)\u001b[0m\n\u001b[1;32m    185\u001b[0m tfidf_vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(max_df\u001b[38;5;241m=\u001b[39mmax_df, max_features\u001b[38;5;241m=\u001b[39mmax_features,\n\u001b[1;32m    186\u001b[0m                                    min_df\u001b[38;5;241m=\u001b[39mmin_df, stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    187\u001b[0m                                    use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, tokenizer\u001b[38;5;241m=\u001b[39mpreprocess_and_tokenize,\n\u001b[1;32m    188\u001b[0m                                    ngram_range\u001b[38;5;241m=\u001b[39mngram_range)\n\u001b[1;32m    189\u001b[0m tfidf_transformer \u001b[38;5;241m=\u001b[39m tfidf_vectorizer\u001b[38;5;241m.\u001b[39mfit(text_list)\n\u001b[0;32m--> 190\u001b[0m tfidf_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mtfidf_transformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m pickle\u001b[38;5;241m.\u001b[39mdump(tfidf_transformer\u001b[38;5;241m.\u001b[39mvocabulary_, \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocabulary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tfidf_matrix, tfidf_vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n",
      "File \u001b[0;32m~/github/virtualenvs/anotebook/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:2157\u001b[0m, in \u001b[0;36mTfidfVectorizer.transform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   2140\u001b[0m \u001b[38;5;124;03m\"\"\"Transform documents to document-term matrix.\u001b[39;00m\n\u001b[1;32m   2141\u001b[0m \n\u001b[1;32m   2142\u001b[0m \u001b[38;5;124;03mUses the vocabulary and document frequencies (df) learned by fit (or\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2153\u001b[0m \u001b[38;5;124;03m    Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[1;32m   2154\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2155\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m, msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe TF-IDF vectorizer is not fitted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2157\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mtransform(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/github/virtualenvs/anotebook/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1433\u001b[0m, in \u001b[0;36mCountVectorizer.transform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1430\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_vocabulary()\n\u001b[1;32m   1432\u001b[0m \u001b[38;5;66;03m# use the same matrix-building strategy as fit_transform\u001b[39;00m\n\u001b[0;32m-> 1433\u001b[0m _, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfixed_vocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1435\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/github/virtualenvs/anotebook/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1275\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1273\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[1;32m   1274\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1275\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1276\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1277\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m~/github/virtualenvs/anotebook/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:113\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    111\u001b[0m     doc \u001b[38;5;241m=\u001b[39m preprocessor(doc)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ngrams \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stop_words \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[3], line 215\u001b[0m, in \u001b[0;36mpreprocess_and_tokenize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    213\u001b[0m tokens \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mword_tokenize(text)\n\u001b[1;32m    214\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(token) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 215\u001b[0m stems \u001b[38;5;241m=\u001b[39m [stemmer\u001b[38;5;241m.\u001b[39mstem(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stems\n",
      "Cell \u001b[0;32mIn[3], line 215\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    213\u001b[0m tokens \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mword_tokenize(text)\n\u001b[1;32m    214\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(token) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 215\u001b[0m stems \u001b[38;5;241m=\u001b[39m [\u001b[43mstemmer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stems\n",
      "File \u001b[0;32m~/github/virtualenvs/anotebook/lib/python3.9/site-packages/nltk/stem/snowball.py:1578\u001b[0m, in \u001b[0;36mEnglishStemmer.stem\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m   1576\u001b[0m \u001b[38;5;66;03m# STEP 2\u001b[39;00m\n\u001b[1;32m   1577\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m suffix \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step2_suffixes:\n\u001b[0;32m-> 1578\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mword\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendswith\u001b[49m\u001b[43m(\u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1579\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m r1\u001b[38;5;241m.\u001b[39mendswith(suffix):\n\u001b[1;32m   1580\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m suffix \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtional\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import time\n",
    "import traceback\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "import sqlalchemy as s\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import LatentDirichletAllocation  as LDA\n",
    "from collections import OrderedDict\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# def clustering_model(repo_git: str,logger,engine, session) -> None:\n",
    "def clustering_model(repo_git: str, engine) -> None:\n",
    "\n",
    "    ngram_range = (1, 4)\n",
    "    clustering_by_content = True\n",
    "    clustering_by_mechanism = False\n",
    "\n",
    "    # define topic modeling specific parameters\n",
    "    num_topics = 8\n",
    "    num_words_per_topic = 12\n",
    "\n",
    "    tool_source = 'Clustering Worker'\n",
    "    tool_version = '0.2.0'\n",
    "    data_source = 'Augur Collected Messages'\n",
    "\n",
    "    #query = session.query(Repo).filter(Repo.repo_git == repo_git)\n",
    "    #repo_id = execute_session_query(query, 'one').repo_id\n",
    "\n",
    "    num_clusters = 6\n",
    "    max_df = 0.9\n",
    "    max_features = 1000\n",
    "    min_df = 0.1\n",
    "\n",
    "    get_messages_for_repo_sql = s.sql.text(\n",
    "        \"\"\"\n",
    "            SELECT\n",
    "                r.repo_group_id,\n",
    "                r.repo_id,\n",
    "                r.repo_git,\n",
    "                r.repo_name,\n",
    "                i.issue_id thread_id,\n",
    "                M.msg_text,\n",
    "                i.issue_title thread_title,\n",
    "                M.msg_id \n",
    "            FROM\n",
    "                augur_data.repo r,\n",
    "                augur_data.issues i,\n",
    "                augur_data.message M,\n",
    "                augur_data.issue_message_ref imr \n",
    "            WHERE\n",
    "                r.repo_id = i.repo_id \n",
    "                AND imr.issue_id = i.issue_id \n",
    "                AND imr.msg_id = M.msg_id \n",
    "                AND r.repo_id = :repo_id \n",
    "            UNION\n",
    "            SELECT\n",
    "                r.repo_group_id,\n",
    "                r.repo_id,\n",
    "                        r.repo_git,\n",
    "                r.repo_name,\n",
    "                pr.pull_request_id thread_id,\n",
    "                M.msg_text,\n",
    "                pr.pr_src_title thread_title,\n",
    "                M.msg_id \n",
    "            FROM\n",
    "                augur_data.repo r,\n",
    "                augur_data.pull_requests pr,\n",
    "                augur_data.message M,\n",
    "                augur_data.pull_request_message_ref prmr \n",
    "            WHERE\n",
    "                r.repo_id = pr.repo_id \n",
    "                AND prmr.pull_request_id = pr.pull_request_id \n",
    "                AND prmr.msg_id = M.msg_id \n",
    "                AND r.repo_id = :repo_id\n",
    "            \"\"\"\n",
    "    )\n",
    "    # result = db.execute(delete_points_SQL, repo_id=repo_id, min_date=min_date)\n",
    "    msg_df_cur_repo = pd.read_sql(get_messages_for_repo_sql, engine, params={\"repo_id\": 25613})\n",
    "  \n",
    "    # check if dumped pickle file exists, if exists no need to train the model\n",
    "    if not os.path.exists(MODEL_FILE_NAME):\n",
    "        #train_model(engine, session, max_df, min_df, max_features, ngram_range, num_clusters, num_topics, num_words_per_topic, tool_source, tool_version, data_source)\n",
    "        train_model(engine, max_df, min_df, max_features, ngram_range, num_clusters, num_topics, num_words_per_topic, tool_source, tool_version, data_source)\n",
    "    else:\n",
    "        model_stats = os.stat(MODEL_FILE_NAME)\n",
    "        model_age = (time.time() - model_stats.st_mtime)\n",
    "        # if the model is more than month old, retrain it.\n",
    "        if model_age > 2000000:\n",
    "            # train_model(logger, engine, session, max_df, min_df, max_features, ngram_range, num_clusters, num_topics, num_words_per_topic, tool_source, tool_version, data_source)\n",
    "            train_model(engine, max_df, min_df, max_features, ngram_range, num_clusters, num_topics, num_words_per_topic, tool_source, tool_version, data_source)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    with open(\"kmeans_repo_messages\", 'rb') as model_file:\n",
    "        kmeans_model = pickle.load(model_file)\n",
    "\n",
    "    msg_df = msg_df_cur_repo.groupby('repo_id')['msg_text'].apply(','.join).reset_index()\n",
    "\n",
    "    # logger.debug(f'messages being clustered: {msg_df}')\n",
    "\n",
    "    if msg_df.empty:\n",
    "        # logger.info(\"not enough data for prediction\")\n",
    "        # self.register_task_completion(task, repo_id, 'clustering')\n",
    "        return\n",
    "\n",
    "    vocabulary = pickle.load(open(\"vocabulary\", \"rb\"))\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=max_df, max_features=max_features,\n",
    "                                       min_df=min_df, stop_words='english',\n",
    "                                       use_idf=True, tokenizer=preprocess_and_tokenize,\n",
    "                                       ngram_range=ngram_range, vocabulary=vocabulary)\n",
    "    tfidf_transformer = tfidf_vectorizer.fit(\n",
    "        msg_df['msg_text'])  # might be fitting twice, might have been used in training\n",
    "\n",
    "    # save new vocabulary ??\n",
    "    feature_matrix_cur_repo = tfidf_transformer.transform(msg_df['msg_text'])\n",
    "\n",
    "    prediction = kmeans_model.predict(feature_matrix_cur_repo)\n",
    "\n",
    "    # inserting data\n",
    "    record = {\n",
    "        'repo_id': int(25613),\n",
    "        'cluster_content': int(prediction[0]),\n",
    "        'cluster_mechanism': -1,\n",
    "        'tool_source': tool_source,\n",
    "        'tool_version': tool_version,\n",
    "        'data_source': data_source\n",
    "    }\n",
    "    #repo_cluster_messages_obj = RepoClusterMessage(**record)\n",
    "    #session.add(repo_cluster_messages_obj)\n",
    "    #session.commit()\n",
    "\n",
    "    # result = db.execute(repo_cluster_messages_table.insert().values(record))\n",
    "    try:\n",
    "        lda_model = pickle.load(open(\"lda_model\", \"rb\"))\n",
    "        vocabulary = pickle.load(open(\"vocabulary_count\", \"rb\"))\n",
    "        count_vectorizer = CountVectorizer(max_df=max_df, max_features=max_features, min_df=min_df,\n",
    "                                           stop_words=\"english\", tokenizer=preprocess_and_tokenize,\n",
    "                                           vocabulary=vocabulary)\n",
    "        count_transformer = count_vectorizer.fit(\n",
    "            msg_df['msg_text'])  # might be fitting twice, might have been used in training\n",
    "\n",
    "        # save new vocabulary ??\n",
    "        count_matrix_cur_repo = count_transformer.transform(msg_df['msg_text'])\n",
    "        prediction = lda_model.transform(count_matrix_cur_repo)\n",
    "\n",
    "        for i, prob_vector in enumerate(prediction):\n",
    "            # repo_id = msg_df.loc[i]['repo_id']\n",
    "            for i, prob in enumerate(prob_vector):\n",
    "                record = {\n",
    "                    'repo_id': int(repo_id),\n",
    "                    'topic_id': i + 1,\n",
    "                    'topic_prob': prob,\n",
    "                    'tool_source': tool_source,\n",
    "                    'tool_version': tool_version,\n",
    "                    'data_source': data_source\n",
    "                }\n",
    "\n",
    "                repo_topic_object = RepoTopic(**record)\n",
    "                #session.add(repo_topic_object)\n",
    "                #session.commit()\n",
    "\n",
    "                    # result = db.execute(repo_topic_table.insert().values(record))\n",
    "    except Exception as e:\n",
    "        stacker = traceback.format_exc()\n",
    "        pass\n",
    "\n",
    "    # self.register_task_completion(task, repo_id, 'clustering')\n",
    "\n",
    "\n",
    "def get_tf_idf_matrix(text_list, max_df, max_features, min_df, ngram_range):\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=max_df, max_features=max_features,\n",
    "                                       min_df=min_df, stop_words='english',\n",
    "                                       use_idf=True, tokenizer=preprocess_and_tokenize,\n",
    "                                       ngram_range=ngram_range)\n",
    "    tfidf_transformer = tfidf_vectorizer.fit(text_list)\n",
    "    tfidf_matrix = tfidf_transformer.transform(text_list)\n",
    "    pickle.dump(tfidf_transformer.vocabulary_, open(\"vocabulary\", 'wb'))\n",
    "    return tfidf_matrix, tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "def cluster_and_label(feature_matrix, num_clusters):\n",
    "    kmeans_model = KMeans(n_clusters=num_clusters)\n",
    "    kmeans_model.fit(feature_matrix)\n",
    "    pickle.dump(kmeans_model, open(\"kmeans_repo_messages\", 'wb'))\n",
    "    return kmeans_model.labels_.tolist()\n",
    "\n",
    "def count_func(msg):\n",
    "    blobed = TextBlob(msg)\n",
    "    counts = Counter(tag for word, tag in blobed.tags if\n",
    "                     tag not in ['NNPS', 'RBS', 'SYM', 'WP$', 'LS', 'POS', 'RP', 'RBR', 'JJS', 'UH', 'FW', 'PDT'])\n",
    "    total = sum(counts.values())\n",
    "    normalized_count = {key: value / total for key, value in counts.items()}\n",
    "    return normalized_count\n",
    "\n",
    "def preprocess_and_tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[@]\\w+', '', text)\n",
    "    text = re.sub(r'[^A-Za-z]+', ' ', text)\n",
    "\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 1]\n",
    "    stems = [stemmer.stem(t) for t in tokens]\n",
    "    return stems\n",
    "\n",
    "def train_model(engine, max_df, min_df, max_features, ngram_range, num_clusters, num_topics, num_words_per_topic, tool_source, tool_version, data_source):\n",
    "    def visualize_labels_PCA(features, labels, annotations, num_components, title):\n",
    "        labels_color_map = {-1: \"red\"}\n",
    "        for label in labels:\n",
    "            labels_color_map[label] = [list([x / 255.0 for x in list(np.random.choice(range(256), size=3))])]\n",
    "        low_dim_data = PCA(n_components=num_components).fit_transform(features)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(20, 10))\n",
    "\n",
    "        for i, data in enumerate(low_dim_data):\n",
    "            pca_comp_1, pca_comp_2 = data\n",
    "            color = labels_color_map[labels[i]]\n",
    "            ax.scatter(pca_comp_1, pca_comp_2, c=color, label=labels[i])\n",
    "        # ax.annotate(annotations[i],(pca_comp_1, pca_comp_2))\n",
    "\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        handles_label_dict = OrderedDict(zip(labels, handles))\n",
    "        ax.legend(handles_label_dict.values(), handles_label_dict.keys())\n",
    "\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"PCA Component 1\")\n",
    "        plt.ylabel(\"PCA Component 2\")\n",
    "        # plt.show()\n",
    "        filename = labels + \"_PCA.png\"\n",
    "        plt.save_fig(filename)\n",
    "\n",
    "    get_messages_sql = s.sql.text(\n",
    "        \"\"\"\n",
    "        SELECT r.repo_group_id, r.repo_id, r.repo_git, r.repo_name, i.issue_id thread_id,m.msg_text,i.issue_title thread_title,m.msg_id\n",
    "        FROM augur_data.repo r, augur_data.issues i,\n",
    "        augur_data.message m, augur_data.issue_message_ref imr\n",
    "        WHERE r.repo_id=i.repo_id\n",
    "        AND imr.issue_id=i.issue_id\n",
    "        AND imr.msg_id=m.msg_id\n",
    "        UNION\n",
    "        SELECT r.repo_group_id, r.repo_id, r.repo_git, r.repo_name, pr.pull_request_id thread_id,m.msg_text,pr.pr_src_title thread_title,m.msg_id\n",
    "        FROM augur_data.repo r, augur_data.pull_requests pr,\n",
    "        augur_data.message m, augur_data.pull_request_message_ref prmr\n",
    "        WHERE r.repo_id=pr.repo_id\n",
    "        AND prmr.pull_request_id=pr.pull_request_id\n",
    "        AND prmr.msg_id=m.msg_id\n",
    "        \"\"\"\n",
    "    )\n",
    "    msg_df_all = pd.read_sql(get_messages_sql, engine, params={})\n",
    "\n",
    "    # select only highly active repos\n",
    "    msg_df_all = msg_df_all.groupby(\"repo_id\").filter(lambda x: len(x) > 500)\n",
    "\n",
    "    # combining all the messages in a repository to form a single doc\n",
    "    msg_df = msg_df_all.groupby('repo_id')['msg_text'].apply(','.join)\n",
    "    msg_df = msg_df.reset_index()\n",
    "\n",
    "    # dataframe summarizing total message count in a repositoryde\n",
    "    message_desc_df = msg_df_all[[\"repo_id\", \"repo_git\", \"repo_name\", \"msg_id\"]].groupby(\n",
    "        [\"repo_id\", \"repo_git\", \"repo_name\"]).agg('count').reset_index()\n",
    "    message_desc_df.columns = [\"repo_id\", \"repo_git\", \"repo_name\", \"message_count\"]\n",
    "\n",
    "    tfidf_matrix, features = get_tf_idf_matrix(msg_df['msg_text'], max_df, max_features, min_df,\n",
    "                                                    ngram_range)\n",
    "    msg_df['cluster'] = cluster_and_label(tfidf_matrix, num_clusters)\n",
    "\n",
    "    # LDA - Topic Modeling\n",
    "    count_vectorizer = CountVectorizer(max_df=max_df, max_features=max_features, min_df=min_df,\n",
    "                                       stop_words=\"english\", tokenizer=preprocess_and_tokenize)\n",
    "\n",
    "    # count_matrix = count_vectorizer.fit_transform(msg_df['msg_text'])\n",
    "    count_transformer = count_vectorizer.fit(msg_df['msg_text'])\n",
    "    count_matrix = count_transformer.transform(msg_df['msg_text'])\n",
    "    pickle.dump(count_transformer.vocabulary_, open(\"vocabulary_count\", 'wb'))\n",
    "    feature_names = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "    # logger.debug(\"Calling LDA\")\n",
    "    lda_model = LDA(n_components=num_topics)\n",
    "    lda_model.fit(count_matrix)\n",
    "    # each component in lda_model.components_ represents probability distribution over words in that topic\n",
    "    topic_list = lda_model.components_\n",
    "    # Getting word probability\n",
    "    # word_prob = lda_model.exp_dirichlet_component_\n",
    "    # word probabilities\n",
    "    # lda_model does not have state variable in this library\n",
    "    # topics_terms = lda_model.state.get_lambda()\n",
    "    # topics_terms_proba = np.apply_along_axis(lambda x: x/x.sum(),1,topics_terms)\n",
    "    # word_prob = [lda_model.id2word[i] for i in range(topics_terms_proba.shape[1])]\n",
    "\n",
    "    # Site explaining main library used for parsing topics: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\n",
    "\n",
    "    # Good site for optimizing: https://medium.com/@yanlinc/how-to-build-a-lda-topic-model-using-from-text-601cdcbfd3a6\n",
    "    # Another Good Site: https://towardsdatascience.com/an-introduction-to-clustering-algorithms-in-python-123438574097\n",
    "    # https://machinelearningmastery.com/clustering-algorithms-with-python/\n",
    "\n",
    "    pickle.dump(lda_model, open(\"lda_model\", 'wb'))\n",
    "\n",
    "    ## Advance Sequence SQL\n",
    "\n",
    "    # key_sequence_words_sql = s.sql.text(\n",
    "    #                           \"\"\"\n",
    "    #       SELECT nextval('augur_data.topic_words_topic_words_id_seq'::text)\n",
    "    #       \"\"\"\n",
    "    #                               )\n",
    "\n",
    "    # twid = self.db.execute(key_sequence_words_sql)\n",
    "    # logger.info(\"twid variable is: {}\".format(twid))\n",
    "    # insert topic list into database\n",
    "    topic_id = 1\n",
    "    for topic in topic_list:\n",
    "        # twid = self.get_max_id('topic_words', 'topic_words_id') + 1\n",
    "        # logger.info(\"twid variable is: {}\".format(twid))\n",
    "        for i in topic.argsort()[:-num_words_per_topic - 1:-1]:\n",
    "            # twid+=1\n",
    "            # logger.info(\"in loop incremented twid variable is: {}\".format(twid))\n",
    "            # logger.info(\"twid variable is: {}\".format(twid))\n",
    "            record = {\n",
    "                # 'topic_words_id': twid,\n",
    "                # 'word_prob': word_prob[i],\n",
    "                'topic_id': int(topic_id),\n",
    "                'word': feature_names[i],\n",
    "                'tool_source': tool_source,\n",
    "                'tool_version': tool_version,\n",
    "                'data_source': data_source\n",
    "            }\n",
    "\n",
    "            topic_word_obj = TopicWord(**record)\n",
    "            #session.add(topic_word_obj)\n",
    "            #session.commit()\n",
    "\n",
    "            # result = db.execute(topic_words_table.insert().values(record))\n",
    "            \n",
    "        topic_id += 1\n",
    "\n",
    "    # insert topic list into database\n",
    "\n",
    "    # save the model and predict on each repo separately\n",
    "\n",
    "    prediction = lda_model.transform(count_matrix)\n",
    "\n",
    "    topic_model_dict_list = []\n",
    "    for i, prob_vector in enumerate(prediction):\n",
    "        topic_model_dict = {}\n",
    "        topic_model_dict['repo_id'] = msg_df.loc[i]['repo_id']\n",
    "        for i, prob in enumerate(prob_vector):\n",
    "            topic_model_dict[\"topic\" + str(i + 1)] = prob\n",
    "        topic_model_dict_list.append(topic_model_dict)\n",
    "    topic_model_df = pd.DataFrame(topic_model_dict_list)\n",
    "\n",
    "    result_content_df = topic_model_df.set_index('repo_id').join(message_desc_df.set_index('repo_id')).join(\n",
    "        msg_df.set_index('repo_id'))\n",
    "    result_content_df = result_content_df.reset_index()\n",
    "    try:\n",
    "        POS_count_dict = msg_df.apply(lambda row: count_func(row['msg_text']), axis=1)\n",
    "    except Exception as e:\n",
    "        stacker = traceback.format_exc()\n",
    "        pass\n",
    "    try:\n",
    "        msg_df_aug = pd.concat([msg_df, pd.DataFrame.from_records(POS_count_dict)], axis=1)\n",
    "    except Exception as e:\n",
    "        stacker = traceback.format_exc()\n",
    "        pass\n",
    "\n",
    "    visualize_labels_PCA(tfidf_matrix.todense(), msg_df['cluster'], msg_df['repo_id'], 2, \"tex!\")\n",
    "\n",
    "# visualize_labels_PCA(tfidf_matrix.todense(), msg_df['cluster'], msg_df['repo_id'], 2, \"MIN_DF={} and MAX_DF={} and NGRAM_RANGE={}\".format(MIN_DF, MAX_DF, NGRAM_RANGE))\n",
    "\n",
    "\n",
    "MODEL_FILE_NAME = \"kmeans_repo_messages\"\n",
    "stemmer = nltk.stem.snowball.SnowballStemmer(\"english\")\n",
    "\n",
    "#clustering_model(\"https://github.com/chaoss/augur\", engine, session)\n",
    "clustering_model(\"https://github.com/chaoss/augur\", engine)\n",
    "display(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
