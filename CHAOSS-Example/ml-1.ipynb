{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the Repos Available in your Database, and What Repository Groups They Are In"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to your database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd \n",
    "import sqlalchemy as salc\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import datetime\n",
    "import json\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "with open(\"config.json\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "database_connection_string = 'postgres+psycopg2://{}:{}@{}:{}/{}'.format(config['user'], config['password'], config['host'], config['port'], config['database'])\n",
    "\n",
    "dbschema='augur_data'\n",
    "engine = salc.create_engine(\n",
    "    database_connection_string,\n",
    "    connect_args={'options': '-csearch_path={}'.format(dbschema)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Available Respositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rg_name</th>\n",
       "      <th>repo_group_id</th>\n",
       "      <th>repo_name</th>\n",
       "      <th>repo_id</th>\n",
       "      <th>forked_from</th>\n",
       "      <th>repo_archived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3scale</td>\n",
       "      <td>25556</td>\n",
       "      <td>3scale</td>\n",
       "      <td>33134</td>\n",
       "      <td>Parent not available</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3scale</td>\n",
       "      <td>25556</td>\n",
       "      <td>3scale-amp-openshift-templates</td>\n",
       "      <td>33126</td>\n",
       "      <td>Parent not available</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3scale</td>\n",
       "      <td>25556</td>\n",
       "      <td>3scale-api-python</td>\n",
       "      <td>33130</td>\n",
       "      <td>3scale-qe/3scale-api-python</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3scale</td>\n",
       "      <td>25556</td>\n",
       "      <td>3scale-api-ruby</td>\n",
       "      <td>33158</td>\n",
       "      <td>Parent not available</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3scale</td>\n",
       "      <td>25556</td>\n",
       "      <td>3scale-go-client</td>\n",
       "      <td>33169</td>\n",
       "      <td>Parent not available</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14764</th>\n",
       "      <td>zerodayz</td>\n",
       "      <td>25482</td>\n",
       "      <td>sosreport-operator</td>\n",
       "      <td>30872</td>\n",
       "      <td>andreaskaris/sosreport-operator</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14765</th>\n",
       "      <td>zerodayz</td>\n",
       "      <td>25482</td>\n",
       "      <td>talos</td>\n",
       "      <td>30863</td>\n",
       "      <td>siderolabs/talos</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14766</th>\n",
       "      <td>zerodayz</td>\n",
       "      <td>25482</td>\n",
       "      <td>tests</td>\n",
       "      <td>30838</td>\n",
       "      <td>kata-containers/tests</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14767</th>\n",
       "      <td>zerodayz</td>\n",
       "      <td>25482</td>\n",
       "      <td>web</td>\n",
       "      <td>30839</td>\n",
       "      <td>openshifttips/web</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14768</th>\n",
       "      <td>zerodayz</td>\n",
       "      <td>25482</td>\n",
       "      <td>zerodayz</td>\n",
       "      <td>30816</td>\n",
       "      <td>Parent not available</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14769 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        rg_name  repo_group_id                       repo_name  repo_id  \\\n",
       "0        3scale          25556                          3scale    33134   \n",
       "1        3scale          25556  3scale-amp-openshift-templates    33126   \n",
       "2        3scale          25556               3scale-api-python    33130   \n",
       "3        3scale          25556                 3scale-api-ruby    33158   \n",
       "4        3scale          25556                3scale-go-client    33169   \n",
       "...         ...            ...                             ...      ...   \n",
       "14764  zerodayz          25482              sosreport-operator    30872   \n",
       "14765  zerodayz          25482                           talos    30863   \n",
       "14766  zerodayz          25482                           tests    30838   \n",
       "14767  zerodayz          25482                             web    30839   \n",
       "14768  zerodayz          25482                        zerodayz    30816   \n",
       "\n",
       "                           forked_from  repo_archived  \n",
       "0                 Parent not available            0.0  \n",
       "1                 Parent not available            0.0  \n",
       "2          3scale-qe/3scale-api-python            0.0  \n",
       "3                 Parent not available            0.0  \n",
       "4                 Parent not available            0.0  \n",
       "...                                ...            ...  \n",
       "14764  andreaskaris/sosreport-operator            0.0  \n",
       "14765                 siderolabs/talos            0.0  \n",
       "14766            kata-containers/tests            0.0  \n",
       "14767                openshifttips/web            0.0  \n",
       "14768             Parent not available            0.0  \n",
       "\n",
       "[14769 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "rg_name           object\n",
       "repo_group_id      int64\n",
       "repo_name         object\n",
       "repo_id            int64\n",
       "forked_from       object\n",
       "repo_archived    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repolist = pd.DataFrame()\n",
    "\n",
    "repo_query = salc.sql.text(f\"\"\"\n",
    "             SELECT a.rg_name,\n",
    "                a.repo_group_id,\n",
    "                b.repo_name,\n",
    "                b.repo_id,\n",
    "                b.forked_from,\n",
    "                b.repo_archived \n",
    "            FROM\n",
    "                repo_groups a,\n",
    "                repo b \n",
    "            WHERE\n",
    "                a.repo_group_id = b.repo_group_id \n",
    "            ORDER BY\n",
    "                rg_name,\n",
    "                repo_name;   \n",
    "\n",
    "    \"\"\")\n",
    "\n",
    "repolist = pd.read_sql(repo_query, con=engine)\n",
    "\n",
    "display(repolist)\n",
    "\n",
    "repolist.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Simpler List for quickly Identifying repo_group_id's and repo_id's for other queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_id</th>\n",
       "      <th>repo_group_id</th>\n",
       "      <th>repo_name</th>\n",
       "      <th>rg_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33134</td>\n",
       "      <td>25556</td>\n",
       "      <td>3scale</td>\n",
       "      <td>3scale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33126</td>\n",
       "      <td>25556</td>\n",
       "      <td>3scale-amp-openshift-templates</td>\n",
       "      <td>3scale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33130</td>\n",
       "      <td>25556</td>\n",
       "      <td>3scale-api-python</td>\n",
       "      <td>3scale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33158</td>\n",
       "      <td>25556</td>\n",
       "      <td>3scale-api-ruby</td>\n",
       "      <td>3scale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33169</td>\n",
       "      <td>25556</td>\n",
       "      <td>3scale-go-client</td>\n",
       "      <td>3scale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14764</th>\n",
       "      <td>30872</td>\n",
       "      <td>25482</td>\n",
       "      <td>sosreport-operator</td>\n",
       "      <td>zerodayz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14765</th>\n",
       "      <td>30863</td>\n",
       "      <td>25482</td>\n",
       "      <td>talos</td>\n",
       "      <td>zerodayz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14766</th>\n",
       "      <td>30838</td>\n",
       "      <td>25482</td>\n",
       "      <td>tests</td>\n",
       "      <td>zerodayz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14767</th>\n",
       "      <td>30839</td>\n",
       "      <td>25482</td>\n",
       "      <td>web</td>\n",
       "      <td>zerodayz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14768</th>\n",
       "      <td>30816</td>\n",
       "      <td>25482</td>\n",
       "      <td>zerodayz</td>\n",
       "      <td>zerodayz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14769 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       repo_id  repo_group_id                       repo_name   rg_name\n",
       "0        33134          25556                          3scale    3scale\n",
       "1        33126          25556  3scale-amp-openshift-templates    3scale\n",
       "2        33130          25556               3scale-api-python    3scale\n",
       "3        33158          25556                 3scale-api-ruby    3scale\n",
       "4        33169          25556                3scale-go-client    3scale\n",
       "...        ...            ...                             ...       ...\n",
       "14764    30872          25482              sosreport-operator  zerodayz\n",
       "14765    30863          25482                           talos  zerodayz\n",
       "14766    30838          25482                           tests  zerodayz\n",
       "14767    30839          25482                             web  zerodayz\n",
       "14768    30816          25482                        zerodayz  zerodayz\n",
       "\n",
       "[14769 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "repo_id           int64\n",
       "repo_group_id     int64\n",
       "repo_name        object\n",
       "rg_name          object\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repolist = pd.DataFrame()\n",
    "\n",
    "repo_query = salc.sql.text(f\"\"\"\n",
    "             SELECT b.repo_id,\n",
    "                a.repo_group_id,\n",
    "                b.repo_name,\n",
    "                a.rg_name\n",
    "            FROM\n",
    "                repo_groups a,\n",
    "                repo b \n",
    "            WHERE\n",
    "                a.repo_group_id = b.repo_group_id \n",
    "            ORDER BY\n",
    "                rg_name,\n",
    "                repo_name;   \n",
    "\n",
    "    \"\"\")\n",
    "\n",
    "repolist = pd.read_sql(repo_query, con=engine)\n",
    "\n",
    "display(repolist)\n",
    "\n",
    "repolist.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import time\n",
    "import traceback\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "import sqlalchemy as s\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import LatentDirichletAllocation  as LDA\n",
    "from collections import OrderedDict\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "MODEL_FILE_NAME = \"kmeans_repo_messages\"\n",
    "stemmer = nltk.stem.snowball.SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "\n",
    "def clustering_model(repo_git: str,logger,engine, session) -> None:\n",
    "\n",
    "    ngram_range = (1, 4)\n",
    "    clustering_by_content = True\n",
    "    clustering_by_mechanism = False\n",
    "\n",
    "    # define topic modeling specific parameters\n",
    "    num_topics = 8\n",
    "    num_words_per_topic = 12\n",
    "\n",
    "    tool_source = 'Clustering Worker'\n",
    "    tool_version = '0.2.0'\n",
    "    data_source = 'Augur Collected Messages'\n",
    "\n",
    "    query = session.query(Repo).filter(Repo.repo_git == repo_git)\n",
    "    repo_id = execute_session_query(query, 'one').repo_id\n",
    "\n",
    "    num_clusters = 6\n",
    "    max_df = 0.9\n",
    "    max_features = 1000\n",
    "    min_df = 0.1\n",
    "\n",
    "    get_messages_for_repo_sql = s.sql.text(\n",
    "        \"\"\"\n",
    "            SELECT\n",
    "                r.repo_group_id,\n",
    "                r.repo_id,\n",
    "                r.repo_git,\n",
    "                r.repo_name,\n",
    "                i.issue_id thread_id,\n",
    "                M.msg_text,\n",
    "                i.issue_title thread_title,\n",
    "                M.msg_id \n",
    "            FROM\n",
    "                augur_data.repo r,\n",
    "                augur_data.issues i,\n",
    "                augur_data.message M,\n",
    "                augur_data.issue_message_ref imr \n",
    "            WHERE\n",
    "                r.repo_id = i.repo_id \n",
    "                AND imr.issue_id = i.issue_id \n",
    "                AND imr.msg_id = M.msg_id \n",
    "                AND r.repo_id = :repo_id \n",
    "            UNION\n",
    "            SELECT\n",
    "                r.repo_group_id,\n",
    "                r.repo_id,\n",
    "                        r.repo_git,\n",
    "                r.repo_name,\n",
    "                pr.pull_request_id thread_id,\n",
    "                M.msg_text,\n",
    "                pr.pr_src_title thread_title,\n",
    "                M.msg_id \n",
    "            FROM\n",
    "                augur_data.repo r,\n",
    "                augur_data.pull_requests pr,\n",
    "                augur_data.message M,\n",
    "                augur_data.pull_request_message_ref prmr \n",
    "            WHERE\n",
    "                r.repo_id = pr.repo_id \n",
    "                AND prmr.pull_request_id = pr.pull_request_id \n",
    "                AND prmr.msg_id = M.msg_id \n",
    "                AND r.repo_id = :repo_id\n",
    "            \"\"\"\n",
    "    )\n",
    "    # result = db.execute(delete_points_SQL, repo_id=repo_id, min_date=min_date)\n",
    "    msg_df_cur_repo = pd.read_sql(get_messages_for_repo_sql, engine, params={\"repo_id\": repo_id})\n",
    "  \n",
    "    # check if dumped pickle file exists, if exists no need to train the model\n",
    "    if not os.path.exists(MODEL_FILE_NAME):\n",
    "        train_model(logger, engine, session, max_df, min_df, max_features, ngram_range, num_clusters, num_topics, num_words_per_topic, tool_source, tool_version, data_source)\n",
    "    else:\n",
    "        model_stats = os.stat(MODEL_FILE_NAME)\n",
    "        model_age = (time.time() - model_stats.st_mtime)\n",
    "        # if the model is more than month old, retrain it.\n",
    "        if model_age > 2000000:\n",
    "            logger.info(\"clustering model to old. Retraining the model.........\")\n",
    "            train_model(logger, engine, session, max_df, min_df, max_features, ngram_range, num_clusters, num_topics, num_words_per_topic, tool_source, tool_version, data_source)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    with open(\"kmeans_repo_messages\", 'rb') as model_file:\n",
    "        kmeans_model = pickle.load(model_file)\n",
    "\n",
    "    msg_df = msg_df_cur_repo.groupby('repo_id')['msg_text'].apply(','.join).reset_index()\n",
    "\n",
    "    logger.debug(f'messages being clustered: {msg_df}')\n",
    "\n",
    "    if msg_df.empty:\n",
    "        logger.info(\"not enough data for prediction\")\n",
    "        # self.register_task_completion(task, repo_id, 'clustering')\n",
    "        return\n",
    "\n",
    "    vocabulary = pickle.load(open(\"vocabulary\", \"rb\"))\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=max_df, max_features=max_features,\n",
    "                                       min_df=min_df, stop_words='english',\n",
    "                                       use_idf=True, tokenizer=preprocess_and_tokenize,\n",
    "                                       ngram_range=ngram_range, vocabulary=vocabulary)\n",
    "    tfidf_transformer = tfidf_vectorizer.fit(\n",
    "        msg_df['msg_text'])  # might be fitting twice, might have been used in training\n",
    "\n",
    "    # save new vocabulary ??\n",
    "    feature_matrix_cur_repo = tfidf_transformer.transform(msg_df['msg_text'])\n",
    "\n",
    "    prediction = kmeans_model.predict(feature_matrix_cur_repo)\n",
    "\n",
    "    # inserting data\n",
    "    record = {\n",
    "        'repo_id': int(repo_id),\n",
    "        'cluster_content': int(prediction[0]),\n",
    "        'cluster_mechanism': -1,\n",
    "        'tool_source': tool_source,\n",
    "        'tool_version': tool_version,\n",
    "        'data_source': data_source\n",
    "    }\n",
    "    repo_cluster_messages_obj = RepoClusterMessage(**record)\n",
    "    session.add(repo_cluster_messages_obj)\n",
    "    session.commit()\n",
    "\n",
    "    # result = db.execute(repo_cluster_messages_table.insert().values(record))\n",
    "    try:\n",
    "        lda_model = pickle.load(open(\"lda_model\", \"rb\"))\n",
    "        vocabulary = pickle.load(open(\"vocabulary_count\", \"rb\"))\n",
    "        count_vectorizer = CountVectorizer(max_df=max_df, max_features=max_features, min_df=min_df,\n",
    "                                           stop_words=\"english\", tokenizer=preprocess_and_tokenize,\n",
    "                                           vocabulary=vocabulary)\n",
    "        count_transformer = count_vectorizer.fit(\n",
    "            msg_df['msg_text'])  # might be fitting twice, might have been used in training\n",
    "\n",
    "        # save new vocabulary ??\n",
    "        count_matrix_cur_repo = count_transformer.transform(msg_df['msg_text'])\n",
    "        prediction = lda_model.transform(count_matrix_cur_repo)\n",
    "\n",
    "        for i, prob_vector in enumerate(prediction):\n",
    "            # repo_id = msg_df.loc[i]['repo_id']\n",
    "            for i, prob in enumerate(prob_vector):\n",
    "                record = {\n",
    "                    'repo_id': int(repo_id),\n",
    "                    'topic_id': i + 1,\n",
    "                    'topic_prob': prob,\n",
    "                    'tool_source': tool_source,\n",
    "                    'tool_version': tool_version,\n",
    "                    'data_source': data_source\n",
    "                }\n",
    "\n",
    "                repo_topic_object = RepoTopic(**record)\n",
    "                session.add(repo_topic_object)\n",
    "                session.commit()\n",
    "\n",
    "                    # result = db.execute(repo_topic_table.insert().values(record))\n",
    "    except Exception as e:\n",
    "        stacker = traceback.format_exc()\n",
    "        pass\n",
    "\n",
    "    # self.register_task_completion(task, repo_id, 'clustering')\n",
    "\n",
    "\n",
    "def get_tf_idf_matrix(text_list, max_df, max_features, min_df, ngram_range, logger):\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=max_df, max_features=max_features,\n",
    "                                       min_df=min_df, stop_words='english',\n",
    "                                       use_idf=True, tokenizer=preprocess_and_tokenize,\n",
    "                                       ngram_range=ngram_range)\n",
    "    tfidf_transformer = tfidf_vectorizer.fit(text_list)\n",
    "    tfidf_matrix = tfidf_transformer.transform(text_list)\n",
    "    pickle.dump(tfidf_transformer.vocabulary_, open(\"vocabulary\", 'wb'))\n",
    "    return tfidf_matrix, tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "def cluster_and_label(feature_matrix, num_clusters):\n",
    "    kmeans_model = KMeans(n_clusters=num_clusters)\n",
    "    kmeans_model.fit(feature_matrix)\n",
    "    pickle.dump(kmeans_model, open(\"kmeans_repo_messages\", 'wb'))\n",
    "    return kmeans_model.labels_.tolist()\n",
    "\n",
    "def count_func(msg):\n",
    "    blobed = TextBlob(msg)\n",
    "    counts = Counter(tag for word, tag in blobed.tags if\n",
    "                     tag not in ['NNPS', 'RBS', 'SYM', 'WP$', 'LS', 'POS', 'RP', 'RBR', 'JJS', 'UH', 'FW', 'PDT'])\n",
    "    total = sum(counts.values())\n",
    "    normalized_count = {key: value / total for key, value in counts.items()}\n",
    "    return normalized_count\n",
    "\n",
    "def preprocess_and_tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[@]\\w+', '', text)\n",
    "    text = re.sub(r'[^A-Za-z]+', ' ', text)\n",
    "\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 1]\n",
    "    stems = [stemmer.stem(t) for t in tokens]\n",
    "    return stems\n",
    "\n",
    "def train_model(logger, engine, session, max_df, min_df, max_features, ngram_range, num_clusters, num_topics, num_words_per_topic, tool_source, tool_version, data_source):\n",
    "    def visualize_labels_PCA(features, labels, annotations, num_components, title):\n",
    "        labels_color_map = {-1: \"red\"}\n",
    "        for label in labels:\n",
    "            labels_color_map[label] = [list([x / 255.0 for x in list(np.random.choice(range(256), size=3))])]\n",
    "        low_dim_data = PCA(n_components=num_components).fit_transform(features)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(20, 10))\n",
    "\n",
    "        for i, data in enumerate(low_dim_data):\n",
    "            pca_comp_1, pca_comp_2 = data\n",
    "            color = labels_color_map[labels[i]]\n",
    "            ax.scatter(pca_comp_1, pca_comp_2, c=color, label=labels[i])\n",
    "        # ax.annotate(annotations[i],(pca_comp_1, pca_comp_2))\n",
    "\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        handles_label_dict = OrderedDict(zip(labels, handles))\n",
    "        ax.legend(handles_label_dict.values(), handles_label_dict.keys())\n",
    "\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"PCA Component 1\")\n",
    "        plt.ylabel(\"PCA Component 2\")\n",
    "        # plt.show()\n",
    "        filename = labels + \"_PCA.png\"\n",
    "        plt.save_fig(filename)\n",
    "\n",
    "    get_messages_sql = s.sql.text(\n",
    "        \"\"\"\n",
    "        SELECT r.repo_group_id, r.repo_id, r.repo_git, r.repo_name, i.issue_id thread_id,m.msg_text,i.issue_title thread_title,m.msg_id\n",
    "        FROM augur_data.repo r, augur_data.issues i,\n",
    "        augur_data.message m, augur_data.issue_message_ref imr\n",
    "        WHERE r.repo_id=i.repo_id\n",
    "        AND imr.issue_id=i.issue_id\n",
    "        AND imr.msg_id=m.msg_id\n",
    "        UNION\n",
    "        SELECT r.repo_group_id, r.repo_id, r.repo_git, r.repo_name, pr.pull_request_id thread_id,m.msg_text,pr.pr_src_title thread_title,m.msg_id\n",
    "        FROM augur_data.repo r, augur_data.pull_requests pr,\n",
    "        augur_data.message m, augur_data.pull_request_message_ref prmr\n",
    "        WHERE r.repo_id=pr.repo_id\n",
    "        AND prmr.pull_request_id=pr.pull_request_id\n",
    "        AND prmr.msg_id=m.msg_id\n",
    "        \"\"\"\n",
    "    )\n",
    "    msg_df_all = pd.read_sql(get_messages_sql, engine, params={})\n",
    "\n",
    "    # select only highly active repos\n",
    "    msg_df_all = msg_df_all.groupby(\"repo_id\").filter(lambda x: len(x) > 500)\n",
    "\n",
    "    # combining all the messages in a repository to form a single doc\n",
    "    msg_df = msg_df_all.groupby('repo_id')['msg_text'].apply(','.join)\n",
    "    msg_df = msg_df.reset_index()\n",
    "\n",
    "    # dataframe summarizing total message count in a repositoryde\n",
    "    message_desc_df = msg_df_all[[\"repo_id\", \"repo_git\", \"repo_name\", \"msg_id\"]].groupby(\n",
    "        [\"repo_id\", \"repo_git\", \"repo_name\"]).agg('count').reset_index()\n",
    "    message_desc_df.columns = [\"repo_id\", \"repo_git\", \"repo_name\", \"message_count\"]\n",
    "\n",
    "    tfidf_matrix, features = get_tf_idf_matrix(msg_df['msg_text'], max_df, max_features, min_df,\n",
    "                                                    ngram_range, logger)\n",
    "    msg_df['cluster'] = cluster_and_label(tfidf_matrix, num_clusters)\n",
    "\n",
    "    # LDA - Topic Modeling\n",
    "    count_vectorizer = CountVectorizer(max_df=max_df, max_features=max_features, min_df=min_df,\n",
    "                                       stop_words=\"english\", tokenizer=preprocess_and_tokenize)\n",
    "\n",
    "    # count_matrix = count_vectorizer.fit_transform(msg_df['msg_text'])\n",
    "    count_transformer = count_vectorizer.fit(msg_df['msg_text'])\n",
    "    count_matrix = count_transformer.transform(msg_df['msg_text'])\n",
    "    pickle.dump(count_transformer.vocabulary_, open(\"vocabulary_count\", 'wb'))\n",
    "    feature_names = count_vectorizer.get_feature_names()\n",
    "\n",
    "    logger.debug(\"Calling LDA\")\n",
    "    lda_model = LDA(n_components=num_topics)\n",
    "    lda_model.fit(count_matrix)\n",
    "    # each component in lda_model.components_ represents probability distribution over words in that topic\n",
    "    topic_list = lda_model.components_\n",
    "    # Getting word probability\n",
    "    # word_prob = lda_model.exp_dirichlet_component_\n",
    "    # word probabilities\n",
    "    # lda_model does not have state variable in this library\n",
    "    # topics_terms = lda_model.state.get_lambda()\n",
    "    # topics_terms_proba = np.apply_along_axis(lambda x: x/x.sum(),1,topics_terms)\n",
    "    # word_prob = [lda_model.id2word[i] for i in range(topics_terms_proba.shape[1])]\n",
    "\n",
    "    # Site explaining main library used for parsing topics: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\n",
    "\n",
    "    # Good site for optimizing: https://medium.com/@yanlinc/how-to-build-a-lda-topic-model-using-from-text-601cdcbfd3a6\n",
    "    # Another Good Site: https://towardsdatascience.com/an-introduction-to-clustering-algorithms-in-python-123438574097\n",
    "    # https://machinelearningmastery.com/clustering-algorithms-with-python/\n",
    "\n",
    "    pickle.dump(lda_model, open(\"lda_model\", 'wb'))\n",
    "\n",
    "    ## Advance Sequence SQL\n",
    "\n",
    "    # key_sequence_words_sql = s.sql.text(\n",
    "    #                           \"\"\"\n",
    "    #       SELECT nextval('augur_data.topic_words_topic_words_id_seq'::text)\n",
    "    #       \"\"\"\n",
    "    #                               )\n",
    "\n",
    "    # twid = self.db.execute(key_sequence_words_sql)\n",
    "    # logger.info(\"twid variable is: {}\".format(twid))\n",
    "    # insert topic list into database\n",
    "    topic_id = 1\n",
    "    for topic in topic_list:\n",
    "        # twid = self.get_max_id('topic_words', 'topic_words_id') + 1\n",
    "        # logger.info(\"twid variable is: {}\".format(twid))\n",
    "        for i in topic.argsort()[:-num_words_per_topic - 1:-1]:\n",
    "            # twid+=1\n",
    "            # logger.info(\"in loop incremented twid variable is: {}\".format(twid))\n",
    "            # logger.info(\"twid variable is: {}\".format(twid))\n",
    "            record = {\n",
    "                # 'topic_words_id': twid,\n",
    "                # 'word_prob': word_prob[i],\n",
    "                'topic_id': int(topic_id),\n",
    "                'word': feature_names[i],\n",
    "                'tool_source': tool_source,\n",
    "                'tool_version': tool_version,\n",
    "                'data_source': data_source\n",
    "            }\n",
    "\n",
    "            topic_word_obj = TopicWord(**record)\n",
    "            session.add(topic_word_obj)\n",
    "            session.commit()\n",
    "\n",
    "            # result = db.execute(topic_words_table.insert().values(record))\n",
    "            \n",
    "        topic_id += 1\n",
    "\n",
    "    # insert topic list into database\n",
    "\n",
    "    # save the model and predict on each repo separately\n",
    "\n",
    "    prediction = lda_model.transform(count_matrix)\n",
    "\n",
    "    topic_model_dict_list = []\n",
    "    for i, prob_vector in enumerate(prediction):\n",
    "        topic_model_dict = {}\n",
    "        topic_model_dict['repo_id'] = msg_df.loc[i]['repo_id']\n",
    "        for i, prob in enumerate(prob_vector):\n",
    "            topic_model_dict[\"topic\" + str(i + 1)] = prob\n",
    "        topic_model_dict_list.append(topic_model_dict)\n",
    "    topic_model_df = pd.DataFrame(topic_model_dict_list)\n",
    "\n",
    "    result_content_df = topic_model_df.set_index('repo_id').join(message_desc_df.set_index('repo_id')).join(\n",
    "        msg_df.set_index('repo_id'))\n",
    "    result_content_df = result_content_df.reset_index()\n",
    "    try:\n",
    "        POS_count_dict = msg_df.apply(lambda row: count_func(row['msg_text']), axis=1)\n",
    "    except Exception as e:\n",
    "        stacker = traceback.format_exc()\n",
    "        pass\n",
    "    try:\n",
    "        msg_df_aug = pd.concat([msg_df, pd.DataFrame.from_records(POS_count_dict)], axis=1)\n",
    "    except Exception as e:\n",
    "        stacker = traceback.format_exc()\n",
    "        pass\n",
    "\n",
    "    visualize_labels_PCA(tfidf_matrix.todense(), msg_df['cluster'], msg_df['repo_id'], 2, \"tex!\")\n",
    "\n",
    "# visualize_labels_PCA(tfidf_matrix.todense(), msg_df['cluster'], msg_df['repo_id'], 2, \"MIN_DF={} and MAX_DF={} and NGRAM_RANGE={}\".format(MIN_DF, MAX_DF, NGRAM_RANGE))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
