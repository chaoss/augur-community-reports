{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the Repos Available in your Database, and What Repository Groups They Are In"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to your database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd \n",
    "import sqlalchemy as salc\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import datetime\n",
    "import json\n",
    "import sklearn \n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "with open(\"config.json\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "database_connection_string = 'postgres+psycopg2://{}:{}@{}:{}/{}'.format(config['user'], config['password'], config['host'], config['port'], config['database'])\n",
    "\n",
    "dbschema='augur_data'\n",
    "engine = salc.create_engine(\n",
    "    database_connection_string,\n",
    "    connect_args={'options': '-csearch_path={}'.format(dbschema)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Available Respositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rg_name</th>\n",
       "      <th>repo_group_id</th>\n",
       "      <th>repo_name</th>\n",
       "      <th>repo_id</th>\n",
       "      <th>forked_from</th>\n",
       "      <th>repo_archived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3scale</td>\n",
       "      <td>25433</td>\n",
       "      <td>3scale</td>\n",
       "      <td>25639</td>\n",
       "      <td>Parent not available</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3scale</td>\n",
       "      <td>25433</td>\n",
       "      <td>3scale-amp-openshift-templates</td>\n",
       "      <td>25613</td>\n",
       "      <td>Parent not available</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3scale</td>\n",
       "      <td>25433</td>\n",
       "      <td>3scale-api-python</td>\n",
       "      <td>25662</td>\n",
       "      <td>3scale-qe/3scale-api-python</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3scale</td>\n",
       "      <td>25433</td>\n",
       "      <td>3scale-api-ruby</td>\n",
       "      <td>25607</td>\n",
       "      <td>Parent not available</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3scale</td>\n",
       "      <td>25433</td>\n",
       "      <td>3scale-go-client</td>\n",
       "      <td>25643</td>\n",
       "      <td>Parent not available</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1325</th>\n",
       "      <td>quay</td>\n",
       "      <td>25430</td>\n",
       "      <td>test-cluster</td>\n",
       "      <td>25525</td>\n",
       "      <td>Parent not available</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1326</th>\n",
       "      <td>quay</td>\n",
       "      <td>25430</td>\n",
       "      <td>update-banner-job</td>\n",
       "      <td>25510</td>\n",
       "      <td>Parent not available</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1327</th>\n",
       "      <td>quay</td>\n",
       "      <td>25430</td>\n",
       "      <td>update-py2-db</td>\n",
       "      <td>25517</td>\n",
       "      <td>Parent not available</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1328</th>\n",
       "      <td>quay</td>\n",
       "      <td>25430</td>\n",
       "      <td>update-ro-keys-job</td>\n",
       "      <td>25518</td>\n",
       "      <td>Parent not available</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1329</th>\n",
       "      <td>quay</td>\n",
       "      <td>25430</td>\n",
       "      <td>zlog</td>\n",
       "      <td>25507</td>\n",
       "      <td>Parent not available</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1330 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     rg_name  repo_group_id                       repo_name  repo_id  \\\n",
       "0     3scale          25433                          3scale    25639   \n",
       "1     3scale          25433  3scale-amp-openshift-templates    25613   \n",
       "2     3scale          25433               3scale-api-python    25662   \n",
       "3     3scale          25433                 3scale-api-ruby    25607   \n",
       "4     3scale          25433                3scale-go-client    25643   \n",
       "...      ...            ...                             ...      ...   \n",
       "1325    quay          25430                    test-cluster    25525   \n",
       "1326    quay          25430               update-banner-job    25510   \n",
       "1327    quay          25430                   update-py2-db    25517   \n",
       "1328    quay          25430              update-ro-keys-job    25518   \n",
       "1329    quay          25430                            zlog    25507   \n",
       "\n",
       "                      forked_from  repo_archived  \n",
       "0            Parent not available            0.0  \n",
       "1            Parent not available            0.0  \n",
       "2     3scale-qe/3scale-api-python            0.0  \n",
       "3            Parent not available            0.0  \n",
       "4            Parent not available            0.0  \n",
       "...                           ...            ...  \n",
       "1325         Parent not available            0.0  \n",
       "1326         Parent not available            0.0  \n",
       "1327         Parent not available            0.0  \n",
       "1328         Parent not available            0.0  \n",
       "1329         Parent not available            0.0  \n",
       "\n",
       "[1330 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "rg_name           object\n",
       "repo_group_id      int64\n",
       "repo_name         object\n",
       "repo_id            int64\n",
       "forked_from       object\n",
       "repo_archived    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repolist = pd.DataFrame()\n",
    "\n",
    "repo_query = salc.sql.text(f\"\"\"\n",
    "             SELECT a.rg_name,\n",
    "                a.repo_group_id,\n",
    "                b.repo_name,\n",
    "                b.repo_id,\n",
    "                b.forked_from,\n",
    "                b.repo_archived \n",
    "            FROM\n",
    "                repo_groups a,\n",
    "                repo b \n",
    "            WHERE\n",
    "                a.repo_group_id = b.repo_group_id \n",
    "            ORDER BY\n",
    "                rg_name,\n",
    "                repo_name;   \n",
    "\n",
    "    \"\"\")\n",
    "\n",
    "repolist = pd.read_sql(repo_query, con=engine)\n",
    "\n",
    "display(repolist)\n",
    "\n",
    "repolist.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapted version of clustering-worker-tasks.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (676861824.py, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[8], line 14\u001b[0;36m\u001b[0m\n\u001b[0;31m    from scikit-learn.feature_extraction.text import TfidfVectorizer, CountVectorizer\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import time\n",
    "import traceback\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "import sqlalchemy as s\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import LatentDirichletAllocation  as LDA\n",
    "from collections import OrderedDict\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# def clustering_model(repo_git: str,logger,engine, session) -> None:\n",
    "def clustering_model(repo_git: str, engine) -> None:\n",
    "\n",
    "    ngram_range = (1, 4)\n",
    "    clustering_by_content = True\n",
    "    clustering_by_mechanism = False\n",
    "\n",
    "    # define topic modeling specific parameters\n",
    "    num_topics = 8\n",
    "    num_words_per_topic = 12\n",
    "\n",
    "    tool_source = 'Clustering Worker'\n",
    "    tool_version = '0.2.0'\n",
    "    data_source = 'Augur Collected Messages'\n",
    "\n",
    "    #query = session.query(Repo).filter(Repo.repo_git == repo_git)\n",
    "    #repo_id = execute_session_query(query, 'one').repo_id\n",
    "\n",
    "    num_clusters = 20\n",
    "    max_df = 0.9\n",
    "    max_features = 10000\n",
    "    min_df = 0.1\n",
    "\n",
    "    get_messages_for_repo_sql = s.sql.text(\n",
    "        \"\"\"\n",
    "            SELECT\n",
    "                r.repo_group_id,\n",
    "                r.repo_id,\n",
    "                r.repo_git,\n",
    "                r.repo_name,\n",
    "                i.issue_id thread_id,\n",
    "                M.msg_text,\n",
    "                i.issue_title thread_title,\n",
    "                M.msg_id \n",
    "            FROM\n",
    "                augur_data.repo r,\n",
    "                augur_data.issues i,\n",
    "                augur_data.message M,\n",
    "                augur_data.issue_message_ref imr \n",
    "            WHERE\n",
    "                r.repo_id = i.repo_id \n",
    "                AND imr.issue_id = i.issue_id \n",
    "                AND imr.msg_id = M.msg_id \n",
    "                AND r.repo_id=:repo_id\n",
    "            UNION\n",
    "            SELECT\n",
    "                r.repo_group_id,\n",
    "                r.repo_id,\n",
    "                        r.repo_git,\n",
    "                r.repo_name,\n",
    "                pr.pull_request_id thread_id,\n",
    "                M.msg_text,\n",
    "                pr.pr_src_title thread_title,\n",
    "                M.msg_id \n",
    "            FROM\n",
    "                augur_data.repo r,\n",
    "                augur_data.pull_requests pr,\n",
    "                augur_data.message M,\n",
    "                augur_data.pull_request_message_ref prmr \n",
    "            WHERE\n",
    "                r.repo_id = pr.repo_id \n",
    "                AND prmr.pull_request_id = pr.pull_request_id \n",
    "                AND prmr.msg_id = M.msg_id \n",
    "                AND r.repo_id=:repo_id\n",
    "            \"\"\"\n",
    "    )\n",
    "    # result = db.execute(delete_points_SQL, repo_id=repo_id, min_date=min_date)\n",
    "    msg_df_cur_repo = pd.read_sql(get_messages_for_repo_sql, engine, params={\"repo_id\": 25613})\n",
    "  \n",
    "    # check if dumped pickle file exists, if exists no need to train the model\n",
    "    if not os.path.exists(MODEL_FILE_NAME):\n",
    "        #train_model(engine, session, max_df, min_df, max_features, ngram_range, num_clusters, num_topics, num_words_per_topic, tool_source, tool_version, data_source)\n",
    "        train_model(engine, max_df, min_df, max_features, ngram_range, num_clusters, num_topics, num_words_per_topic, tool_source, tool_version, data_source)\n",
    "    else:\n",
    "        model_stats = os.stat(MODEL_FILE_NAME)\n",
    "        model_age = (time.time() - model_stats.st_mtime)\n",
    "        # if the model is more than month old, retrain it.\n",
    "        if model_age > 2000000:\n",
    "            # train_model(logger, engine, session, max_df, min_df, max_features, ngram_range, num_clusters, num_topics, num_words_per_topic, tool_source, tool_version, data_source)\n",
    "            train_model(engine, max_df, min_df, max_features, ngram_range, num_clusters, num_topics, num_words_per_topic, tool_source, tool_version, data_source)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    with open(\"kmeans_repo_messages\", 'rb') as model_file:\n",
    "        kmeans_model = pickle.load(model_file)\n",
    "\n",
    "    msg_df = msg_df_cur_repo.groupby('repo_id')['msg_text'].apply(','.join).reset_index()\n",
    "\n",
    "    # logger.debug(f'messages being clustered: {msg_df}')\n",
    "\n",
    "    if msg_df.empty:\n",
    "        # logger.info(\"not enough data for prediction\")\n",
    "        # self.register_task_completion(task, repo_id, 'clustering')\n",
    "        return\n",
    "\n",
    "    vocabulary = pickle.load(open(\"vocabulary\", \"rb\"))\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=max_df, max_features=max_features,\n",
    "                                       min_df=min_df, stop_words='english',\n",
    "                                       use_idf=True, tokenizer=preprocess_and_tokenize,\n",
    "                                       ngram_range=ngram_range, vocabulary=vocabulary)\n",
    "    tfidf_transformer = tfidf_vectorizer.fit(\n",
    "        msg_df['msg_text'])  # might be fitting twice, might have been used in training\n",
    "\n",
    "    # save new vocabulary ??\n",
    "    feature_matrix_cur_repo = tfidf_transformer.transform(msg_df['msg_text'])\n",
    "\n",
    "    prediction = kmeans_model.predict(feature_matrix_cur_repo)\n",
    "\n",
    "    # inserting data\n",
    "    record = {\n",
    "        'repo_id': int(25613),\n",
    "        'cluster_content': int(prediction[0]),\n",
    "        'cluster_mechanism': -1,\n",
    "        'tool_source': tool_source,\n",
    "        'tool_version': tool_version,\n",
    "        'data_source': data_source\n",
    "    }\n",
    "    #repo_cluster_messages_obj = RepoClusterMessage(**record)\n",
    "    #session.add(repo_cluster_messages_obj)\n",
    "    #session.commit()\n",
    "\n",
    "    # result = db.execute(repo_cluster_messages_table.insert().values(record))\n",
    "    try:\n",
    "        lda_model = pickle.load(open(\"lda_model\", \"rb\"))\n",
    "        vocabulary = pickle.load(open(\"vocabulary_count\", \"rb\"))\n",
    "        count_vectorizer = CountVectorizer(max_df=max_df, max_features=max_features, min_df=min_df,\n",
    "                                           stop_words=\"english\", tokenizer=preprocess_and_tokenize,\n",
    "                                           vocabulary=vocabulary)\n",
    "        count_transformer = count_vectorizer.fit(\n",
    "            msg_df['msg_text'])  # might be fitting twice, might have been used in training\n",
    "\n",
    "        # save new vocabulary ??\n",
    "        count_matrix_cur_repo = count_transformer.transform(msg_df['msg_text'])\n",
    "        prediction = lda_model.transform(count_matrix_cur_repo)\n",
    "\n",
    "        for i, prob_vector in enumerate(prediction):\n",
    "            # repo_id = msg_df.loc[i]['repo_id']\n",
    "            for i, prob in enumerate(prob_vector):\n",
    "                record = {\n",
    "                    'repo_id': int(repo_id),\n",
    "                    'topic_id': i + 1,\n",
    "                    'topic_prob': prob,\n",
    "                    'tool_source': tool_source,\n",
    "                    'tool_version': tool_version,\n",
    "                    'data_source': data_source\n",
    "                }\n",
    "\n",
    "                # repo_topic_object = RepoTopic(**record)\n",
    "                #session.add(repo_topic_object)\n",
    "                #session.commit()\n",
    "\n",
    "                    # result = db.execute(repo_topic_table.insert().values(record))\n",
    "    except Exception as e:\n",
    "        stacker = traceback.format_exc()\n",
    "        pass\n",
    "\n",
    "    # self.register_task_completion(task, repo_id, 'clustering')\n",
    "\n",
    "\n",
    "def get_tf_idf_matrix(text_list, max_df, max_features, min_df, ngram_range):\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=max_df, max_features=max_features,\n",
    "                                       min_df=min_df, stop_words='english',\n",
    "                                       use_idf=True, tokenizer=preprocess_and_tokenize,\n",
    "                                       ngram_range=ngram_range)\n",
    "    tfidf_transformer = tfidf_vectorizer.fit(text_list)\n",
    "    tfidf_matrix = tfidf_transformer.transform(text_list)\n",
    "    pickle.dump(tfidf_transformer.vocabulary_, open(\"vocabulary\", 'wb'))\n",
    "    return tfidf_matrix, tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "def cluster_and_label(feature_matrix, num_clusters):\n",
    "    kmeans_model = KMeans(n_clusters=num_clusters)\n",
    "    kmeans_model.fit(feature_matrix)\n",
    "    pickle.dump(kmeans_model, open(\"kmeans_repo_messages\", 'wb'))\n",
    "    return kmeans_model.labels_.tolist()\n",
    "\n",
    "def count_func(msg):\n",
    "    blobed = TextBlob(msg)\n",
    "    counts = Counter(tag for word, tag in blobed.tags if\n",
    "                     tag not in ['NNPS', 'RBS', 'SYM', 'WP$', 'LS', 'POS', 'RP', 'RBR', 'JJS', 'UH', 'FW', 'PDT'])\n",
    "    total = sum(counts.values())\n",
    "    normalized_count = {key: value / total for key, value in counts.items()}\n",
    "    return normalized_count\n",
    "\n",
    "def preprocess_and_tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[@]\\w+', '', text)\n",
    "    text = re.sub(r'[^A-Za-z]+', ' ', text)\n",
    "\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 1]\n",
    "    stems = [stemmer.stem(t) for t in tokens]\n",
    "    return stems\n",
    "\n",
    "def train_model(engine, max_df, min_df, max_features, ngram_range, num_clusters, num_topics, num_words_per_topic, tool_source, tool_version, data_source):\n",
    "    def visualize_labels_PCA(features, labels, annotations, num_components, title):\n",
    "        labels_color_map = {-1: \"red\"}\n",
    "        for label in labels:\n",
    "            labels_color_map[label] = [list([x / 255.0 for x in list(np.random.choice(range(256), size=3))])]\n",
    "        low_dim_data = PCA(n_components=num_components).fit_transform(features)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(20, 10))\n",
    "\n",
    "        for i, data in enumerate(low_dim_data):\n",
    "            pca_comp_1, pca_comp_2 = data\n",
    "            color = labels_color_map[labels[i]]\n",
    "            ax.scatter(pca_comp_1, pca_comp_2, c=color, label=labels[i])\n",
    "        # ax.annotate(annotations[i],(pca_comp_1, pca_comp_2))\n",
    "\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        handles_label_dict = OrderedDict(zip(labels, handles))\n",
    "        ax.legend(handles_label_dict.values(), handles_label_dict.keys())\n",
    "\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"PCA Component 1\")\n",
    "        plt.ylabel(\"PCA Component 2\")\n",
    "        # plt.show()\n",
    "        filename = labels + \"_PCA.png\"\n",
    "        plt.save_fig(filename)\n",
    "\n",
    "    get_messages_sql = s.sql.text(\n",
    "        \"\"\"\n",
    "        SELECT r.repo_group_id, r.repo_id, r.repo_git, r.repo_name, i.issue_id thread_id,m.msg_text,i.issue_title thread_title,m.msg_id\n",
    "        FROM augur_data.repo r, augur_data.issues i,\n",
    "        augur_data.message m, augur_data.issue_message_ref imr\n",
    "        WHERE r.repo_id=i.repo_id\n",
    "        AND imr.issue_id=i.issue_id\n",
    "        AND imr.msg_id=m.msg_id\n",
    "        UNION\n",
    "        SELECT r.repo_group_id, r.repo_id, r.repo_git, r.repo_name, pr.pull_request_id thread_id,m.msg_text,pr.pr_src_title thread_title,m.msg_id\n",
    "        FROM augur_data.repo r, augur_data.pull_requests pr,\n",
    "        augur_data.message m, augur_data.pull_request_message_ref prmr\n",
    "        WHERE r.repo_id=pr.repo_id\n",
    "        AND prmr.pull_request_id=pr.pull_request_id\n",
    "        AND prmr.msg_id=m.msg_id\n",
    "        \"\"\"\n",
    "    )\n",
    "    msg_df_all = pd.read_sql(get_messages_sql, engine, params={})\n",
    "\n",
    "    # select only highly active repos\n",
    "    msg_df_all = msg_df_all.groupby(\"repo_id\").filter(lambda x: len(x) > 100)\n",
    "\n",
    "    # combining all the messages in a repository to form a single doc\n",
    "    msg_df = msg_df_all.groupby('repo_id')['msg_text'].apply(','.join)\n",
    "    msg_df = msg_df.reset_index()\n",
    "\n",
    "    # dataframe summarizing total message count in a repositoryde\n",
    "    message_desc_df = msg_df_all[[\"repo_id\", \"repo_git\", \"repo_name\", \"msg_id\"]].groupby(\n",
    "        [\"repo_id\", \"repo_git\", \"repo_name\"]).agg('count').reset_index()\n",
    "    message_desc_df.columns = [\"repo_id\", \"repo_git\", \"repo_name\", \"message_count\"]\n",
    "\n",
    "    tfidf_matrix, features = get_tf_idf_matrix(msg_df['msg_text'], max_df, max_features, min_df,\n",
    "                                                    ngram_range)\n",
    "    msg_df['cluster'] = cluster_and_label(tfidf_matrix, num_clusters)\n",
    "\n",
    "    # LDA - Topic Modeling\n",
    "    count_vectorizer = CountVectorizer(max_df=max_df, max_features=max_features, min_df=min_df,\n",
    "                                       stop_words=\"english\", tokenizer=preprocess_and_tokenize)\n",
    "\n",
    "    # count_matrix = count_vectorizer.fit_transform(msg_df['msg_text'])\n",
    "    count_transformer = count_vectorizer.fit(msg_df['msg_text'])\n",
    "    count_matrix = count_transformer.transform(msg_df['msg_text'])\n",
    "    pickle.dump(count_transformer.vocabulary_, open(\"vocabulary_count\", 'wb'))\n",
    "    feature_names = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "    # logger.debug(\"Calling LDA\")\n",
    "    lda_model = LDA(n_components=num_topics)\n",
    "    lda_model.fit(count_matrix)\n",
    "    # each component in lda_model.components_ represents probability distribution over words in that topic\n",
    "    topic_list = lda_model.components_\n",
    "    # Getting word probability\n",
    "    # word_prob = lda_model.exp_dirichlet_component_\n",
    "    # word probabilities\n",
    "    # lda_model does not have state variable in this library\n",
    "    # topics_terms = lda_model.state.get_lambda()\n",
    "    # topics_terms_proba = np.apply_along_axis(lambda x: x/x.sum(),1,topics_terms)\n",
    "    # word_prob = [lda_model.id2word[i] for i in range(topics_terms_proba.shape[1])]\n",
    "\n",
    "    # Site explaining main library used for parsing topics: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\n",
    "\n",
    "    # Good site for optimizing: https://medium.com/@yanlinc/how-to-build-a-lda-topic-model-using-from-text-601cdcbfd3a6\n",
    "    # Another Good Site: https://towardsdatascience.com/an-introduction-to-clustering-algorithms-in-python-123438574097\n",
    "    # https://machinelearningmastery.com/clustering-algorithms-with-python/\n",
    "\n",
    "    pickle.dump(lda_model, open(\"lda_model\", 'wb'))\n",
    "\n",
    "    ## Advance Sequence SQL\n",
    "\n",
    "    # key_sequence_words_sql = s.sql.text(\n",
    "    #                           \"\"\"\n",
    "    #       SELECT nextval('augur_data.topic_words_topic_words_id_seq'::text)\n",
    "    #       \"\"\"\n",
    "    #                               )\n",
    "\n",
    "    # twid = self.db.execute(key_sequence_words_sql)\n",
    "    # logger.info(\"twid variable is: {}\".format(twid))\n",
    "    # insert topic list into database\n",
    "    topic_id = 1\n",
    "    for topic in topic_list:\n",
    "        # twid = self.get_max_id('topic_words', 'topic_words_id') + 1\n",
    "        # logger.info(\"twid variable is: {}\".format(twid))\n",
    "        for i in topic.argsort()[:-num_words_per_topic - 1:-1]:\n",
    "            # twid+=1\n",
    "            # logger.info(\"in loop incremented twid variable is: {}\".format(twid))\n",
    "            # logger.info(\"twid variable is: {}\".format(twid))\n",
    "            record = {\n",
    "                # 'topic_words_id': twid,\n",
    "                # 'word_prob': word_prob[i],\n",
    "                'topic_id': int(topic_id),\n",
    "                'word': feature_names[i],\n",
    "                'tool_source': tool_source,\n",
    "                'tool_version': tool_version,\n",
    "                'data_source': data_source\n",
    "            }\n",
    "\n",
    "            # topic_word_obj = TopicWord(**record)\n",
    "            #session.add(topic_word_obj)\n",
    "            #session.commit()\n",
    "\n",
    "            # result = db.execute(topic_words_table.insert().values(record))\n",
    "            \n",
    "        topic_id += 1\n",
    "\n",
    "    # insert topic list into database\n",
    "\n",
    "    # save the model and predict on each repo separately\n",
    "\n",
    "    prediction = lda_model.transform(count_matrix)\n",
    "\n",
    "    topic_model_dict_list = []\n",
    "    for i, prob_vector in enumerate(prediction):\n",
    "        topic_model_dict = {}\n",
    "        topic_model_dict['repo_id'] = msg_df.loc[i]['repo_id']\n",
    "        for i, prob in enumerate(prob_vector):\n",
    "            topic_model_dict[\"topic\" + str(i + 1)] = prob\n",
    "        topic_model_dict_list.append(topic_model_dict)\n",
    "    topic_model_df = pd.DataFrame(topic_model_dict_list)\n",
    "\n",
    "    result_content_df = topic_model_df.set_index('repo_id').join(message_desc_df.set_index('repo_id')).join(\n",
    "        msg_df.set_index('repo_id'))\n",
    "    result_content_df = result_content_df.reset_index()\n",
    "    try:\n",
    "        POS_count_dict = msg_df.apply(lambda row: count_func(row['msg_text']), axis=1)\n",
    "    except Exception as e:\n",
    "        stacker = traceback.format_exc()\n",
    "        pass\n",
    "    try:\n",
    "        msg_df_aug = pd.concat([msg_df, pd.DataFrame.from_records(POS_count_dict)], axis=1)\n",
    "    except Exception as e:\n",
    "        stacker = traceback.format_exc()\n",
    "        pass\n",
    "\n",
    "    visualize_labels_PCA(tfidf_matrix.todense(), msg_df['cluster'], msg_df['repo_id'], 2, \"tex!\")\n",
    "\n",
    "# visualize_labels_PCA(tfidf_matrix.todense(), msg_df['cluster'], msg_df['repo_id'], 2, \"MIN_DF={} and MAX_DF={} and NGRAM_RANGE={}\".format(MIN_DF, MAX_DF, NGRAM_RANGE))\n",
    "\n",
    "\n",
    "MODEL_FILE_NAME = \"kmeans_repo_messages\"\n",
    "stemmer = nltk.stem.snowball.SnowballStemmer(\"english\")\n",
    "\n",
    "#clustering_model(\"https://github.com/chaoss/augur\", engine, session)\n",
    "clustering_model(\"https://github.com/chaoss/augur\", engine)\n",
    "display(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
