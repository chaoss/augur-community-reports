{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Contributor Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd \n",
    "import sqlalchemy as salc\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import datetime\n",
    "import json\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "with open(\"config.json\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "database_connection_string = 'postgresql+psycopg2://{}:{}@{}:{}/{}'.format(config['user'], config['password'], config['host'], config['port'], config['database'])\n",
    "\n",
    "dbschema='augur_data'\n",
    "engine = salc.create_engine(\n",
    "    database_connection_string,\n",
    "    connect_args={'options': '-csearch_path={}'.format(dbschema)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repo Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declare all repo ids you would like to produce charts for\n",
    "repo_set = {25766, 25768, 25765}\n",
    "#25766 - Linkerd/linkerd\n",
    "#25768 - istio/istio\n",
    "#25765 - etcd-io/etcd \n",
    "\n",
    "#can be set as 'competitors' or 'repo'\n",
    "#'competitors' will group graphs by type, so it is easy to compare across repos\n",
    "# 'repo' will group graphs by repo so it is easy to look at all the contributor data for each repo\n",
    "display_grouping = 'repo'\n",
    "\n",
    "#if display_grouping is set to 'competitors', enter the repo ids you do no want to alias, if 'display_grouping' is set to repo the list will not effect anything\n",
    "not_aliased_repos = [25766, 25768, 25765]\n",
    "\n",
    "#group_by can be set as 'month' or 'year'\n",
    "group_by = 'month'\n",
    "\n",
    "#requirements for a contributor to be considered a repeat contributor\n",
    "time = 365\n",
    "num_contributions_required = 5\n",
    "\n",
    "#specify dates for filtering\n",
    "#if the end_date is in the future, the end_date will default to the current_date\n",
    "begin_date = '2022-09-01'\n",
    "end_date = '2024-03-31'\n",
    "\n",
    "save_files = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 3, 4, 5)\n"
     ]
    }
   ],
   "source": [
    "#create tuple that contains all the contributor rankings needed\n",
    "rank_list = []\n",
    "for num in range(1, num_contributions_required + 1):\n",
    "    rank_list.append(num)\n",
    "rank_tuple = tuple(rank_list)\n",
    "print(rank_tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Contributor and Month Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2023.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2023.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2023.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2023.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2023.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2023.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2024.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2024.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2024.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      year  month\n",
       "0   2022.0    9.0\n",
       "1   2022.0   10.0\n",
       "2   2022.0   11.0\n",
       "3   2022.0   12.0\n",
       "4   2023.0    1.0\n",
       "5   2023.0    2.0\n",
       "6   2023.0    3.0\n",
       "7   2023.0    4.0\n",
       "8   2023.0    5.0\n",
       "9   2023.0    6.0\n",
       "10  2023.0    7.0\n",
       "11  2023.0    8.0\n",
       "12  2023.0    9.0\n",
       "13  2023.0   10.0\n",
       "14  2023.0   11.0\n",
       "15  2023.0   12.0\n",
       "16  2024.0    1.0\n",
       "17  2024.0    2.0\n",
       "18  2024.0    3.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cntrb_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>repo_id</th>\n",
       "      <th>repo_name</th>\n",
       "      <th>full_name</th>\n",
       "      <th>login</th>\n",
       "      <th>action</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [cntrb_id, created_at, month, year, repo_id, repo_name, full_name, login, action, rank]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "for repo_id in repo_set: \n",
    "\n",
    "    pr_query = salc.sql.text(f\"\"\"        \n",
    "    SELECT * FROM (\n",
    "        SELECT ID AS\n",
    "            cntrb_id,\n",
    "            A.created_at AS created_at,\n",
    "            date_part('month', A.created_at::DATE) AS month,\n",
    "            date_part('year', A.created_at::DATE) AS year,\n",
    "            A.repo_id,\n",
    "            repo_name,\n",
    "            full_name,\n",
    "            login,\n",
    "        ACTION,\n",
    "        rank() OVER (\n",
    "                PARTITION BY id\n",
    "                ORDER BY A.created_at ASC\n",
    "            )\n",
    "        FROM\n",
    "            (\n",
    "                (\n",
    "                SELECT\n",
    "                    canonical_id AS ID,\n",
    "                    created_at AS created_at,\n",
    "                    repo_id,\n",
    "                    'issue_opened' AS ACTION,\n",
    "                    contributors.cntrb_full_name AS full_name,\n",
    "                    contributors.cntrb_login AS login \n",
    "                FROM\n",
    "                    augur_data.issues\n",
    "                    LEFT OUTER JOIN augur_data.contributors ON contributors.cntrb_id = issues.reporter_id\n",
    "                    LEFT OUTER JOIN ( SELECT DISTINCT ON ( cntrb_canonical ) cntrb_full_name, cntrb_canonical AS canonical_email, data_collection_date, cntrb_id AS canonical_id \n",
    "                    FROM augur_data.contributors WHERE cntrb_canonical = cntrb_email ORDER BY cntrb_canonical \n",
    "                    ) canonical_full_names ON canonical_full_names.canonical_email = contributors.cntrb_canonical \n",
    "                WHERE\n",
    "                    repo_id = {repo_id}\n",
    "                    AND pull_request IS NULL \n",
    "                GROUP BY\n",
    "                    canonical_id,\n",
    "                    repo_id,\n",
    "                    issues.created_at,\n",
    "                    contributors.cntrb_full_name,\n",
    "                    contributors.cntrb_login \n",
    "                ) UNION ALL\n",
    "                (\n",
    "                SELECT\n",
    "                    canonical_id AS ID,\n",
    "                    TO_TIMESTAMP( cmt_author_date, 'YYYY-MM-DD' ) AS created_at,\n",
    "                    repo_id,\n",
    "                    'commit' AS ACTION,\n",
    "                    contributors.cntrb_full_name AS full_name,\n",
    "                    contributors.cntrb_login AS login \n",
    "                FROM\n",
    "                    augur_data.commits\n",
    "                    LEFT OUTER JOIN augur_data.contributors ON cntrb_email = cmt_author_email\n",
    "                    LEFT OUTER JOIN ( SELECT DISTINCT ON ( cntrb_canonical ) cntrb_full_name, cntrb_canonical AS canonical_email, data_collection_date, cntrb_id AS canonical_id \n",
    "                    FROM augur_data.contributors WHERE cntrb_canonical = cntrb_email ORDER BY cntrb_canonical \n",
    "                    ) canonical_full_names ON canonical_full_names.canonical_email = contributors.cntrb_canonical \n",
    "                WHERE\n",
    "                    repo_id = {repo_id} \n",
    "                GROUP BY\n",
    "                    repo_id,\n",
    "                    canonical_email,\n",
    "                    canonical_id,\n",
    "                    commits.cmt_author_date,\n",
    "                    contributors.cntrb_full_name,\n",
    "                    contributors.cntrb_login \n",
    "                ) UNION ALL\n",
    "                (\n",
    "                SELECT\n",
    "                    message.cntrb_id AS ID,\n",
    "                    created_at AS created_at,\n",
    "                    commits.repo_id,\n",
    "                    'commit_comment' AS ACTION,\n",
    "                    contributors.cntrb_full_name AS full_name,\n",
    "                    contributors.cntrb_login AS login\n",
    "      \n",
    "                FROM\n",
    "                    augur_data.commit_comment_ref,\n",
    "                    augur_data.commits,\n",
    "                    augur_data.message\n",
    "                    LEFT OUTER JOIN augur_data.contributors ON contributors.cntrb_id = message.cntrb_id\n",
    "                    LEFT OUTER JOIN ( SELECT DISTINCT ON ( cntrb_canonical ) cntrb_full_name, cntrb_canonical AS canonical_email, data_collection_date, cntrb_id AS canonical_id \n",
    "                    FROM augur_data.contributors WHERE cntrb_canonical = cntrb_email ORDER BY cntrb_canonical \n",
    "                    ) canonical_full_names ON canonical_full_names.canonical_email = contributors.cntrb_canonical \n",
    "                WHERE\n",
    "                    commits.cmt_id = commit_comment_ref.cmt_id \n",
    "                    AND commits.repo_id = {repo_id} \n",
    "                    AND commit_comment_ref.msg_id = message.msg_id\n",
    " \n",
    "                GROUP BY\n",
    "                    ID,\n",
    "                    commits.repo_id,\n",
    "                    commit_comment_ref.created_at,\n",
    "                    contributors.cntrb_full_name,\n",
    "                    contributors.cntrb_login\n",
    "                ) UNION ALL\n",
    "                (\n",
    "                SELECT\n",
    "                    issue_events.cntrb_id AS ID,\n",
    "                    issue_events.created_at AS created_at,\n",
    "                    issues.repo_id,\n",
    "                    'issue_closed' AS ACTION,\n",
    "                    contributors.cntrb_full_name AS full_name,\n",
    "                    contributors.cntrb_login AS login \n",
    "                FROM\n",
    "                    augur_data.issues,\n",
    "                    augur_data.issue_events\n",
    "                    LEFT OUTER JOIN augur_data.contributors ON contributors.cntrb_id = issue_events.cntrb_id\n",
    "                    LEFT OUTER JOIN ( SELECT DISTINCT ON ( cntrb_canonical ) cntrb_full_name, cntrb_canonical AS canonical_email, data_collection_date, cntrb_id AS canonical_id \n",
    "                    FROM augur_data.contributors WHERE cntrb_canonical = cntrb_email ORDER BY cntrb_canonical \n",
    "                    ) canonical_full_names ON canonical_full_names.canonical_email = contributors.cntrb_canonical \n",
    "                WHERE\n",
    "                    issues.repo_id = {repo_id} \n",
    "                    AND issues.issue_id = issue_events.issue_id \n",
    "                    AND issues.pull_request IS NULL \n",
    "                    AND issue_events.cntrb_id IS NOT NULL \n",
    "                    AND ACTION = 'closed' \n",
    "                GROUP BY\n",
    "                    issue_events.cntrb_id,\n",
    "                    issues.repo_id,\n",
    "                    issue_events.created_at,\n",
    "                    contributors.cntrb_full_name,\n",
    "                    contributors.cntrb_login \n",
    "                ) UNION ALL\n",
    "                (\n",
    "                SELECT\n",
    "                    pr_augur_contributor_id AS ID,\n",
    "                    pr_created_at AS created_at,\n",
    "                    repo_id,\n",
    "                    'open_pull_request' AS ACTION,\n",
    "                    contributors.cntrb_full_name AS full_name,\n",
    "                    contributors.cntrb_login AS login \n",
    "                FROM\n",
    "                    augur_data.pull_requests\n",
    "                    LEFT OUTER JOIN augur_data.contributors ON pull_requests.pr_augur_contributor_id = contributors.cntrb_id\n",
    "                    LEFT OUTER JOIN ( SELECT DISTINCT ON ( cntrb_canonical ) cntrb_full_name, cntrb_canonical AS canonical_email, data_collection_date, cntrb_id AS canonical_id \n",
    "                    FROM augur_data.contributors WHERE cntrb_canonical = cntrb_email ORDER BY cntrb_canonical \n",
    "                    ) canonical_full_names ON canonical_full_names.canonical_email = contributors.cntrb_canonical \n",
    "                WHERE\n",
    "                    pull_requests.repo_id = {repo_id} \n",
    "                GROUP BY\n",
    "                    pull_requests.pr_augur_contributor_id,\n",
    "                    pull_requests.repo_id,\n",
    "                    pull_requests.pr_created_at,\n",
    "                    contributors.cntrb_full_name,\n",
    "                    contributors.cntrb_login \n",
    "                ) UNION ALL\n",
    "                (\n",
    "                SELECT\n",
    "                    message.cntrb_id AS ID,\n",
    "                    msg_timestamp AS created_at,\n",
    "                    pull_requests.repo_id,\n",
    "                    'pull_request_comment' AS ACTION,\n",
    "                    contributors.cntrb_full_name AS full_name,\n",
    "                    contributors.cntrb_login AS login \n",
    "                FROM\n",
    "                    augur_data.pull_requests,\n",
    "                    augur_data.pull_request_message_ref,\n",
    "                    augur_data.message\n",
    "                    LEFT OUTER JOIN augur_data.contributors ON contributors.cntrb_id = message.cntrb_id\n",
    "                    LEFT OUTER JOIN ( SELECT DISTINCT ON ( cntrb_canonical ) cntrb_full_name, cntrb_canonical AS canonical_email, data_collection_date, cntrb_id AS canonical_id \n",
    "                    FROM augur_data.contributors WHERE cntrb_canonical = cntrb_email ORDER BY cntrb_canonical \n",
    "                    ) canonical_full_names ON canonical_full_names.canonical_email = contributors.cntrb_canonical \n",
    "                WHERE\n",
    "                    pull_requests.repo_id = {repo_id}\n",
    "                    AND pull_request_message_ref.pull_request_id = pull_requests.pull_request_id \n",
    "                    AND pull_request_message_ref.msg_id = message.msg_id \n",
    "                GROUP BY\n",
    "                    message.cntrb_id,\n",
    "                    pull_requests.repo_id,\n",
    "                    message.msg_timestamp,\n",
    "                    contributors.cntrb_full_name,\n",
    "                    contributors.cntrb_login \n",
    "                ) UNION ALL\n",
    "                (\n",
    "                SELECT\n",
    "                    issues.reporter_id AS ID,\n",
    "                    msg_timestamp AS created_at,\n",
    "                    augur_data.issues.repo_id,\n",
    "                    'issue_comment' AS ACTION,\n",
    "                    contributors.cntrb_full_name AS full_name,\n",
    "                    contributors.cntrb_login AS login \n",
    "                FROM\n",
    "                    issues,\n",
    "                    issue_message_ref,\n",
    "                    message\n",
    "                    LEFT OUTER JOIN augur_data.contributors ON contributors.cntrb_id = message.cntrb_id\n",
    "                    LEFT OUTER JOIN ( SELECT DISTINCT ON ( cntrb_canonical ) cntrb_full_name, cntrb_canonical AS canonical_email, data_collection_date, cntrb_id AS canonical_id \n",
    "                    FROM augur_data.contributors WHERE cntrb_canonical = cntrb_email ORDER BY cntrb_canonical \n",
    "                    ) canonical_full_names ON canonical_full_names.canonical_email = contributors.cntrb_canonical \n",
    "                WHERE\n",
    "                    issues.repo_id = {repo_id}\n",
    "                    AND issue_message_ref.msg_id = message.msg_id \n",
    "                    AND issues.issue_id = issue_message_ref.issue_id\n",
    "                    AND issues.pull_request_id = NULL\n",
    "                GROUP BY\n",
    "                    issues.reporter_id,\n",
    "                    issues.repo_id,\n",
    "                    message.msg_timestamp,\n",
    "                    contributors.cntrb_full_name,\n",
    "                    contributors.cntrb_login \n",
    "                ) \n",
    "            ) A,\n",
    "            repo \n",
    "        WHERE\n",
    "        ID IS NOT NULL \n",
    "            AND A.repo_id = repo.repo_id \n",
    "        GROUP BY\n",
    "            A.ID,\n",
    "            A.repo_id,\n",
    "            A.ACTION,\n",
    "            A.created_at,\n",
    "            repo.repo_name,\n",
    "            A.full_name,\n",
    "            A.login\n",
    "        ORDER BY \n",
    "            cntrb_id\n",
    "        ) b\n",
    "        WHERE RANK IN {rank_tuple}\n",
    "\"\"\")\n",
    "    df_first_repo = pd.read_sql(pr_query, con=engine)\n",
    "    if not df.empty: \n",
    "        df = pd.concat([df, df_first_repo]) \n",
    "    else: \n",
    "        # first repo\n",
    "        df = df_first_repo\n",
    "\n",
    "#end_date = pd.to_datetime(end_date)\n",
    "#current_time = datetime.datetime.now()\n",
    "#if end_date > current_time:\n",
    "#    end_date = current_time\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "months_df = pd.DataFrame()\n",
    "\n",
    "#months_query makes a df of years and months, this is used to fill the months with no data in the visualizaitons\n",
    "months_query = salc.sql.text(f\"\"\"        \n",
    "  SELECT\n",
    "            *\n",
    "        FROM\n",
    "        (\n",
    "        SELECT\n",
    "            date_part( 'year', created_month :: DATE ) AS year,\n",
    "            date_part( 'month', created_month :: DATE ) AS MONTH\n",
    "        FROM\n",
    "            (SELECT * FROM ( SELECT created_month :: DATE FROM generate_series (TIMESTAMP '{begin_date}', TIMESTAMP '{end_date}', INTERVAL '1 month' ) created_month ) d ) x \n",
    "        ) y\n",
    "\"\"\")\n",
    "months_df = pd.read_sql(months_query, con=engine)\n",
    "\n",
    "display(months_df)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Bots and Only Keep Actions in Actions List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[~df['full_name'].str.contains('bot', na=False)]\n",
    "df = df.loc[~df['login'].str.contains('bot', na=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Date Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add yearmonths to contributor\n",
    "df[['month', 'year']] = df[['month', 'year']].astype(int).astype(str)\n",
    "df['yearmonth'] = df['month'] + '/' + df['year']\n",
    "df['yearmonth'] = pd.to_datetime(df['yearmonth'])\n",
    "\n",
    "#add yearmonths to months_df\n",
    "months_df[['year','month']] = months_df[['year','month']].astype(float).astype(int).astype(str)\n",
    "months_df['yearmonth'] = months_df['month'] + '/' + months_df['year']\n",
    "months_df['yearmonth'] = pd.to_datetime(months_df['yearmonth'])\n",
    "\n",
    "#filter months_df with begin_date and end_date, the contributor df is filtered in the visualizations\n",
    "months_df = months_df.set_index(months_df['yearmonth'])\n",
    "months_df = months_df.loc[begin_date : end_date].reset_index(drop = True)\n",
    "\n",
    "# add column with every value being one, so when the contributor df is concatenated with the months df, the filler months won't be counted in the sums\n",
    "df['new_contributors'] = 1\n",
    "\n",
    "#return the quarter in yearmonth form, when given a month and year\n",
    "def quarters(month, year):\n",
    "    if month >= 1 and month <=3:\n",
    "        return '01' + '/' + year\n",
    "    elif month >=4 and month <=6:\n",
    "        return '04' + '/' + year\n",
    "    elif month >= 5 and month <=9:\n",
    "        return '07' + '/' + year\n",
    "    elif month >= 10 and month <= 12:\n",
    "        return '10' + '/' + year\n",
    "\n",
    "#add quarters to contributor dataframe\n",
    "df['month'] = df['month'].astype(int)\n",
    "df['quarter'] = df.apply(lambda x: quarters(x['month'], x['year']), axis=1)\n",
    "df['quarter'] = pd.to_datetime(df['quarter'])\n",
    "\n",
    "#add quarters to months dataframe\n",
    "months_df['month'] = months_df['month'].astype(int)\n",
    "months_df['quarter'] = months_df.apply(lambda x: quarters(x['month'], x['year']), axis=1)\n",
    "months_df['quarter'] = pd.to_datetime(months_df['quarter'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repo Aliasing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m repo_id \u001b[38;5;129;01min\u001b[39;00m repo_set:\n\u001b[1;32m     20\u001b[0m     \n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m#find corresponding repo name from each repo_id \u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     repo_name \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrepo_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m#if competitor grouping is enabled turn all repo names, other than the ones in the 'not_aliased_repos' into an alias\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m display_grouping \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompetitors\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m repo_id \u001b[38;5;129;01min\u001b[39;00m not_aliased_repos:\n",
      "File \u001b[0;32m~/github/virtualenvs/phds/lib/python3.11/site-packages/pandas/core/indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/virtualenvs/phds/lib/python3.11/site-packages/pandas/core/indexing.py:1752\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index by location index with a non-integer key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[0;32m-> 1752\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_ixs(key, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m~/github/virtualenvs/phds/lib/python3.11/site-packages/pandas/core/indexing.py:1685\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1683\u001b[0m len_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis))\n\u001b[1;32m   1684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m len_axis \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39mlen_axis:\n\u001b[0;32m-> 1685\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle positional indexer is out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "#create a dictionairy with a number(0-26) as the key and a letter(A-Z) as the value\n",
    "#this is used to alias repos when using 'competor' display grouping is specified\n",
    "letters = []\n",
    "nums = []\n",
    "alpha = 'a'\n",
    "for i in range(0, 26): \n",
    "    letters.append(alpha) \n",
    "    alpha = chr(ord(alpha) + 1)\n",
    "    nums.append(i)\n",
    "letters = [x.upper() for x in letters]\n",
    "\n",
    "#create dict out of list of numbers and letters\n",
    "repo_alias_dict = {nums[i]: letters[i] for i in range(len(nums))}\n",
    "\n",
    "# create dict in the form {repo_id : repo_name}\n",
    "aliased_repos = []\n",
    "repo_dict = {}\n",
    "count = 0\n",
    "for repo_id in repo_set:\n",
    "    \n",
    "    #find corresponding repo name from each repo_id \n",
    "    repo_name = df.loc[df['repo_id'] == repo_id].iloc[0]['repo_name']\n",
    "    \n",
    "    #if competitor grouping is enabled turn all repo names, other than the ones in the 'not_aliased_repos' into an alias\n",
    "    if display_grouping == 'competitors' and not repo_id in not_aliased_repos:\n",
    "        repo_name =  'Repo ' + repo_alias_dict[count]\n",
    "        \n",
    "        #add repo_id to list of aliased repos, this is used for ordering\n",
    "        aliased_repos.append(repo_id)\n",
    "        count += 1\n",
    "        \n",
    "    #add repo_id and repo names as key value pairs into a dict, this is used to label the title of the visualizations\n",
    "    repo_dict.update({repo_id : repo_name})\n",
    "\n",
    "\n",
    "#gurantees that the non_aliased repos come first when display grouping is set as 'competitors'\n",
    "repo_list = not_aliased_repos + aliased_repos\n",
    "\n",
    "#gurantee that the 'repo_list' only includes repos from the main 'repo_set'\n",
    "for repo_id in repo_list:\n",
    "    if repo_id not in repo_set:\n",
    "        repo_list.remove(repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(repo_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Visualization Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import visualization libraries\n",
    "from bokeh.io import output_notebook, show, export_png\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import Label, LabelSet, ColumnDataSource, Legend\n",
    "from bokeh.palettes import Colorblind\n",
    "from bokeh.layouts import gridplot\n",
    "from bokeh.transform import cumsum\n",
    "\n",
    "from math import pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Contributors Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vertical_bar_chart(input_df, months_df, repo_id, group_by, y_axis='new_contributors', title = \"{}: {} {} Time Contributors Per {}\", required_contributions = 4, required_time = 5):\n",
    "    \n",
    "    contributor_types = ['All', 'repeat', 'drive_by']\n",
    "    ranks = [1,2]\n",
    "    \n",
    "    #determine if a list or integer is the input, and create a list\n",
    "    if type(repo_id) == type(repo_list):\n",
    "        repo_ids = repo_id\n",
    "    else:\n",
    "        repo_ids = [repo_id]\n",
    "\n",
    "    for rank in ranks:\n",
    "        for contributor_type in contributor_types:\n",
    "            #do not display these visualizations since drive-by's do not have second contributions, and the second contribution of a repeat contributor is the same thing as the all the second time contributors\n",
    "            if (rank == 2 and contributor_type == 'drive_by') or (rank == 2 and contributor_type == 'repeat'):\n",
    "                continue\n",
    "            #do not display these visualizations since drive-by's do not have second contributions, and the second contribution of a repeat contributor is the same thing as the all the second time contributors\n",
    "            for repo_id in repo_ids:\n",
    "\n",
    "                output_notebook()\n",
    "\n",
    "                #create a copy of contributor dataframe\n",
    "                driver_df = input_df.copy()\n",
    "                \n",
    "                #filter dataframe by repo_id\n",
    "                driver_df = driver_df.loc[driver_df['repo_id'] == repo_id]\n",
    "                \n",
    "                #remove first time contributors before begin date, along with their second contribution\n",
    "                mask = (driver_df['yearmonth'] < begin_date)\n",
    "                driver_df= driver_df[~driver_df['cntrb_id'].isin(driver_df.loc[mask]['cntrb_id'])]\n",
    "                \n",
    "                \n",
    "           \n",
    "                #create separate repeat_df that includes all repeat contributors\n",
    "                #then any contributor that is not in the repeat_df is a drive-by contributor\n",
    "                repeats_df = driver_df.copy()\n",
    "                \n",
    "                #discards rows other than the first and the row required to be a repeat contributor\n",
    "                repeats_df = repeats_df.loc[repeats_df['rank'].isin([1,required_contributions])]\n",
    "\n",
    "                #removes all the contributors that only have a first contirbution\n",
    "                repeats_df = repeats_df[repeats_df['cntrb_id'].isin(repeats_df.loc[driver_df['rank'] == required_contributions]['cntrb_id'])]\n",
    "                \n",
    "                #create lists of 'created_at' times for the final required contribution and the first contribution\n",
    "                repeat_list = repeats_df.loc[driver_df['rank'] == required_contributions]['created_at'].tolist()\n",
    "                first_list = repeats_df.loc[driver_df['rank'] == 1]['created_at'].tolist()\n",
    "\n",
    "                #only keep first time contributions, since those are the dates needed for visualization\n",
    "                repeats_df = repeats_df.loc[driver_df['rank'] == 1]\n",
    "\n",
    "                #create list of time differences between the final required contribution and the first contribution, and add it to the df\n",
    "                differences = []\n",
    "                for i in range(0, len(repeat_list)):\n",
    "                    time_difference = repeat_list[i] - first_list[i]\n",
    "                    total = time_difference.days * 86400 + time_difference.seconds\n",
    "                    differences.append(total)\n",
    "                repeats_df['differences'] = differences\n",
    "\n",
    "                #remove contributions who made enough contributions, but not in a short enough time\n",
    "                repeats_df = repeats_df.loc[repeats_df['differences'] <= required_time * 86400]\n",
    "                \n",
    "                \n",
    "                \n",
    "                if contributor_type == 'repeat':\n",
    "                    driver_df = repeats_df\n",
    "                    \n",
    "                    caption = \"\"\"This graph shows repeat contributors in the specified time period. Repeat contributors are contributors who have \n",
    "                    made {} or more contributions in {} days and their first contribution is in the specified time period. New contributors \n",
    "                    are individuals who make their first contribution in the specified time period.\"\"\"\n",
    "                    \n",
    "                elif contributor_type == 'drive_by':\n",
    "\n",
    "                    #create list of 'cntrb_ids' for repeat contributors\n",
    "                    repeat_cntrb_ids = repeats_df['cntrb_id'].to_list()\n",
    "        \n",
    "                    #create df with all contributors other than the ones in the repeats_df\n",
    "                    driver_df = driver_df.loc[~driver_df['cntrb_id'].isin(repeat_cntrb_ids)]\n",
    "                 \n",
    "                    #filter df so it only includes the first contribution\n",
    "                    driver_df = driver_df.loc[driver_df['rank'] == 1]\n",
    "                    \n",
    "                    caption = \"\"\"This graph shows drive by contributors in the specified time period. Drive by contributors are contributors who \n",
    "                    make less than the required {} contributions in {} days. New contributors are individuals who make their first contribution \n",
    "                    in the specified time period. Of course, then, “All drive-by’s are by definition first time contributors”. However, not all \n",
    "                    first time contributors are drive-by’s.\"\"\"\n",
    "                \n",
    "                \n",
    "                elif contributor_type == 'All':\n",
    "                    if rank == 1:\n",
    "                        #makes df with all first time contributors\n",
    "                        driver_df = driver_df.loc[driver_df['rank'] == 1]\n",
    "                        caption = \"\"\"This graph shows all the first time contributors, whether they contribute once, or contribute multiple times. \n",
    "                        New contributors are individuals who make their first contribution in the specified time period.\"\"\"\n",
    "                        \n",
    "                    if rank == 2:\n",
    "                        #creates df with all second time contributors\n",
    "                        driver_df = driver_df.loc[driver_df['rank'] == 2]\n",
    "                        caption = \"\"\"This graph shows the second contribution of all first time contributors in the specified time period.\"\"\"\n",
    "                        y_axis_label = 'Second Time Contributors'\n",
    "                \n",
    "\n",
    "\n",
    "                #filter by end_date, this is not done with the begin date filtering because a repeat contributor will look like drive-by if the second contribution is removed by end_date filtering\n",
    "                mask = (driver_df['yearmonth'] < end_date)\n",
    "                driver_df = driver_df.loc[mask]\n",
    "\n",
    "                #adds all months to driver_df so the lists of dates will include all months and years    \n",
    "                driver_df = pd.concat([driver_df, months_df])\n",
    "\n",
    "                data = pd.DataFrame()\n",
    "                if group_by == 'year': \n",
    "\n",
    "                    data['dates'] = driver_df[group_by].unique()\n",
    "\n",
    "                    #new contributor counts for y-axis\n",
    "                    data['new_contributor_counts'] = driver_df.groupby([group_by]).sum().reset_index()[y_axis]\n",
    "\n",
    "                    #used to format x-axis and title\n",
    "                    group_by_format_string = \"Year\"\n",
    "\n",
    "                elif group_by == 'quarter' or group_by == 'month':\n",
    "                    \n",
    "                    #set variables to group the data by quarter or month\n",
    "                    if group_by == 'quarter':\n",
    "                        date_column = 'quarter'\n",
    "                        group_by_format_string = \"Quarter\"\n",
    "                        \n",
    "                    elif group_by == 'month':\n",
    "                        date_column = 'yearmonth'\n",
    "                        group_by_format_string = \"Month\"\n",
    "                        \n",
    "                    #modifies the driver_df[date_column] to be a string with year and month, then finds all the unique values   \n",
    "                    data['dates'] = np.unique(np.datetime_as_string(driver_df[date_column], unit = 'M'))\n",
    "                    \n",
    "                    #new contributor counts for y-axis\n",
    "                    data['new_contributor_counts'] = driver_df.groupby([date_column]).sum().reset_index()[y_axis]\n",
    "                \n",
    "                #if the data set is large enough it will dynamically assign the width, if the data set is too small it will by default set to 870 pixel so the title fits\n",
    "                if len(data['new_contributor_counts']) >= 15:\n",
    "                    plot_width = 46 * len(data['new_contributor_counts'])\n",
    "                else:\n",
    "                    plot_width = 870\n",
    "                    \n",
    "                #create a dict convert an integer number into a word\n",
    "                #used to turn the rank into a word, so it is nicely displayed in the title\n",
    "                numbers = ['Zero', 'First', 'Second']\n",
    "                num_conversion_dict = {}\n",
    "                for i in range(1, len(numbers)):\n",
    "                    num_conversion_dict[i] = numbers[i]\n",
    "                number =  '{}'.format(num_conversion_dict[rank])\n",
    "\n",
    "                #define pot for bar chart\n",
    "                p = figure(x_range=data['dates'], plot_height=400, plot_width = plot_width, title=title.format(repo_dict[repo_id], contributor_type.capitalize(), number, group_by_format_string), \n",
    "                           y_range=(0, max(data['new_contributor_counts'])* 1.15), margin = (0, 0, 10, 0))\n",
    "                \n",
    "                p.vbar(x=data['dates'], top=data['new_contributor_counts'], width=0.8)\n",
    "\n",
    "                source = ColumnDataSource(data=dict(dates=data['dates'], new_contributor_counts=data['new_contributor_counts']))\n",
    "                \n",
    "                #add contributor_count labels to chart\n",
    "                p.add_layout(LabelSet(x='dates', y='new_contributor_counts', text='new_contributor_counts', y_offset=4,\n",
    "                          text_font_size=\"13pt\", text_color=\"black\",\n",
    "                          source=source, text_align='center'))\n",
    "\n",
    "                p.xgrid.grid_line_color = None\n",
    "                p.y_range.start = 0\n",
    "                p.axis.minor_tick_line_color = None\n",
    "                p.outline_line_color = None\n",
    "\n",
    "                p.title.align = \"center\"\n",
    "                p.title.text_font_size = \"18px\"\n",
    "\n",
    "                p.yaxis.axis_label = 'Second Time Contributors' if rank == 2 else 'New Contributors'\n",
    "                p.xaxis.axis_label = group_by_format_string \n",
    "\n",
    "                p.xaxis.axis_label_text_font_size = \"18px\"\n",
    "                p.yaxis.axis_label_text_font_size = \"16px\"\n",
    "\n",
    "                p.xaxis.major_label_text_font_size = \"16px\"\n",
    "                p.xaxis.major_label_orientation = 45.0\n",
    "\n",
    "                p.yaxis.major_label_text_font_size = \"16px\"\n",
    "\n",
    "                plot = p\n",
    "                \n",
    "                #creates plot to hold caption \n",
    "                p = figure(width = plot_width, height=200, margin = (0, 0, 0, 0))\n",
    "\n",
    "                p.add_layout(Label(\n",
    "                x = 0, # Change to shift caption left or right\n",
    "                y = 160, \n",
    "                x_units = 'screen',\n",
    "                y_units = 'screen',\n",
    "                text='{}'.format(caption.format(num_contributions_required, time)),\n",
    "                text_font = 'times', # Use same font as paper\n",
    "                text_font_size = '15pt',\n",
    "                render_mode='css'\n",
    "                ))\n",
    "                p.outline_line_color = None\n",
    "\n",
    "                caption_plot = p\n",
    "\n",
    "                #puts plots together into a grid\n",
    "                grid = gridplot([[plot], [caption_plot]])\n",
    "\n",
    "                show(grid)\n",
    "                \n",
    "                if save_files:\n",
    "                    output_file = 'images/' + 'new_contributors_stacked_bar' + '_' + contributor_type + '_' + group_by + '_' + repo_dict[repo_id] + '.png'\n",
    "                    export_png(grid, filename=output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vertical_bar_chart(df, months_df, repo_id =25502, group_by = group_by, required_contributions = num_contributions_required, required_time = time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Contributors Action Stacked Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vertical_stacked_bar_chart(input_df, months_df, repo_id, group_by, y_axis='new_contributors', title = \"{}: {} {} Time Contributors Per {}\", required_contributions = 4, required_time = 5):\n",
    "    \n",
    "    contributor_types = ['All', 'repeat', 'drive_by']\n",
    "    ranks = [1,2]\n",
    "    \n",
    "    #determine if a list or integer is the input, and create a list\n",
    "    if type(repo_id) == type(repo_list):\n",
    "        repo_ids = repo_id\n",
    "    else:\n",
    "        repo_ids = [repo_id]\n",
    "\n",
    "    for rank in ranks:\n",
    "        for contributor_type in contributor_types:\n",
    "            #do not display these visualizations since drive-by's do not have second contributions, and the second contribution of a repeat contributor is the same thing as the all the second time contributors\n",
    "            if (rank == 2 and contributor_type == 'drive_by') or (rank == 2 and contributor_type == 'repeat'):\n",
    "                continue\n",
    "            #do not display these visualizations since drive-by's do not have second contributions, and the second contribution of a repeat contributor is the same thing as the all the second time contributors\n",
    "            for repo_id in repo_ids:\n",
    "\n",
    "                output_notebook()\n",
    "\n",
    "                #create a copy of contributor dataframe\n",
    "                driver_df = input_df.copy()\n",
    "                \n",
    "                #filter dataframe by repo_id\n",
    "                driver_df = driver_df.loc[driver_df['repo_id'] == repo_id]\n",
    "                \n",
    "                #remove first time contributors before begin date, along with their second contribution\n",
    "                mask = (driver_df['yearmonth'] < begin_date)\n",
    "                driver_df= driver_df[~driver_df['cntrb_id'].isin(driver_df.loc[mask]['cntrb_id'])]\n",
    "                        \n",
    "                \n",
    "           \n",
    "                #create separate repeat_df that includes all repeat contributors\n",
    "                #then any contributor that is not in the repeat_df is a drive-by contributor\n",
    "                repeats_df = driver_df.copy()\n",
    "                \n",
    "                #discards rows other than the first and the row required to be a repeat contributor\n",
    "                repeats_df = repeats_df.loc[repeats_df['rank'].isin([1,required_contributions])]\n",
    "\n",
    "                #removes all the contributors that only have a first contirbution\n",
    "                repeats_df = repeats_df[repeats_df['cntrb_id'].isin(repeats_df.loc[driver_df['rank'] == required_contributions]['cntrb_id'])]\n",
    "                \n",
    "                #create lists of 'created_at' times for the final required contribution and the first contribution\n",
    "                repeat_list = repeats_df.loc[driver_df['rank'] == required_contributions]['created_at'].tolist()\n",
    "                first_list = repeats_df.loc[driver_df['rank'] == 1]['created_at'].tolist()\n",
    "\n",
    "                #only keep first time contributions, since those are the dates needed for visualization\n",
    "                repeats_df = repeats_df.loc[driver_df['rank'] == 1]\n",
    "\n",
    "                #create list of time differences between the final required contribution and the first contribution, and add it to the df\n",
    "                differences = []\n",
    "                for i in range(0, len(repeat_list)):\n",
    "                    time_difference = repeat_list[i] - first_list[i]\n",
    "                    total = time_difference.days * 86400 + time_difference.seconds\n",
    "                    differences.append(total)\n",
    "                repeats_df['differences'] = differences\n",
    "\n",
    "                #remove contributions who made enough contributions, but not in a short enough time\n",
    "                repeats_df = repeats_df.loc[repeats_df['differences'] <= required_time * 86400]\n",
    "                \n",
    "                \n",
    "                \n",
    "                if contributor_type == 'repeat':\n",
    "                    driver_df = repeats_df\n",
    "                    \n",
    "                    caption = \"\"\"This graph shows repeat contributors in the specified time period. Repeat contributors are contributors who have \n",
    "                    made {} or more contributions in {} days and their first contribution is in the specified time period. New contributors \n",
    "                    are individuals who make their first contribution in the specified time period.\"\"\"\n",
    "                    \n",
    "                elif contributor_type == 'drive_by':\n",
    "\n",
    "                    #create list of 'cntrb_ids' for repeat contributors\n",
    "                    repeat_cntrb_ids = repeats_df['cntrb_id'].to_list()\n",
    "        \n",
    "                    #create df with all contributors other than the ones in the repeats_df\n",
    "                    driver_df = driver_df.loc[~driver_df['cntrb_id'].isin(repeat_cntrb_ids)]\n",
    "                 \n",
    "                    #filter df so it only includes the first contribution\n",
    "                    driver_df = driver_df.loc[driver_df['rank'] == 1]\n",
    "                    \n",
    "                    caption = \"\"\"This graph shows drive by contributors in the specified time period. Drive by contributors are contributors who \n",
    "                    make less than the required {} contributions in {} days. New contributors are individuals who make their first contribution \n",
    "                    in the specified time period. Of course, then, “All drive-by’s are by definition first time contributors”. However, not all \n",
    "                    first time contributors are drive-by’s.\"\"\"\n",
    "                \n",
    "                \n",
    "                elif contributor_type == 'All':\n",
    "                    if rank == 1:\n",
    "                        #makes df with all first time contributors\n",
    "                        driver_df = driver_df.loc[driver_df['rank'] == 1]\n",
    "                        caption = \"\"\"This graph shows all the first time contributors, whether they contribute once, or contribute multiple times. \n",
    "                        New contributors are individuals who make their first contribution in the specified time period.\"\"\"\n",
    "                        \n",
    "                    if rank == 2:\n",
    "                        #creates df with all second time contributor\n",
    "                        driver_df = driver_df.loc[driver_df['rank'] == 2]\n",
    "                        caption = \"\"\"This graph shows the second contribution of all first time contributors in the specified time period.\"\"\"\n",
    "                        y_axis_label = 'Second Time Contributors'\n",
    "                \n",
    "\n",
    "\n",
    "                #filter by end_date, this is not done with the begin date filtering because a repeat contributor will look like drive-by if the second contribution is removed by end_date filtering\n",
    "                mask = (driver_df['yearmonth'] < end_date)\n",
    "                driver_df = driver_df.loc[mask]\n",
    "                \n",
    "\n",
    "\n",
    "                #adds all months to driver_df so the lists of dates will include all months and years    \n",
    "                driver_df = pd.concat([driver_df, months_df])\n",
    "                \n",
    "                actions = ['open_pull_request', 'pull_request_comment', 'commit', 'issue_closed', 'issue_opened', 'issue_comment']\n",
    "                \n",
    "                \n",
    "                data = pd.DataFrame()\n",
    "                if group_by == 'year': \n",
    "\n",
    "                    #x-axis dates\n",
    "                    data['dates'] = driver_df[group_by].unique()\n",
    "                    \n",
    "                    for contribution_type in actions:\n",
    "                        data[contribution_type] = pd.concat([driver_df.loc[driver_df['action'] == contribution_type], months_df]).groupby(group_by).sum().reset_index()[y_axis]\n",
    "\n",
    "                    #new contributor counts for all actions\n",
    "                    data['new_contributor_counts'] = driver_df.groupby([group_by]).sum().reset_index()[y_axis]\n",
    "\n",
    "                    #used to format x-axis and graph title\n",
    "                    group_by_format_string = \"Year\"\n",
    "\n",
    "                elif group_by == 'quarter' or group_by == 'month':\n",
    "                    \n",
    "                    #set variables to group the data by quarter or month\n",
    "                    if group_by == 'quarter':\n",
    "                        date_column = 'quarter'\n",
    "                        group_by_format_string = \"Quarter\"\n",
    "                        \n",
    "                    elif group_by == 'month':\n",
    "                        date_column = 'yearmonth'\n",
    "                        group_by_format_string = \"Month\"\n",
    "                        \n",
    "                    #modifies the driver_df[date_column] to be a string with year and month, then finds all the unique values   \n",
    "                    data['dates'] = np.unique(np.datetime_as_string(driver_df[date_column], unit = 'M'))\n",
    "                    \n",
    "                    #new_contributor counts for each type of action\n",
    "                    for contribution_type in actions:\n",
    "                        data[contribution_type] = pd.concat([driver_df.loc[driver_df['action'] == contribution_type], months_df]).groupby(date_column).sum().reset_index()[y_axis]\n",
    "\n",
    "                    #new contributor counts for all actions\n",
    "                    data['new_contributor_counts'] = driver_df.groupby([date_column]).sum().reset_index()[y_axis]\n",
    "                    \n",
    "                    \n",
    "                #if the data set is large enough it will dynamically assign the width, if the data set is too small it will by default set to 870 pixel so the title fits\n",
    "                if len(data['new_contributor_counts']) >= 15:\n",
    "                    plot_width = 46 * len(data['new_contributor_counts']) + 200\n",
    "                else:\n",
    "                    plot_width = 870\n",
    "                    \n",
    "                #create list of values for data source dict\n",
    "                actions_df_references = []\n",
    "                for action in actions:\n",
    "                    actions_df_references.append(data[action])\n",
    "                    \n",
    "                #created dict with the actions as the keys, and the values as the values from the df\n",
    "                data_source = {actions[i]: actions_df_references[i] for i in range(len(actions))} \n",
    "                data_source.update( {'dates' : data['dates'], 'New Contributor Counts': data['new_contributor_counts']} )\n",
    "\n",
    "                colors = Colorblind[len(actions)]\n",
    "\n",
    "                source = ColumnDataSource(data=data_source)\n",
    "\n",
    "                #create a dict convert an integer number into a word\n",
    "                #used to turn the rank into a word, so it is nicely displayed in the title\n",
    "                numbers = ['Zero', 'First', 'Second']\n",
    "                num_conversion_dict = {}\n",
    "                for i in range(1, len(numbers)):\n",
    "                    num_conversion_dict[i] = numbers[i]\n",
    "                number =  '{}'.format(num_conversion_dict[rank])\n",
    "\n",
    "                #y_max = 20\n",
    "                #creates plot to hold chart\n",
    "                p = figure(x_range=data['dates'], plot_height=400, plot_width = plot_width, title=title.format(repo_dict[repo_id], contributor_type.capitalize(), number, group_by_format_string), \n",
    "                           toolbar_location=None, y_range=(0, max(data['new_contributor_counts'])* 1.15))\n",
    "                                #max(data['new_contributor_counts'])* 1.15), margin = (0, 0, 0, 0))\n",
    "\n",
    "                vbar = p.vbar_stack(actions, x='dates', width=0.8, color=colors, source=source)\n",
    "\n",
    "                #add total count labels\n",
    "                p.add_layout(LabelSet(x='dates', y='New Contributor Counts', text='New Contributor Counts', y_offset=4, text_font_size=\"14pt\", \n",
    "                                  text_color=\"black\", source=source, text_align='center'))\n",
    "\n",
    "                #add legend\n",
    "                legend = Legend(items=[(date, [action]) for (date, action) in zip(actions, vbar)], location=(0, 120), label_text_font_size = \"16px\")\n",
    "                p.add_layout(legend, 'right')\n",
    "\n",
    "\n",
    "                p.xgrid.grid_line_color = None\n",
    "                p.y_range.start = 0\n",
    "                p.axis.minor_tick_line_color = None\n",
    "                p.outline_line_color = None\n",
    "\n",
    "                p.title.align = \"center\"\n",
    "                p.title.text_font_size = \"18px\"\n",
    "\n",
    "                p.yaxis.axis_label = 'Second Time Contributors' if rank == 2 else 'New Contributors'\n",
    "                p.xaxis.axis_label = group_by_format_string \n",
    "\n",
    "                p.xaxis.axis_label_text_font_size = \"18px\"\n",
    "                p.yaxis.axis_label_text_font_size = \"16px\"\n",
    "\n",
    "                p.xaxis.major_label_text_font_size = \"16px\"\n",
    "                p.xaxis.major_label_orientation = 45.0\n",
    "\n",
    "                p.yaxis.major_label_text_font_size = \"16px\"\n",
    "\n",
    "                plot = p\n",
    "\n",
    "                #creates plot to hold caption \n",
    "                p = figure(width = plot_width, height=200, margin = (0, 0, 0, 0))\n",
    "\n",
    "                p.add_layout(Label(\n",
    "                x = 0, # Change to shift caption left or right\n",
    "                y = 160, \n",
    "                x_units = 'screen',\n",
    "                y_units = 'screen',\n",
    "                text='{}'.format(caption.format(num_contributions_required, time)),\n",
    "                text_font = 'times', # Use same font as paper\n",
    "                text_font_size = '15pt',\n",
    "                render_mode='css'\n",
    "                ))\n",
    "                p.outline_line_color = None\n",
    "\n",
    "                caption_plot = p\n",
    "\n",
    "                #puts plots together into a grid\n",
    "                grid = gridplot([[plot], [caption_plot]])\n",
    "\n",
    "                show(grid)\n",
    "                \n",
    "                if save_files:\n",
    "                    output_file = 'images/' + 'new_contributors_stacked_bar' + '_' + contributor_type + '_' + group_by + '_' + repo_dict[repo_id] + '.png'\n",
    "                    export_png(grid, filename=output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat and Drive By Contributor Counts Pie Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pie_chart(input_df, repo_id, title = \" {}: Number of Returning Contributors out of {} from {} to {}\", required_contributions = 4, required_time = 5):\n",
    "    \n",
    "    if type(repo_id) == type(repo_list):\n",
    "        repo_ids = repo_id\n",
    "    else:\n",
    "        repo_ids = [repo_id]\n",
    "        \n",
    "    for repo_id in repo_ids:\n",
    "        output_notebook()\n",
    "        \n",
    "        #create a copy of contributor dataframe\n",
    "        driver_df = input_df.copy()\n",
    "        \n",
    "        #filter dataframe by repo_id\n",
    "        driver_df = driver_df.loc[driver_df['repo_id'] == repo_id]\n",
    "        \n",
    "        #remove first time contributors before begin date, along with their second contribution\n",
    "        mask = (driver_df['yearmonth'] < begin_date)\n",
    "        driver_df= driver_df[~driver_df['cntrb_id'].isin(driver_df.loc[mask]['cntrb_id'])]\n",
    "\n",
    "        \n",
    "        #determine if contributor is a drive by by finding all the cntrb_id's that do not have a second contribution\n",
    "        repeats_df = driver_df.copy()\n",
    "\n",
    "        repeats_df = repeats_df.loc[repeats_df['rank'].isin([1,required_contributions])]\n",
    "\n",
    "        #removes all the contributors that only have a first contirbution\n",
    "        repeats_df = repeats_df[repeats_df['cntrb_id'].isin(repeats_df.loc[driver_df['rank'] == required_contributions]['cntrb_id'])]\n",
    "\n",
    "        repeat_list = repeats_df.loc[driver_df['rank'] == required_contributions]['created_at'].tolist()\n",
    "        first_list = repeats_df.loc[driver_df['rank'] == 1]['created_at'].tolist()\n",
    "\n",
    "        repeats_df = repeats_df.loc[driver_df['rank'] == 1]\n",
    "        repeats_df['type'] = 'repeat'\n",
    "        \n",
    "        differences = []\n",
    "        for i in range(0, len(repeat_list)):\n",
    "            time_difference = repeat_list[i] - first_list[i]\n",
    "            total = time_difference.days * 86400 + time_difference.seconds\n",
    "            differences.append(total)\n",
    "        repeats_df['differences'] = differences\n",
    "\n",
    "        repeats_df = repeats_df.loc[repeats_df['differences'] <= required_time * 86400]\n",
    "        \n",
    "        \n",
    "        repeat_cntrb_ids = repeats_df['cntrb_id'].to_list()\n",
    "\n",
    "        drive_by_df = driver_df.loc[~driver_df['cntrb_id'].isin(repeat_cntrb_ids)]\n",
    "\n",
    "        drive_by_df = drive_by_df.loc[driver_df['rank'] == 1]\n",
    "        drive_by_df['type'] = 'drive_by'\n",
    "        \n",
    "        driver_df = pd.concat([drive_by_df, repeats_df])\n",
    "        \n",
    "        #filter df by end date\n",
    "        mask = (driver_df['yearmonth'] < end_date)\n",
    "        driver_df = driver_df.loc[mask]\n",
    "\n",
    "        #first and second time contributor counts\n",
    "        drive_by_contributors = driver_df.loc[driver_df['type'] == 'drive_by'].count()['new_contributors']\n",
    "        repeat_contributors = driver_df.loc[driver_df['type'] == 'repeat'].count()['new_contributors']\n",
    "        \n",
    "        #create a dict with the # of drive-by and repeat contributors\n",
    "        x = {'Drive_By': drive_by_contributors,\n",
    "             'Repeat' : repeat_contributors}\n",
    " \n",
    "        #turn dict 'x' into a dataframe with columns 'contributor_type', and 'counts'\n",
    "        data = pd.Series(x).reset_index(name='counts').rename(columns={'index':'contributor_type'})\n",
    "\n",
    "        data['angle'] = data['counts']/data['counts'].sum() * 2*pi\n",
    "        data['color'] = ('#0072B2', '#E69F00')\n",
    "        data['percentage'] = ((data['angle']/(2*pi))*100).round(2)\n",
    "        \n",
    "        #format title \n",
    "        title = title.format(repo_dict[repo_id], drive_by_contributors + repeat_contributors, begin_date, end_date)\n",
    "        title_text_font_size = 18\n",
    "        \n",
    "        plot_width = 850\n",
    "        \n",
    "        #sets plot_width to width of title if title is wider than 850 pixels\n",
    "        if len(title) * title_text_font_size / 2 > plot_width:\n",
    "            plot_width = int(len(title) * title_text_font_size / 2)\n",
    "        \n",
    "        \n",
    "        source = ColumnDataSource(data)\n",
    "        \n",
    "        #creates plot for chart\n",
    "        p = figure(plot_height=450, plot_width =plot_width, title=title, \n",
    "                   toolbar_location=None, x_range=(-0.5, 1.3), tools = 'hover', tooltips = \"@contributor_type\", margin = (0, 0, 0, 0))\n",
    "\n",
    "        wedge = p.wedge(x=0.87, y=1, radius=0.4, start_angle=cumsum('angle', include_zero=True), end_angle=cumsum('angle'),\n",
    "            line_color=None, fill_color='color', legend_field='contributor_type', source=data)\n",
    "\n",
    "        start_point = 0.88\n",
    "        for i in range(0, len(data['percentage'])):\n",
    "            #percentages\n",
    "            p.add_layout(Label(x=-0.17, y= start_point + 0.13*(len(data['percentage']) - 1 - i), text='{}%'.format(data.iloc[i]['percentage']), \n",
    "                        render_mode='css', text_font_size = '15px', text_font_style= 'bold'))\n",
    "\n",
    "            #contributors\n",
    "            p.add_layout(Label(x=0.12, y= start_point + 0.13*(len(data['percentage']) - 1 - i), text='{}'.format(data.iloc[i]['counts']), \n",
    "                        render_mode='css', text_font_size = '15px', text_font_style= 'bold'))\n",
    "\n",
    "        #percentages header    \n",
    "        p.add_layout(Label(x=-0.22, y= start_point + 0.13*(len(data['percentage'])), text='Percentages', render_mode='css', \n",
    "                    text_font_size = '15px', text_font_style= 'bold'))\n",
    "\n",
    "        #legend header\n",
    "        p.add_layout(Label(x=-0.43, y= start_point + 0.13*(len(data['percentage'])), text='Category', render_mode='css', \n",
    "                    text_font_size = '15px', text_font_style= 'bold'))\n",
    "\n",
    "        #contributors header\n",
    "        p.add_layout(Label(x=0, y= start_point + 0.13*(len(data['percentage'])), text='# Contributors', render_mode='css', \n",
    "                    text_font_size = '15px', text_font_style= 'bold'))\n",
    "\n",
    "        p.axis.axis_label=None\n",
    "        p.axis.visible=False\n",
    "        p.grid.grid_line_color = None \n",
    "        \n",
    "        p.title.align = \"center\"\n",
    "        p.title.text_font_size = \"{}px\".format(title_text_font_size)\n",
    "   \n",
    "        p.legend.location = \"center_left\"\n",
    "        p.legend.border_line_color = None\n",
    "        p.legend.label_text_font_style = 'bold'\n",
    "        p.legend.label_text_font_size = \"15px\"\n",
    "\n",
    "        plot = p\n",
    "\n",
    "        #creates plot for caption\n",
    "        p = figure(width = 850, height=200, margin = (0, 0, 0, 0))\n",
    "        \n",
    "        caption= \"\"\"This pie chart shows the percentage of new contributors who were drive-by or repeat contributors. Drive by contributors are contributors who make less than the required {0} contributions in {1} days. New contributors are \n",
    "                    individuals who make their first contribution in the specified time period. Repeat contributors are contributors who have made {0} or more contributions in {1} days and their first \n",
    "                        contribution is in the specified time period.\"\"\"\n",
    "\n",
    "        p.add_layout(Label(\n",
    "        x = 0, \n",
    "        y = 160, \n",
    "        x_units = 'screen',\n",
    "        y_units = 'screen',\n",
    "        text='{}'.format(caption.format(num_contributions_required, time)),\n",
    "        text_font = 'times', \n",
    "        text_font_size = '15pt',\n",
    "        render_mode='css'\n",
    "        ))\n",
    "        p.outline_line_color = None\n",
    "\n",
    "        caption_plot = p\n",
    "\n",
    "        #put graph and caption plot together into one grid\n",
    "        grid = gridplot([[plot], [caption_plot]])\n",
    "\n",
    "        show(grid)\n",
    "        \n",
    "        if save_files:\n",
    "            output_file = 'images/'  + '_' + repo_dict[repo_id] + '.png'\n",
    "            export_png(grid, filename=output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat and Drive-by Contributor Counts Stacked Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vertical_stacked_bar_chart_2(input_df, months_df, repo_id, group_by, y_axis='new_contributors', title = \"{}: Drive By and Repeat Contributor Counts per {}\", required_contributions= 5, required_time=100):\n",
    "\n",
    "    if type(repo_id) == type(repo_list):\n",
    "        repo_ids = repo_id\n",
    "    else:\n",
    "        repo_ids = [repo_id]\n",
    "\n",
    "    for repo_id in repo_ids:\n",
    "\n",
    "        output_notebook()\n",
    "\n",
    "        #create a copy of contributor dataframe\n",
    "        driver_df = input_df.copy()\n",
    "\n",
    "        #filter dataframe by repo_id\n",
    "        driver_df = driver_df.loc[driver_df['repo_id'] == repo_id]\n",
    "        \n",
    "        #remove first time contributors before begin date, along with their second contribution\n",
    "        mask = (driver_df['yearmonth'] < begin_date)\n",
    "        driver_df= driver_df[~driver_df['cntrb_id'].isin(driver_df.loc[mask]['cntrb_id'])]\n",
    "        \n",
    "        #determine if contributor is a drive by by finding all the cntrb_id's that do not have a second contribution\n",
    "        repeats_df = driver_df.copy()\n",
    "\n",
    "        #discards rows other than the first and the row required to be a repeat contributor\n",
    "        repeats_df = repeats_df.loc[repeats_df['rank'].isin([1,required_contributions])]\n",
    "\n",
    "        #removes all the contributors that only have a first contirbution\n",
    "        repeats_df = repeats_df[repeats_df['cntrb_id'].isin(repeats_df.loc[driver_df['rank'] == required_contributions]['cntrb_id'])]\n",
    "\n",
    "        #create lists of 'created_at' times for the final required contribution and the first contribution\n",
    "        repeat_list = repeats_df.loc[driver_df['rank'] == required_contributions]['created_at'].tolist()\n",
    "        first_list = repeats_df.loc[driver_df['rank'] == 1]['created_at'].tolist()\n",
    "\n",
    "        #only keep first time contributions, since there only needs to be one instance of each 'cntrb_id' in df\n",
    "        repeats_df = repeats_df.loc[driver_df['rank'] == 1]\n",
    "        repeats_df['type'] = 'repeat'\n",
    "        \n",
    "        #create list of time differences between the final required contribution and the first contribution, and add it to the df\n",
    "        differences = []\n",
    "        for i in range(0, len(repeat_list)):\n",
    "            time_difference = repeat_list[i] - first_list[i]\n",
    "            total = time_difference.days * 86400 + time_difference.seconds\n",
    "            differences.append(total)\n",
    "        repeats_df['differences'] = differences\n",
    "\n",
    "        #remove contributions who made enough contributions, but not in a short enough time\n",
    "        repeats_df = repeats_df.loc[repeats_df['differences'] <= required_time * 86400]\n",
    "        \n",
    "        \n",
    "\n",
    "        #create list of 'cntrb_ids' for repeat contributors\n",
    "        repeat_cntrb_ids = repeats_df['cntrb_id'].to_list()\n",
    "\n",
    "        #create df with all contributors other than the ones in the repeats_df\n",
    "        drive_by_df = driver_df.loc[~driver_df['cntrb_id'].isin(repeat_cntrb_ids)]\n",
    "\n",
    "        #filter df so it only includes the first contribution\n",
    "        drive_by_df = drive_by_df.loc[driver_df['rank'] == 1]\n",
    "        drive_by_df['type'] = 'drive_by'\n",
    "        \n",
    "        driver_df = pd.concat([drive_by_df, repeats_df, months_df])\n",
    "\n",
    "        #filter by end_date\n",
    "        mask = (driver_df['yearmonth'] < end_date)\n",
    "        driver_df = driver_df.loc[mask]\n",
    "\n",
    "        #create df to hold data needed for chart\n",
    "        data = pd.DataFrame()\n",
    "        if group_by == 'year': \n",
    "\n",
    "            #x-axis dates\n",
    "            data['dates'] = driver_df[group_by].unique()\n",
    "            \n",
    "            data['repeat_counts'] = driver_df.loc[driver_df['type'] == 'repeat'].groupby(group_by).count().reset_index()[y_axis]\n",
    "            data['drive_by_counts'] = driver_df.loc[driver_df['type'] == 'drive_by'].groupby(group_by).count().reset_index()[y_axis]\n",
    "\n",
    "            #new contributor counts for all contributor counts\n",
    "            total_counts = []\n",
    "            for i in range(0, len(data['drive_by_counts'])):\n",
    "                total_counts.append(data.iloc[i]['drive_by_counts'] + data.iloc[i]['repeat_counts'])\n",
    "            data['total_counts'] = total_counts\n",
    "\n",
    "            #used to format x-axis and graph title\n",
    "            group_by_format_string = \"Year\"\n",
    "\n",
    "            #font size of drive by and repeat labels\n",
    "            label_text_font_size = \"14pt\"\n",
    "\n",
    "        elif group_by == 'quarter' or group_by == 'month':\n",
    "\n",
    "            #set variables to group the data by quarter or month\n",
    "            if group_by == 'quarter':\n",
    "                date_column = 'quarter'\n",
    "                group_by_format_string = \"Quarter\"\n",
    "                \n",
    "            elif group_by == 'month':\n",
    "                date_column = 'yearmonth'\n",
    "                group_by_format_string = \"Month\"\n",
    "\n",
    "            #modifies the driver_df[date_column] to be a string with year and month, then finds all the unique values   \n",
    "            data['dates'] = np.unique(np.datetime_as_string(driver_df[date_column], unit = 'M'))\n",
    "     \n",
    "            data['drive_by_counts'] = pd.concat([driver_df.loc[driver_df['type'] == 'drive_by'], months_df]).groupby(date_column).sum().reset_index()[y_axis]\n",
    "            data['repeat_counts'] = pd.concat([driver_df.loc[driver_df['type'] == 'repeat'], months_df]).groupby(date_column).sum().reset_index()[y_axis]\n",
    "\n",
    "            #new contributor counts for all contributor types\n",
    "            total_counts = []\n",
    "            for i in range(0, len(data['drive_by_counts'])):\n",
    "                total_counts.append(data.iloc[i]['drive_by_counts'] + data.iloc[i]['repeat_counts'])\n",
    "            data['total_counts'] = total_counts\n",
    "\n",
    "            #font size of drive by and repeat labels\n",
    "            label_text_font_size = \"13pt\"\n",
    "            \n",
    "        data_source = {'Dates' : data['dates'],\n",
    "                'Drive By'     : data['drive_by_counts'],\n",
    "                'Repeat'       : data['repeat_counts'],\n",
    "                'All'          : data['total_counts']}\n",
    "\n",
    "        groups = [\"Drive By\", \"Repeat\"]\n",
    "\n",
    "        colors = ['#56B4E9', '#E69F00']\n",
    "\n",
    "        source = ColumnDataSource(data=data_source)\n",
    "        \n",
    "        #format title\n",
    "        title_text_font_size = 18\n",
    "        title = title.format(repo_dict[repo_id], group_by_format_string)\n",
    "        \n",
    "        #if the data set is large enough it will dynamically assign the width, if the data set is too small it will by default set to 780 pixel so the title fits\n",
    "        if len(data['total_counts']) >= 13:\n",
    "            plot_width = 46 * len(data['total_counts']) + 210\n",
    "        else:\n",
    "            plot_width = 780\n",
    "\n",
    "        p = figure(x_range=data['dates'], plot_height=500, plot_width = plot_width, title=title, \n",
    "                   toolbar_location=None, y_range=(0, max(total_counts)* 1.15), margin = (0, 0, 0, 0))\n",
    "        \n",
    "        vbar = p.vbar_stack(groups, x='Dates', width=0.8, color=colors, source=source)\n",
    "\n",
    "        #add total counts above bars\n",
    "        p.add_layout(LabelSet(x='Dates', y='All', text='All', y_offset=8, text_font_size=\"14pt\", \n",
    "                          text_color=\"black\", source=source, text_align='center'))\n",
    "\n",
    "        #add drive by count labels\n",
    "        p.add_layout(LabelSet(x='Dates', y='Drive By', text='Drive By', y_offset=-22, text_font_size=label_text_font_size, \n",
    "                  text_color=\"black\", source=source, text_align='center'))\n",
    "\n",
    "        #add repeat count labels\n",
    "        p.add_layout(LabelSet(x='Dates', y='All', text='Repeat', y_offset=-22, text_font_size=label_text_font_size, \n",
    "                  text_color=\"black\", source=source, text_align='center'))\n",
    "\n",
    "        #add legend\n",
    "        legend = Legend(items=[(date, [group]) for (date, group) in zip(groups, vbar)], location=(0, 200), label_text_font_size = \"16px\")\n",
    "        p.add_layout(legend, 'right')\n",
    "\n",
    "        p.xgrid.grid_line_color = None\n",
    "        p.y_range.start = 0\n",
    "        p.axis.minor_tick_line_color = None\n",
    "        p.outline_line_color = None\n",
    "\n",
    "        p.title.align = \"center\"\n",
    "        p.title.text_font_size = \"{}px\".format(title_text_font_size)\n",
    "\n",
    "        p.yaxis.axis_label = '# Contributors'\n",
    "        p.xaxis.axis_label = group_by_format_string \n",
    "\n",
    "        p.xaxis.axis_label_text_font_size = \"18px\"\n",
    "        p.yaxis.axis_label_text_font_size = \"16px\"\n",
    "\n",
    "        p.xaxis.major_label_text_font_size = \"16px\"\n",
    "        p.xaxis.major_label_orientation = 45.0\n",
    "\n",
    "        p.yaxis.major_label_text_font_size = \"16px\"\n",
    "\n",
    "        p.legend.label_text_font_size = \"20px\"\n",
    "\n",
    "        plot = p\n",
    "\n",
    "        #add plot to hold caption\n",
    "        p = figure(width = plot_width, height=200, margin = (0, 0, 0, 0))\n",
    "\n",
    "        caption = \"\"\"This graph shows the number of new contributors in the specified time period, and indicates how many were drive-by and repeat \n",
    "        contributors. Drive by contributors are contributors who make less than the required {0} contributions in {1} days. New contributors are \n",
    "        individuals who make their first contribution in the specified time period. Repeat contributors are contributors who have made {0} or more \n",
    "        contributions in {1} days and their first contribution is in the specified time period.\"\"\"\n",
    "\n",
    "        p.add_layout(Label(\n",
    "        x = 0, \n",
    "        y = 160, \n",
    "        x_units = 'screen',\n",
    "        y_units = 'screen',\n",
    "        text='{}'.format(caption.format(num_contributions_required, time)),\n",
    "        text_font = 'times', \n",
    "        text_font_size = '15pt',\n",
    "        render_mode='css'\n",
    "        ))\n",
    "        p.outline_line_color = None\n",
    "\n",
    "        caption_plot = p\n",
    "\n",
    "        #put graph and caption plot together into one grid\n",
    "        grid = gridplot([[plot], [caption_plot]])\n",
    "\n",
    "        show(grid)\n",
    "        \n",
    "        if save_files:\n",
    "            output_file = 'images/' + '_' + group_by + '_' + repo_dict[repo_id] + '.png'\n",
    "            export_png(grid, filename=output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Calls to Create Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if display_grouping == 'repo':\n",
    "    for repo_id in repo_set:\n",
    "        vertical_bar_chart(df, months_df, repo_id = repo_id, group_by = group_by, required_contributions = num_contributions_required, required_time = time)\n",
    "        vertical_stacked_bar_chart(df, months_df, repo_id = repo_id, group_by = group_by, required_contributions = num_contributions_required, required_time = time)\n",
    "        pie_chart(df, repo_id=repo_id, required_contributions = num_contributions_required, required_time = time)\n",
    "        vertical_stacked_bar_chart_2(df, months_df,repo_id = repo_id, group_by = group_by, required_contributions = num_contributions_required, required_time = time)\n",
    "        \n",
    "elif display_grouping == 'competitors':\n",
    "    vertical_bar_chart(df, months_df, repo_id = repo_list, group_by = group_by, required_contributions = num_contributions_required, required_time = time)\n",
    "    vertical_stacked_bar_chart(df, months_df, repo_id = repo_list , group_by = group_by, required_contributions = num_contributions_required, required_time = time)\n",
    "    pie_chart(df, repo_id=repo_list, required_contributions = num_contributions_required, required_time = time)\n",
    "    vertical_stacked_bar_chart_2(df, months_df,repo_id = repo_list, group_by = group_by, required_contributions = num_contributions_required, required_time = time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
