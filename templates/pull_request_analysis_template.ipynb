{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull Request Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Limitations for Reporting on Several Repos\n",
    "The visualizations in this notebook are, like most, able to coherently display information for between 1 and 8 different repositories simultaneously. \n",
    "\n",
    "## Alternatives for Reporting on Repo Groups, Comprising Many Repos\n",
    "The included queries could be rewritten to show an entire repository group's characteristics of that is your primary aim. Specifically, any query could replace this line: \n",
    "```\n",
    "                            WHERE repo.repo_id = {repo_id}\n",
    "```\n",
    "\n",
    "with this line to accomplish the goal of comparing different groups of repositories: \n",
    "```\n",
    "                            WHERE repogroups.repo_group_id = {repo_id}\n",
    "```\n",
    "\n",
    "Simply replace the set of id's in the **Pull Request Filter** section with a list of repo_group_id numbers as well, to accomplish this view. \n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd \n",
    "import sqlalchemy as salc\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import datetime\n",
    "import json\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "with open(\"config.json\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "database_connection_string = 'postgresql+psycopg2://{}:{}@{}:{}/{}'.format(config['user'], config['password'], config['host'], config['port'], config['database'])\n",
    "\n",
    "dbschema='augur_data'\n",
    "engine = salc.create_engine(\n",
    "    database_connection_string,\n",
    "    connect_args={'options': '-csearch_path={}'.format(dbschema)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declare all repo ids you would like to produce charts for\n",
    "repo_set = {25766, 25768, 25765}\n",
    "#25766 - Linkerd/linkerd\n",
    "#25768 - istio/istio\n",
    "#25765 - etcd-io/etcd \n",
    "#can be set as 'competitors' or 'repo'\n",
    "#'competitors' will group graphs by type, so it is easy to compare across repos\n",
    "# 'repo' will group graphs by repo so it is easy to look at all the contributor data for each repo\n",
    "display_grouping = 'repo'\n",
    "\n",
    "#if display_grouping is set to 'competitors', enter the repo ids you do no want to alias, if 'display_grouping' is set to repo the list will not effect anything\n",
    "not_aliased_repos = [25440, 25448]\n",
    "\n",
    "begin_date = '2022-09-01'\n",
    "end_date = '2024-03-31'\n",
    "\n",
    "#specify number of outliers for removal in scatter plot\n",
    "scatter_plot_outliers_removed = 5\n",
    "save_files = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying the Longest Running Pull Requests\n",
    "\n",
    "## Getting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_id</th>\n",
       "      <th>pr_src_id</th>\n",
       "      <th>repo_name</th>\n",
       "      <th>pr_src_author_association</th>\n",
       "      <th>repo_group</th>\n",
       "      <th>pr_src_state</th>\n",
       "      <th>pr_merged_at</th>\n",
       "      <th>pr_created_at</th>\n",
       "      <th>pr_closed_at</th>\n",
       "      <th>created_year</th>\n",
       "      <th>...</th>\n",
       "      <th>head_ref_force_pushed_count</th>\n",
       "      <th>merged_count</th>\n",
       "      <th>milestoned_count</th>\n",
       "      <th>unlabeled_count</th>\n",
       "      <th>head_ref_deleted_count</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>lines_added</th>\n",
       "      <th>lines_removed</th>\n",
       "      <th>commit_count</th>\n",
       "      <th>file_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [repo_id, pr_src_id, repo_name, pr_src_author_association, repo_group, pr_src_state, pr_merged_at, pr_created_at, pr_closed_at, created_year, created_month, closed_year, closed_month, pr_src_meta_label, pr_head_or_base, hours_to_close, days_to_close, hours_to_first_response, days_to_first_response, hours_to_last_response, days_to_last_response, first_response_time, last_response_time, average_time_between_responses, assigned_count, review_requested_count, labeled_count, subscribed_count, mentioned_count, referenced_count, closed_count, head_ref_force_pushed_count, merged_count, milestoned_count, unlabeled_count, head_ref_deleted_count, comment_count, lines_added, lines_removed, commit_count, file_count]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 41 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "repo_id                           object\n",
       "pr_src_id                         object\n",
       "repo_name                         object\n",
       "pr_src_author_association         object\n",
       "repo_group                        object\n",
       "pr_src_state                      object\n",
       "pr_merged_at                      object\n",
       "pr_created_at                     object\n",
       "pr_closed_at                      object\n",
       "created_year                      object\n",
       "created_month                     object\n",
       "closed_year                       object\n",
       "closed_month                      object\n",
       "pr_src_meta_label                 object\n",
       "pr_head_or_base                   object\n",
       "hours_to_close                    object\n",
       "days_to_close                     object\n",
       "hours_to_first_response           object\n",
       "days_to_first_response            object\n",
       "hours_to_last_response            object\n",
       "days_to_last_response             object\n",
       "first_response_time               object\n",
       "last_response_time                object\n",
       "average_time_between_responses    object\n",
       "assigned_count                    object\n",
       "review_requested_count            object\n",
       "labeled_count                     object\n",
       "subscribed_count                  object\n",
       "mentioned_count                   object\n",
       "referenced_count                  object\n",
       "closed_count                      object\n",
       "head_ref_force_pushed_count       object\n",
       "merged_count                      object\n",
       "milestoned_count                  object\n",
       "unlabeled_count                   object\n",
       "head_ref_deleted_count            object\n",
       "comment_count                     object\n",
       "lines_added                       object\n",
       "lines_removed                     object\n",
       "commit_count                      object\n",
       "file_count                        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr_all = pd.DataFrame()\n",
    "\n",
    "for repo_id in repo_set: \n",
    "\n",
    "    pr_query = salc.sql.text(f\"\"\"\n",
    "                    SELECT\n",
    "                        repo.repo_id AS repo_id,\n",
    "                        pull_requests.pr_src_id AS pr_src_id,\n",
    "                        repo.repo_name AS repo_name,\n",
    "                        pr_src_author_association,\n",
    "                        repo_groups.rg_name AS repo_group,\n",
    "                        pull_requests.pr_src_state,\n",
    "                        pull_requests.pr_merged_at,\n",
    "                        pull_requests.pr_created_at AS pr_created_at,\n",
    "                        pull_requests.pr_closed_at AS pr_closed_at,\n",
    "                        date_part( 'year', pr_created_at :: DATE ) AS CREATED_YEAR,\n",
    "                        date_part( 'month', pr_created_at :: DATE ) AS CREATED_MONTH,\n",
    "                        date_part( 'year', pr_closed_at :: DATE ) AS CLOSED_YEAR,\n",
    "                        date_part( 'month', pr_closed_at :: DATE ) AS CLOSED_MONTH,\n",
    "                        pr_src_meta_label,\n",
    "                        pr_head_or_base,\n",
    "                        ( EXTRACT ( EPOCH FROM pull_requests.pr_closed_at ) - EXTRACT ( EPOCH FROM pull_requests.pr_created_at ) ) / 3600 AS hours_to_close,\n",
    "                        ( EXTRACT ( EPOCH FROM pull_requests.pr_closed_at ) - EXTRACT ( EPOCH FROM pull_requests.pr_created_at ) ) / 86400 AS days_to_close, \n",
    "                        ( EXTRACT ( EPOCH FROM first_response_time ) - EXTRACT ( EPOCH FROM pull_requests.pr_created_at ) ) / 3600 AS hours_to_first_response,\n",
    "                        ( EXTRACT ( EPOCH FROM first_response_time ) - EXTRACT ( EPOCH FROM pull_requests.pr_created_at ) ) / 86400 AS days_to_first_response, \n",
    "                        ( EXTRACT ( EPOCH FROM last_response_time ) - EXTRACT ( EPOCH FROM pull_requests.pr_created_at ) ) / 3600 AS hours_to_last_response,\n",
    "                        ( EXTRACT ( EPOCH FROM last_response_time ) - EXTRACT ( EPOCH FROM pull_requests.pr_created_at ) ) / 86400 AS days_to_last_response, \n",
    "                        first_response_time,\n",
    "                        last_response_time,\n",
    "                        average_time_between_responses,\n",
    "                        assigned_count,\n",
    "                        review_requested_count,\n",
    "                        labeled_count,\n",
    "                        subscribed_count,\n",
    "                        mentioned_count,\n",
    "                        referenced_count,\n",
    "                        closed_count,\n",
    "                        head_ref_force_pushed_count,\n",
    "                        merged_count,\n",
    "                        milestoned_count,\n",
    "                        unlabeled_count,\n",
    "                        head_ref_deleted_count,\n",
    "                        comment_count,\n",
    "                        lines_added, \n",
    "                        lines_removed,\n",
    "                        commit_count, \n",
    "                        file_count\n",
    "                    FROM\n",
    "                        repo,\n",
    "                        repo_groups,\n",
    "                        pull_requests LEFT OUTER JOIN ( \n",
    "                            SELECT pull_requests.pull_request_id,\n",
    "                            count(*) FILTER (WHERE action = 'assigned') AS assigned_count,\n",
    "                            count(*) FILTER (WHERE action = 'review_requested') AS review_requested_count,\n",
    "                            count(*) FILTER (WHERE action = 'labeled') AS labeled_count,\n",
    "                            count(*) FILTER (WHERE action = 'unlabeled') AS unlabeled_count,\n",
    "                            count(*) FILTER (WHERE action = 'subscribed') AS subscribed_count,\n",
    "                            count(*) FILTER (WHERE action = 'mentioned') AS mentioned_count,\n",
    "                            count(*) FILTER (WHERE action = 'referenced') AS referenced_count,\n",
    "                            count(*) FILTER (WHERE action = 'closed') AS closed_count,\n",
    "                            count(*) FILTER (WHERE action = 'head_ref_force_pushed') AS head_ref_force_pushed_count,\n",
    "                            count(*) FILTER (WHERE action = 'head_ref_deleted') AS head_ref_deleted_count,\n",
    "                            count(*) FILTER (WHERE action = 'milestoned') AS milestoned_count,\n",
    "                            count(*) FILTER (WHERE action = 'merged') AS merged_count,\n",
    "                            MIN(message.msg_timestamp) AS first_response_time,\n",
    "                            COUNT(DISTINCT message.msg_timestamp) AS comment_count,\n",
    "                            MAX(message.msg_timestamp) AS last_response_time,\n",
    "                            (MAX(message.msg_timestamp) - MIN(message.msg_timestamp)) / COUNT(DISTINCT message.msg_timestamp) AS average_time_between_responses\n",
    "                            FROM pull_request_events, pull_requests, repo, pull_request_message_ref, message\n",
    "                            WHERE repo.repo_id = {repo_id}\n",
    "                            AND repo.repo_id = pull_requests.repo_id\n",
    "                            AND pull_requests.pull_request_id = pull_request_events.pull_request_id\n",
    "                            AND pull_requests.pull_request_id = pull_request_message_ref.pull_request_id\n",
    "                            AND pull_request_message_ref.msg_id = message.msg_id\n",
    "                            GROUP BY pull_requests.pull_request_id\n",
    "                        ) response_times\n",
    "                        ON pull_requests.pull_request_id = response_times.pull_request_id\n",
    "                        LEFT OUTER JOIN (\n",
    "                            SELECT pull_request_commits.pull_request_id, count(DISTINCT pr_cmt_sha) AS commit_count                                \n",
    "                            FROM pull_request_commits, pull_requests, pull_request_meta\n",
    "                            WHERE pull_requests.pull_request_id = pull_request_commits.pull_request_id\n",
    "                            AND pull_requests.pull_request_id = pull_request_meta.pull_request_id\n",
    "                            AND pull_requests.repo_id = {repo_id}\n",
    "                            AND pr_cmt_sha <> pull_requests.pr_merge_commit_sha\n",
    "                            AND pr_cmt_sha <> pull_request_meta.pr_sha\n",
    "                            GROUP BY pull_request_commits.pull_request_id\n",
    "                        ) all_commit_counts\n",
    "                        ON pull_requests.pull_request_id = all_commit_counts.pull_request_id\n",
    "                        LEFT OUTER JOIN (\n",
    "                            SELECT MAX(pr_repo_meta_id), pull_request_meta.pull_request_id, pr_head_or_base, pr_src_meta_label\n",
    "                            FROM pull_requests, pull_request_meta\n",
    "                            WHERE pull_requests.pull_request_id = pull_request_meta.pull_request_id\n",
    "                            AND pull_requests.repo_id = {repo_id}\n",
    "                            AND pr_head_or_base = 'base'\n",
    "                            GROUP BY pull_request_meta.pull_request_id, pr_head_or_base, pr_src_meta_label\n",
    "                        ) base_labels\n",
    "                        ON base_labels.pull_request_id = all_commit_counts.pull_request_id\n",
    "                        LEFT OUTER JOIN (\n",
    "                            SELECT sum(cmt_added) AS lines_added, sum(cmt_removed) AS lines_removed, pull_request_commits.pull_request_id, count(DISTINCT cmt_filename) AS file_count\n",
    "                            FROM pull_request_commits, commits, pull_requests, pull_request_meta\n",
    "                            WHERE cmt_commit_hash = pr_cmt_sha\n",
    "                            AND pull_requests.pull_request_id = pull_request_commits.pull_request_id\n",
    "                            AND pull_requests.pull_request_id = pull_request_meta.pull_request_id\n",
    "                            AND pull_requests.repo_id = {repo_id}\n",
    "                            AND commits.repo_id = pull_requests.repo_id\n",
    "                            AND commits.cmt_commit_hash <> pull_requests.pr_merge_commit_sha\n",
    "                            AND commits.cmt_commit_hash <> pull_request_meta.pr_sha\n",
    "                            GROUP BY pull_request_commits.pull_request_id\n",
    "                        ) master_merged_counts \n",
    "                        ON base_labels.pull_request_id = master_merged_counts.pull_request_id                    \n",
    "                    WHERE \n",
    "                        repo.repo_group_id = repo_groups.repo_group_id \n",
    "                        AND repo.repo_id = pull_requests.repo_id \n",
    "                        AND repo.repo_id = {repo_id}\n",
    "                    ORDER BY\n",
    "                       merged_count DESC\n",
    "        \"\"\")\n",
    "    pr_a = pd.read_sql(pr_query, con=engine)\n",
    "    if not pr_all.empty: \n",
    "        pr_all = pd.concat([pr_all, pr_a]) \n",
    "    else: \n",
    "        # first repo\n",
    "        pr_all = pr_a\n",
    "display(pr_all.head())\n",
    "pr_all.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin data pre-processing and adding columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data type changing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "repo_id                            object\n",
       "pr_src_id                          object\n",
       "repo_name                          object\n",
       "pr_src_author_association          object\n",
       "repo_group                         object\n",
       "pr_src_state                       object\n",
       "pr_merged_at                       object\n",
       "pr_created_at                      object\n",
       "pr_closed_at                       object\n",
       "created_year                       object\n",
       "created_month                      object\n",
       "closed_year                        object\n",
       "closed_month                       object\n",
       "pr_src_meta_label                  object\n",
       "pr_head_or_base                    object\n",
       "hours_to_close                     object\n",
       "days_to_close                      object\n",
       "hours_to_first_response            object\n",
       "days_to_first_response             object\n",
       "hours_to_last_response             object\n",
       "days_to_last_response              object\n",
       "first_response_time                object\n",
       "last_response_time                 object\n",
       "average_time_between_responses     object\n",
       "assigned_count                    float64\n",
       "review_requested_count            float64\n",
       "labeled_count                     float64\n",
       "subscribed_count                  float64\n",
       "mentioned_count                   float64\n",
       "referenced_count                  float64\n",
       "closed_count                      float64\n",
       "head_ref_force_pushed_count       float64\n",
       "merged_count                      float64\n",
       "milestoned_count                  float64\n",
       "unlabeled_count                   float64\n",
       "head_ref_deleted_count            float64\n",
       "comment_count                     float64\n",
       "lines_added                       float64\n",
       "lines_removed                     float64\n",
       "commit_count                      float64\n",
       "file_count                        float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change count columns from float datatype to integer\n",
    "pr_all[['assigned_count',\n",
    "          'review_requested_count',\n",
    "          'labeled_count',\n",
    "          'subscribed_count',\n",
    "          'mentioned_count',\n",
    "          'referenced_count',\n",
    "          'closed_count',\n",
    "          'head_ref_force_pushed_count',\n",
    "          'merged_count',\n",
    "          'milestoned_count',\n",
    "          'unlabeled_count',\n",
    "          'head_ref_deleted_count',\n",
    "          'comment_count',\n",
    "        'commit_count',\n",
    "        'file_count',\n",
    "        'lines_added',\n",
    "        'lines_removed'\n",
    "       ]] = pr_all[['assigned_count',\n",
    "                                      'review_requested_count',\n",
    "                                      'labeled_count',\n",
    "                                      'subscribed_count',\n",
    "                                      'mentioned_count',\n",
    "                                      'referenced_count',\n",
    "                                      'closed_count',\n",
    "                                        'head_ref_force_pushed_count',\n",
    "                                    'merged_count',\n",
    "                                      'milestoned_count',          \n",
    "                                      'unlabeled_count',\n",
    "                                      'head_ref_deleted_count',\n",
    "                                      'comment_count',\n",
    "                                        'commit_count',\n",
    "                                        'file_count',\n",
    "                                        'lines_added',\n",
    "                                        'lines_removed'\n",
    "                   ]].astype(float)\n",
    "# Change years to int so that doesn't display as 2019.0 for example\n",
    "pr_all[[\n",
    "            'created_year',\n",
    "           'closed_year']] = pr_all[['created_year',\n",
    "                                       'closed_year']].fillna(-1).astype(int).astype(str)\n",
    "pr_all.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(pr_all['repo_name'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add `average_days_between_responses` and `average_hours_between_responses` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_id</th>\n",
       "      <th>pr_src_id</th>\n",
       "      <th>repo_name</th>\n",
       "      <th>pr_src_author_association</th>\n",
       "      <th>repo_group</th>\n",
       "      <th>pr_src_state</th>\n",
       "      <th>pr_merged_at</th>\n",
       "      <th>pr_created_at</th>\n",
       "      <th>pr_closed_at</th>\n",
       "      <th>created_year</th>\n",
       "      <th>...</th>\n",
       "      <th>milestoned_count</th>\n",
       "      <th>unlabeled_count</th>\n",
       "      <th>head_ref_deleted_count</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>lines_added</th>\n",
       "      <th>lines_removed</th>\n",
       "      <th>commit_count</th>\n",
       "      <th>file_count</th>\n",
       "      <th>average_days_between_responses</th>\n",
       "      <th>average_hours_between_responses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [repo_id, pr_src_id, repo_name, pr_src_author_association, repo_group, pr_src_state, pr_merged_at, pr_created_at, pr_closed_at, created_year, created_month, closed_year, closed_month, pr_src_meta_label, pr_head_or_base, hours_to_close, days_to_close, hours_to_first_response, days_to_first_response, hours_to_last_response, days_to_last_response, first_response_time, last_response_time, average_time_between_responses, assigned_count, review_requested_count, labeled_count, subscribed_count, mentioned_count, referenced_count, closed_count, head_ref_force_pushed_count, merged_count, milestoned_count, unlabeled_count, head_ref_deleted_count, comment_count, lines_added, lines_removed, commit_count, file_count, average_days_between_responses, average_hours_between_responses]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 43 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get days for average_time_between_responses time delta\n",
    "\n",
    "pr_all['average_days_between_responses'] = pr_all['average_time_between_responses'].map(lambda x: x.days).astype(float)\n",
    "pr_all['average_hours_between_responses'] = pr_all['average_time_between_responses'].map(lambda x: x.days * 24).astype(float)\n",
    "\n",
    "pr_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date filtering entire dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_id</th>\n",
       "      <th>pr_src_id</th>\n",
       "      <th>repo_name</th>\n",
       "      <th>pr_src_author_association</th>\n",
       "      <th>repo_group</th>\n",
       "      <th>pr_src_state</th>\n",
       "      <th>pr_merged_at</th>\n",
       "      <th>pr_created_at</th>\n",
       "      <th>pr_closed_at</th>\n",
       "      <th>created_year</th>\n",
       "      <th>...</th>\n",
       "      <th>unlabeled_count</th>\n",
       "      <th>head_ref_deleted_count</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>lines_added</th>\n",
       "      <th>lines_removed</th>\n",
       "      <th>commit_count</th>\n",
       "      <th>file_count</th>\n",
       "      <th>average_days_between_responses</th>\n",
       "      <th>average_hours_between_responses</th>\n",
       "      <th>created_yearmonth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [repo_id, pr_src_id, repo_name, pr_src_author_association, repo_group, pr_src_state, pr_merged_at, pr_created_at, pr_closed_at, created_year, created_month, closed_year, closed_month, pr_src_meta_label, pr_head_or_base, hours_to_close, days_to_close, hours_to_first_response, days_to_first_response, hours_to_last_response, days_to_last_response, first_response_time, last_response_time, average_time_between_responses, assigned_count, review_requested_count, labeled_count, subscribed_count, mentioned_count, referenced_count, closed_count, head_ref_force_pushed_count, merged_count, milestoned_count, unlabeled_count, head_ref_deleted_count, comment_count, lines_added, lines_removed, commit_count, file_count, average_days_between_responses, average_hours_between_responses, created_yearmonth]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 44 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_date = pd.to_datetime(begin_date)\n",
    "# end_date = pd.to_datetime('2020-02-01 09:00:00')\n",
    "end_date = pd.to_datetime(end_date)\n",
    "pr_all = pr_all[(pr_all['pr_created_at'] > start_date) & (pr_all['pr_closed_at'] < end_date)]\n",
    "\n",
    "pr_all['created_year'] = pr_all['created_year'].map(int)\n",
    "pr_all['created_month'] = pr_all['created_month'].map(int)\n",
    "pr_all['created_month'] = pr_all['created_month'].map(lambda x: '{0:0>2}'.format(x))\n",
    "pr_all['created_yearmonth'] = pd.to_datetime(pr_all['created_year'].map(str) + '-' + pr_all['created_month'].map(str) + '-01')\n",
    "pr_all.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add `days_to_close` column for pull requests that are still open (closed pull requests already have this column filled from the query)\n",
    "\n",
    "Note: there will be no pull requests that are still open in the dataframe if you filtered by an end date in the above cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_id</th>\n",
       "      <th>pr_src_id</th>\n",
       "      <th>repo_name</th>\n",
       "      <th>pr_src_author_association</th>\n",
       "      <th>repo_group</th>\n",
       "      <th>pr_src_state</th>\n",
       "      <th>pr_merged_at</th>\n",
       "      <th>pr_created_at</th>\n",
       "      <th>pr_closed_at</th>\n",
       "      <th>created_year</th>\n",
       "      <th>...</th>\n",
       "      <th>unlabeled_count</th>\n",
       "      <th>head_ref_deleted_count</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>lines_added</th>\n",
       "      <th>lines_removed</th>\n",
       "      <th>commit_count</th>\n",
       "      <th>file_count</th>\n",
       "      <th>average_days_between_responses</th>\n",
       "      <th>average_hours_between_responses</th>\n",
       "      <th>created_yearmonth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [repo_id, pr_src_id, repo_name, pr_src_author_association, repo_group, pr_src_state, pr_merged_at, pr_created_at, pr_closed_at, created_year, created_month, closed_year, closed_month, pr_src_meta_label, pr_head_or_base, hours_to_close, days_to_close, hours_to_first_response, days_to_first_response, hours_to_last_response, days_to_last_response, first_response_time, last_response_time, average_time_between_responses, assigned_count, review_requested_count, labeled_count, subscribed_count, mentioned_count, referenced_count, closed_count, head_ref_force_pushed_count, merged_count, milestoned_count, unlabeled_count, head_ref_deleted_count, comment_count, lines_added, lines_removed, commit_count, file_count, average_days_between_responses, average_hours_between_responses, created_yearmonth]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 44 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "# getting the number of days of (today - created at) for the PRs that are still open\n",
    "# and putting this in the days_to_close column\n",
    "\n",
    "# get timedeltas of creation time to todays date/time\n",
    "days_to_close_open_pr = datetime.datetime.now() - pr_all.loc[pr_all['pr_src_state'] == 'open']['pr_created_at']\n",
    "\n",
    "# get num days from above timedelta\n",
    "days_to_close_open_pr = days_to_close_open_pr.apply(lambda x: x.days).astype(int)\n",
    "\n",
    "# for only OPEN pr's, set the days_to_close column equal to above dataframe\n",
    "pr_all.loc[pr_all['pr_src_state'] == 'open'] = pr_all.loc[pr_all['pr_src_state'] == 'open'].assign(days_to_close=days_to_close_open_pr)\n",
    "\n",
    "pr_all.loc[pr_all['pr_src_state'] == 'open'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add `closed_yearmonth` column for only CLOSED pull requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "UFuncTypeError",
     "evalue": "ufunc 'add' did not contain a loop with signature matching types (dtype('int64'), dtype('<U1')) -> None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m pr_all[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclosed_yearmonth\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(np\u001b[38;5;241m.\u001b[39mnan)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Fill column with prettified string of year/month closed that looks like: 2019-07-01\u001b[39;00m\n\u001b[1;32m      5\u001b[0m pr_all\u001b[38;5;241m.\u001b[39mloc[pr_all[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpr_src_state\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pr_all\u001b[38;5;241m.\u001b[39mloc[pr_all[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpr_src_state\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39massign(\n\u001b[0;32m----> 6\u001b[0m     closed_yearmonth \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[43mpr_all\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpr_all\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpr_src_state\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclosed\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclosed_year\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m \u001b[38;5;241m+\u001b[39m pr_all\u001b[38;5;241m.\u001b[39mloc[pr_all[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpr_src_state\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclosed_month\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-01\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      9\u001b[0m pr_all\u001b[38;5;241m.\u001b[39mloc[pr_all[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpr_src_state\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/github/virtualenvs/phds/lib/python3.11/site-packages/pandas/core/ops/common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/virtualenvs/phds/lib/python3.11/site-packages/pandas/core/arraylike.py:186\u001b[0m, in \u001b[0;36mOpsMixin.__add__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__add__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__add__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m    100\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m    Get Addition of DataFrame and other, column-wise.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m    moose     3.0     NaN\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_arith_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/virtualenvs/phds/lib/python3.11/site-packages/pandas/core/series.py:6126\u001b[0m, in \u001b[0;36mSeries._arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_arith_method\u001b[39m(\u001b[38;5;28mself\u001b[39m, other, op):\n\u001b[1;32m   6125\u001b[0m     \u001b[38;5;28mself\u001b[39m, other \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_align_for_op(other)\n\u001b[0;32m-> 6126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIndexOpsMixin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_arith_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/virtualenvs/phds/lib/python3.11/site-packages/pandas/core/base.py:1382\u001b[0m, in \u001b[0;36mIndexOpsMixin._arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   1379\u001b[0m     rvalues \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(rvalues\u001b[38;5;241m.\u001b[39mstart, rvalues\u001b[38;5;241m.\u001b[39mstop, rvalues\u001b[38;5;241m.\u001b[39mstep)\n\u001b[1;32m   1381\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1382\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marithmetic_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(result, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[0;32m~/github/virtualenvs/phds/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:283\u001b[0m, in \u001b[0;36marithmetic_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    279\u001b[0m     _bool_arith_check(op, left, right)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;66;03m# error: Argument 1 to \"_na_arithmetic_op\" has incompatible type\u001b[39;00m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;66;03m# \"Union[ExtensionArray, ndarray[Any, Any]]\"; expected \"ndarray[Any, Any]\"\u001b[39;00m\n\u001b[0;32m--> 283\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43m_na_arithmetic_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res_values\n",
      "File \u001b[0;32m~/github/virtualenvs/phds/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:218\u001b[0m, in \u001b[0;36m_na_arithmetic_op\u001b[0;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[1;32m    215\u001b[0m     func \u001b[38;5;241m=\u001b[39m partial(expressions\u001b[38;5;241m.\u001b[39mevaluate, op)\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 218\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cmp \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    221\u001b[0m         left\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(right, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m\n\u001b[1;32m    222\u001b[0m     ):\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;66;03m# Don't do this for comparisons, as that will handle complex numbers\u001b[39;00m\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;66;03m#  incorrectly, see GH#32047\u001b[39;00m\n",
      "\u001b[0;31mUFuncTypeError\u001b[0m: ufunc 'add' did not contain a loop with signature matching types (dtype('int64'), dtype('<U1')) -> None"
     ]
    }
   ],
   "source": [
    "# initiate column by setting all null datetimes\n",
    "pr_all['closed_yearmonth'] = pd.to_datetime(np.nan)\n",
    "\n",
    "# Fill column with prettified string of year/month closed that looks like: 2019-07-01\n",
    "pr_all.loc[pr_all['pr_src_state'] == 'closed'] = pr_all.loc[pr_all['pr_src_state'] == 'closed'].assign(\n",
    "    closed_yearmonth = pd.to_datetime(pr_all.loc[pr_all['pr_src_state'] == 'closed']['closed_year'].astype(int\n",
    "        ).map(str) + '-' + pr_all.loc[pr_all['pr_src_state'] == 'closed']['closed_month'].astype(int).map(str) + '-01'))\n",
    "\n",
    "pr_all.loc[pr_all['pr_src_state'] == 'closed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add `merged_flag` column which is just prettified strings based off of if the `pr_merged_at` column is null or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: merged_flag, dtype: object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Merged flag \"\"\"\n",
    "if 'pr_merged_at' in pr_all.columns.values:\n",
    "    pr_all['pr_merged_at'] = pr_all['pr_merged_at'].fillna(0)\n",
    "    pr_all['merged_flag'] = 'Not Merged / Rejected'\n",
    "    pr_all['merged_flag'].loc[pr_all['pr_merged_at'] != 0] = 'Merged / Accepted'\n",
    "    pr_all['merged_flag'].loc[pr_all['pr_src_state'] == 'open'] = 'Still Open'\n",
    "    del pr_all['pr_merged_at']\n",
    "pr_all['merged_flag']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into different dataframes\n",
    "### All, open, closed, and slowest 20% of these 3 categories (6 dataframes total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: merged_flag, dtype: object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Isolate the different state PRs for now\n",
    "pr_open = pr_all.loc[pr_all['pr_src_state'] == 'open']\n",
    "pr_closed = pr_all.loc[pr_all['pr_src_state'] == 'closed']\n",
    "pr_merged = pr_all.loc[pr_all['merged_flag'] == 'Merged / Accepted']\n",
    "pr_not_merged = pr_all.loc[pr_all['merged_flag'] == 'Not Merged / Rejected']\n",
    "pr_closed['merged_flag']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframes that contain the slowest 20% pull requests of each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_id</th>\n",
       "      <th>pr_src_id</th>\n",
       "      <th>repo_name</th>\n",
       "      <th>pr_src_author_association</th>\n",
       "      <th>repo_group</th>\n",
       "      <th>pr_src_state</th>\n",
       "      <th>pr_created_at</th>\n",
       "      <th>pr_closed_at</th>\n",
       "      <th>created_year</th>\n",
       "      <th>created_month</th>\n",
       "      <th>...</th>\n",
       "      <th>lines_added</th>\n",
       "      <th>lines_removed</th>\n",
       "      <th>commit_count</th>\n",
       "      <th>file_count</th>\n",
       "      <th>average_days_between_responses</th>\n",
       "      <th>average_hours_between_responses</th>\n",
       "      <th>created_yearmonth</th>\n",
       "      <th>closed_yearmonth</th>\n",
       "      <th>merged_flag</th>\n",
       "      <th>percentile_rank_local</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [repo_id, pr_src_id, repo_name, pr_src_author_association, repo_group, pr_src_state, pr_created_at, pr_closed_at, created_year, created_month, closed_year, closed_month, pr_src_meta_label, pr_head_or_base, hours_to_close, days_to_close, hours_to_first_response, days_to_first_response, hours_to_last_response, days_to_last_response, first_response_time, last_response_time, average_time_between_responses, assigned_count, review_requested_count, labeled_count, subscribed_count, mentioned_count, referenced_count, closed_count, head_ref_force_pushed_count, merged_count, milestoned_count, unlabeled_count, head_ref_deleted_count, comment_count, lines_added, lines_removed, commit_count, file_count, average_days_between_responses, average_hours_between_responses, created_yearmonth, closed_yearmonth, merged_flag, percentile_rank_local]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 46 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering the 80th percentile slowest PRs\n",
    "\n",
    "def filter_20_per_slowest(input_df):\n",
    "    pr_slow20_filtered = pd.DataFrame()\n",
    "    pr_slow20_x = pd.DataFrame()\n",
    "    for value in repo_set: \n",
    "        if not pr_slow20_filtered.empty: \n",
    "            pr_slow20x = input_df.query('repo_id==@value')\n",
    "            pr_slow20x['percentile_rank_local'] = pr_slow20x.days_to_close.rank(pct=True)\n",
    "            pr_slow20x = pr_slow20x.query('percentile_rank_local >= .8', )\n",
    "            pr_slow20_filtered = pd.concat([pr_slow20x, pr_slow20_filtered]) \n",
    "            reponame = str(value)\n",
    "            filename = ''.join(['output/pr_slowest20pct', reponame, '.csv'])\n",
    "            pr_slow20x.to_csv(filename)\n",
    "        else: \n",
    "            # first time\n",
    "            pr_slow20_filtered = input_df.copy()\n",
    "            pr_slow20_filtered['percentile_rank_local'] = pr_slow20_filtered.days_to_close.rank(pct=True)\n",
    "            pr_slow20_filtered = pr_slow20_filtered.query('percentile_rank_local >= .8', )\n",
    "#     print(pr_slow20_filtered.describe())\n",
    "    return pr_slow20_filtered\n",
    "\n",
    "pr_slow20_open = filter_20_per_slowest(pr_open)\n",
    "pr_slow20_closed = filter_20_per_slowest(pr_closed)\n",
    "pr_slow20_merged = filter_20_per_slowest(pr_merged)\n",
    "pr_slow20_not_merged = filter_20_per_slowest(pr_not_merged)\n",
    "pr_slow20_all = filter_20_per_slowest(pr_all)\n",
    "pr_slow20_merged#.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m repo_id \u001b[38;5;129;01min\u001b[39;00m repo_set:\n\u001b[1;32m     20\u001b[0m     \n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m#find corresponding repo name from each repo_id \u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     repo_name \u001b[38;5;241m=\u001b[39m \u001b[43mpr_all\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpr_all\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrepo_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m#if competitor grouping is enabled turn all repo names, other than the ones in the 'not_aliased_repos' into an alias\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m display_grouping \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompetitors\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m repo_id \u001b[38;5;129;01min\u001b[39;00m not_aliased_repos:\n",
      "File \u001b[0;32m~/github/virtualenvs/phds/lib/python3.11/site-packages/pandas/core/indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/virtualenvs/phds/lib/python3.11/site-packages/pandas/core/indexing.py:1752\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index by location index with a non-integer key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[0;32m-> 1752\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_ixs(key, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m~/github/virtualenvs/phds/lib/python3.11/site-packages/pandas/core/indexing.py:1685\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1683\u001b[0m len_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis))\n\u001b[1;32m   1684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m len_axis \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39mlen_axis:\n\u001b[0;32m-> 1685\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle positional indexer is out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "#create dictionairy with number as the key and a letter as the value\n",
    "#this is used to alias repos when using 'compeitor' display grouping\n",
    "letters = []\n",
    "nums = []\n",
    "alpha = 'a'\n",
    "for i in range(0, 26): \n",
    "    letters.append(alpha) \n",
    "    alpha = chr(ord(alpha) + 1)\n",
    "    nums.append(i)\n",
    "letters = [x.upper() for x in letters]\n",
    "\n",
    "#create dict out of list of numbers and letters\n",
    "repo_alias_dict = {nums[i]: letters[i] for i in range(len(nums))}\n",
    "\n",
    "# create dict in the form {repo_id : repo_name}\n",
    "aliased_repos = []\n",
    "repo_dict = {}\n",
    "count = 0\n",
    "for repo_id in repo_set:\n",
    "    \n",
    "    #find corresponding repo name from each repo_id \n",
    "    repo_name = pr_all.loc[pr_all['repo_id'] == repo_id].iloc[0]['repo_name']\n",
    "    \n",
    "    #if competitor grouping is enabled turn all repo names, other than the ones in the 'not_aliased_repos' into an alias\n",
    "    if display_grouping == 'competitors' and not repo_id in not_aliased_repos:\n",
    "        repo_name =  'Repo ' + repo_alias_dict[count]\n",
    "        \n",
    "        #add repo_id to list of aliased repos, this is used for ordering\n",
    "        aliased_repos.append(repo_id)\n",
    "        count += 1\n",
    "        \n",
    "    #add repo_id and repo names as key value pairs into a dict, this is used to label the title of the visualizations\n",
    "    repo_dict.update({repo_id : repo_name})\n",
    "\n",
    "#gurantees that the non_aliased repos come first when display grouping is set as 'competitors'\n",
    "repo_list = not_aliased_repos + aliased_repos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Visualization Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bokeh'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbokeh\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpalettes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Colorblind, mpl, Category20\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbokeh\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayouts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gridplot\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbokeh\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mannotations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Title\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bokeh'"
     ]
    }
   ],
   "source": [
    "from bokeh.palettes import Colorblind, mpl, Category20\n",
    "from bokeh.layouts import gridplot\n",
    "from bokeh.models.annotations import Title\n",
    "from bokeh.io import export_png\n",
    "from bokeh.io import show, output_notebook\n",
    "from bokeh.models import ColumnDataSource, Legend, LabelSet, Range1d, LinearAxis, Label\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models.glyphs import Rect\n",
    "from bokeh.transform import dodge\n",
    "\n",
    "try:\n",
    "    colors = Colorblind[len(repo_set)]\n",
    "except:\n",
    "    colors = Colorblind[3]\n",
    "#mpl['Plasma'][len(repo_set)]\n",
    "#['A6CEE3','B2DF8A','33A02C','FB9A99']\n",
    "\n",
    "def remove_outliers(input_df, field, num_outliers_repo_map):\n",
    "    df_no_outliers = input_df.copy()\n",
    "    for repo_name, num_outliers in num_outliers_repo_map.items():\n",
    "        indices_to_drop = input_df.loc[input_df['repo_name'] == repo_name].nlargest(num_outliers, field).index\n",
    "        df_no_outliers = df_no_outliers.drop(index=indices_to_drop)\n",
    "    return df_no_outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.dates import DateFormatter\n",
    "import datetime as dt\n",
    "\n",
    "def visualize_mean_days_to_close(input_df, x_axis='closed_yearmonth', description='Closed', save_file=False, num_remove_outliers=0, drop_outliers_repo=None):\n",
    "\n",
    "    # Set the df you want to build the viz's for\n",
    "    driver_df = input_df.copy()\n",
    "    \n",
    "    driver_df = driver_df[['repo_id', 'repo_name', 'pr_src_id', 'created_yearmonth', 'closed_yearmonth', 'days_to_close']]\n",
    "\n",
    "    if save_file:\n",
    "        driver_df.to_csv('output/c.westw20small {}.csv'.format(description))\n",
    "    \n",
    "    driver_df_mean = driver_df.groupby(['repo_id', x_axis, 'repo_name'],as_index=False).mean()\n",
    "        \n",
    "    # Total PRS Closed\n",
    "    fig, ax = plt.subplots()\n",
    "    # the size of A4 paper\n",
    "    fig.set_size_inches(16, 8)\n",
    "    plotter = sns.lineplot(x=x_axis, y='days_to_close', style='repo_name', data=driver_df_mean, sort=True, legend='full', linewidth=2.5, hue='repo_name').set_title(\"Average Days to Close of {} Pull Requests, July 2017-January 2020\".format(description))  \n",
    "    if save_file:\n",
    "        fig.savefig('images/slow_20_mean {}.png'.format(description))\n",
    "    \n",
    "    # Copying array and deleting the outlier in the copy to re-visualize\n",
    "    def drop_n_largest(input_df, n, repo_name):\n",
    "        input_df_copy = input_df.copy()\n",
    "        indices_to_drop = input_df.loc[input_df['repo_name'] == 'amazon-freertos'].nlargest(n,'days_to_close').index\n",
    "        print(\"Indices to drop: {}\".format(indices_to_drop))\n",
    "        input_df_copy = input_df_copy.drop(index=indices_to_drop)\n",
    "        input_df_copy.loc[input_df['repo_name'] == repo_name]\n",
    "        return input_df_copy\n",
    "\n",
    "    if num_remove_outliers > 0 and drop_outliers_repo:\n",
    "        driver_df_mean_no_outliers = drop_n_largest(driver_df_mean, num_remove_outliers, drop_outliers_repo)\n",
    "    \n",
    "        # Total PRS Closed without outlier\n",
    "        fig, ax = plt.subplots()\n",
    "        # the size of A4 paper\n",
    "        fig.set_size_inches(16, 8)\n",
    "        plotter = sns.lineplot(x=x_axis, y='days_to_close', style='repo_name', data=driver_df_mean_no_outliers, sort=False, legend='full', linewidth=2.5, hue='repo_name').set_title(\"Average Days to Close among {} Pull Requests Without Outlier, July 2017-January 2020\".format(description))\n",
    "        plotterlabels = ax.set_xticklabels(driver_df_mean_no_outliers[x_axis], rotation=90, fontsize=8)\n",
    "        if save_file:\n",
    "            fig.savefig('images/slow_20_mean_no_outlier {}.png'.format(description))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize_mean_days_to_close(pr_closed, description='All Closed', save_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bokeh'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbokeh\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ColumnDataSource, FactorRange\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbokeh\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m factor_cmap\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvertical_grouped_bar\u001b[39m(input_df, repo_id, group_by \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmerged_flag\u001b[39m\u001b[38;5;124m'\u001b[39m, x_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclosed_year\u001b[39m\u001b[38;5;124m'\u001b[39m, y_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_commits\u001b[39m\u001b[38;5;124m'\u001b[39m, description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAll\u001b[39m\u001b[38;5;124m'\u001b[39m, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124mAverage Commit Counts Per Year for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m Pull Requests\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bokeh'"
     ]
    }
   ],
   "source": [
    "from bokeh.models import ColumnDataSource, FactorRange\n",
    "from bokeh.transform import factor_cmap\n",
    "\n",
    "def vertical_grouped_bar(input_df, repo_id, group_by = 'merged_flag', x_axis='closed_year', y_axis='num_commits', description='All', title=\"{}Average Commit Counts Per Year for {} Pull Requests\"):\n",
    "    \n",
    "    if type(repo_id) == type(repo_list):\n",
    "        repo_ids = repo_id\n",
    "    else:\n",
    "        repo_ids = [repo_id]\n",
    "    \n",
    "    for repo_id in repo_ids:\n",
    "    \n",
    "        output_notebook() # let bokeh display plot in jupyter cell output\n",
    "\n",
    "        driver_df = input_df.copy() # deep copy input data so we do not change the external dataframe \n",
    "\n",
    "        # Filter df by passed *repo_id* param\n",
    "        driver_df = driver_df.loc[driver_df['repo_id'] == repo_id]\n",
    "\n",
    "        # Change closed year to int so that doesn't display as 2019.0 for example\n",
    "        driver_df['closed_year'] = driver_df['closed_year'].astype(int).astype(str)\n",
    "\n",
    "        # contains the closed years\n",
    "        x_groups = sorted(list(driver_df[x_axis].unique()))\n",
    "\n",
    "        # inner groups on x_axis they are merged and not_merged\n",
    "        groups = list(driver_df[group_by].unique())\n",
    "\n",
    "        # setup color pallete\n",
    "        try:\n",
    "            colors = mpl['Plasma'][len(groups)]\n",
    "        except:\n",
    "            colors = [mpl['Plasma'][3][0]] + [mpl['Plasma'][3][1]]\n",
    "\n",
    "        merged_avg_values = list(driver_df.loc[driver_df[group_by] == 'Merged / Accepted'].groupby([x_axis],as_index=False).mean().round(1)['commit_count'])\n",
    "        not_merged_avg_values = list(driver_df.loc[driver_df[group_by] == 'Not Merged / Rejected'].groupby([x_axis],as_index=False).mean().round(1)['commit_count'])\n",
    "\n",
    "\n",
    "        # Setup data in format for grouped bar chart\n",
    "        data = {\n",
    "                'years'                   : x_groups,\n",
    "                'Merged / Accepted'       : merged_avg_values,\n",
    "                'Not Merged / Rejected'   : not_merged_avg_values,\n",
    "            }\n",
    "\n",
    "        x = [ (year, pr_state) for year in x_groups for pr_state in groups ]\n",
    "        counts = sum(zip(data['Merged / Accepted'], data['Not Merged / Rejected']), ())\n",
    "\n",
    "        source = ColumnDataSource(data=dict(x=x, counts=counts))\n",
    "\n",
    "        title_beginning = '{}: '.format(repo_dict[repo_id])\n",
    "        title=title.format(title_beginning, description)\n",
    "        \n",
    "        plot_width = len(x_groups) * 300\n",
    "        title_text_font_size = 16 \n",
    "        \n",
    "        if (len(title) * title_text_font_size / 2) > plot_width:\n",
    "            plot_width = int(len(title) * title_text_font_size / 2) + 40\n",
    "        \n",
    "        p = figure(x_range=FactorRange(*x), plot_height=450, plot_width=plot_width, title=title, y_range=(0, max(merged_avg_values + not_merged_avg_values)*1.15), toolbar_location=None)\n",
    "\n",
    "        # Vertical bar glyph\n",
    "        p.vbar(x='x', top='counts', width=0.9, source=source, line_color=\"white\",\n",
    "               fill_color=factor_cmap('x', palette=colors, factors=groups, start=1, end=2))\n",
    "\n",
    "        # Data label \n",
    "        labels = LabelSet(x='x', y='counts', text='counts',# y_offset=-8, x_offset=34,\n",
    "                  text_font_size=\"12pt\", text_color=\"black\",\n",
    "                  source=source, text_align='center')\n",
    "        p.add_layout(labels)\n",
    "\n",
    "        p.y_range.start = 0\n",
    "        p.x_range.range_padding = 0.1\n",
    "        p.xaxis.major_label_orientation = 1\n",
    "        p.xgrid.grid_line_color = None\n",
    "\n",
    "        p.yaxis.axis_label = 'Average Commits / Pull Request'\n",
    "        p.xaxis.axis_label = 'Year Closed'\n",
    "\n",
    "        p.title.align = \"center\"\n",
    "        p.title.text_font_size = \"{}px\".format(title_text_font_size)\n",
    "\n",
    "        p.xaxis.axis_label_text_font_size = \"16px\"\n",
    "        p.xaxis.major_label_text_font_size = \"15px\"\n",
    "\n",
    "        p.yaxis.axis_label_text_font_size = \"15px\"\n",
    "        p.yaxis.major_label_text_font_size = \"15px\"\n",
    "        \n",
    "        plot = p\n",
    "\n",
    "        p = figure(width = plot_width, height=200, margin = (0, 0, 0, 0))\n",
    "        caption = \"This graph shows the average commits per pull requests over an entire year, for merged and not merged pull requests.\"\n",
    "        p.add_layout(Label(\n",
    "        x = 0, # Change to shift caption left or right\n",
    "        y = 160, \n",
    "        x_units = 'screen',\n",
    "        y_units = 'screen',\n",
    "        text='{}'.format(caption),\n",
    "        text_font = 'times', # Use same font as paper\n",
    "        text_font_size = '15pt',\n",
    "        render_mode='css'\n",
    "        ))\n",
    "        p.outline_line_color = None\n",
    "\n",
    "        caption_plot = p\n",
    "\n",
    "        grid = gridplot([[plot], [caption_plot]])\n",
    "        \n",
    "        show(grid)\n",
    "\n",
    "\n",
    "        #show(p)\n",
    "\n",
    "        if save_files:\n",
    "            export_png(grid, filename=\"./images/v_grouped_bar/v_grouped_bar__{}_PRs__yaxis_{}__repo_{}.png\".format(description, y_axis, repo_dict[repo_id]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vertical_grouped_bar(pr_all, repo_id=repo_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vertical_grouped_bar_line_counts(input_df, repo_id, x_axis='closed_year', y_max1=600000, y_max2=1000, description=\"\", title =\"\", save_file=False):\n",
    "    output_notebook() # let bokeh display plot in jupyter cell output\n",
    "    \n",
    "    driver_df = input_df.copy() # deep copy input data so we do not change the external dataframe \n",
    "    \n",
    "    # Filter df by passed *repo_id* param\n",
    "    driver_df = driver_df.loc[driver_df['repo_id'] == repo_id]\n",
    "        \n",
    "    # Change closed year to int so that doesn't display as 2019.0 for example\n",
    "    driver_df['closed_year'] = driver_df['closed_year'].astype(int).astype(str)\n",
    "    \n",
    "    # contains the closed years\n",
    "    x_groups = sorted(list(driver_df[x_axis].unique()))\n",
    "    \n",
    "    groups = ['Lines Added', 'Lines Removed', 'Files Changed']\n",
    "    \n",
    "    # setup color pallete\n",
    "    colors = mpl['Plasma'][3]\n",
    "    \n",
    "    display(pr_all[pr_all['lines_added'].notna()])#.groupby([x_axis],as_index=False).mean())\n",
    "        \n",
    "    files_avg_values = list(driver_df.groupby([x_axis],as_index=False).mean().round(1)['file_count'])\n",
    "    added_avg_values = list(driver_df.groupby([x_axis],as_index=False).mean().round(1)['lines_added'])\n",
    "    removed_avg_values = list(driver_df.groupby([x_axis],as_index=False).mean().round(1)['lines_removed'])\n",
    "    display(driver_df.groupby([x_axis],as_index=False).mean())\n",
    "    print(files_avg_values)\n",
    "    print(added_avg_values)\n",
    "    print(removed_avg_values)\n",
    "    \n",
    "        \n",
    "    # Setup data in format for grouped bar chart\n",
    "    data = {\n",
    "            'years' : x_groups,\n",
    "            'Lines Added'   : added_avg_values,\n",
    "            'Lines Removed' : removed_avg_values,\n",
    "            'Files Changed' : files_avg_values\n",
    "        }\n",
    "\n",
    "    x = [ (year, pr_state) for year in x_groups for pr_state in groups ]\n",
    "    line_counts = sum(zip(data['Lines Added'], data['Lines Removed'], [0]*len(x_groups)), ())\n",
    "    file_counts = sum(zip([0]*len(x_groups),[0]*len(x_groups),data['Files Changed']), ())\n",
    "    print(line_counts)\n",
    "    print(file_counts)\n",
    "    \n",
    "    source = ColumnDataSource(data=dict(x=x, line_counts=line_counts, file_counts=file_counts))\n",
    "\n",
    "    if y_max1:\n",
    "        p = figure(x_range=FactorRange(*x), plot_height=450, plot_width=700, title=title.format(description), y_range=(0,y_max1))\n",
    "    else:\n",
    "        p = figure(x_range=FactorRange(*x), plot_height=450, plot_width=700, title=title.format(description))\n",
    "                \n",
    "    # Setting the second y axis range name and range\n",
    "    p.extra_y_ranges = {\"file_counts\": Range1d(start=0, end=y_max2)}\n",
    "    \n",
    "    # Adding the second axis to the plot.  \n",
    "    p.add_layout(LinearAxis(y_range_name=\"file_counts\"), 'right')\n",
    "    \n",
    "    # Data label for line counts\n",
    "    labels = LabelSet(x='x', y='line_counts', text='line_counts',y_offset=8,# x_offset=34,\n",
    "              text_font_size=\"10pt\", text_color=\"black\",\n",
    "              source=source, text_align='center')\n",
    "    p.add_layout(labels)\n",
    "\n",
    "    # Vertical bar glyph for line counts\n",
    "    p.vbar(x='x', top='line_counts', width=0.9, source=source, line_color=\"white\",\n",
    "           fill_color=factor_cmap('x', palette=colors, factors=groups, start=1, end=2))\n",
    "    \n",
    "    # Data label for file counts\n",
    "    labels = LabelSet(x='x', y='file_counts', text='file_counts', y_offset=0, #x_offset=34,\n",
    "              text_font_size=\"10pt\", text_color=\"black\",\n",
    "              source=source, text_align='center', y_range_name=\"file_counts\")\n",
    "    p.add_layout(labels)\n",
    "    \n",
    "    # Vertical bar glyph for file counts\n",
    "    p.vbar(x='x', top='file_counts', width=0.9, source=source, line_color=\"white\",\n",
    "           fill_color=factor_cmap('x', palette=colors, factors=groups, start=1, end=2), y_range_name=\"file_counts\")\n",
    "\n",
    "    p.left[0].formatter.use_scientific = False\n",
    "    p.y_range.start = 0\n",
    "    p.x_range.range_padding = 0.1\n",
    "    p.xaxis.major_label_orientation = 1\n",
    "    p.xgrid.grid_line_color = None\n",
    "    \n",
    "    p.yaxis.axis_label = 'Average Commits / Pull Request'\n",
    "    p.xaxis.axis_label = 'Year Closed'\n",
    "    \n",
    "    p.title.align = \"center\"\n",
    "    p.title.text_font_size = \"16px\"\n",
    "    \n",
    "    p.xaxis.axis_label_text_font_size = \"16px\"\n",
    "    p.xaxis.major_label_text_font_size = \"16px\"\n",
    "    \n",
    "    p.yaxis.axis_label_text_font_size = \"16px\"\n",
    "    p.yaxis.major_label_text_font_size = \"16px\"\n",
    "    \n",
    "    show(p)\n",
    "    \n",
    "    if save_files:\n",
    "        export_png(p, filename=\"./images/v_grouped_bar/v_grouped_bar__{}_PRs__yaxis_{}__repo_{}.png\".format(description, y_axis, repo_dict[repo_id]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" THIS VIZ IS NOT READY YET , BUT UNCOMMENT LINE BELOW IF YOU WANT TO SEE\"\"\"\n",
    "# vertical_grouped_bar_line_counts(pr_all, description='All', title=\"Average Size Metrics Per Year for {} Merged Pull Requests in Master\", save_file=False, y_max1=580000, y_max2=1100)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def horizontal_stacked_bar(input_df, repo_id, group_by='merged_flag', x_axis='comment_count', description=\"All Closed\", y_axis='closed_year', title=\"Mean Comments for {} Pull Requests\"):\n",
    "    \n",
    "    if type(repo_id) == type(repo_list):\n",
    "        repo_ids = repo_id\n",
    "    else:\n",
    "        repo_ids = [repo_id]\n",
    "    \n",
    "    for repo_id in repo_ids:\n",
    "    \n",
    "        driver_df = input_df.copy()\n",
    "\n",
    "        driver_df = driver_df.loc[driver_df['repo_id'] == repo_id]\n",
    "\n",
    "        output_notebook()\n",
    "\n",
    "        try:\n",
    "            y_groups = sorted(list(driver_df[y_axis].unique()))\n",
    "        except:\n",
    "            y_groups = [repo_id]\n",
    "\n",
    "        groups = driver_df[group_by].unique()\n",
    "        try:\n",
    "            colors = mpl['Plasma'][len(groups)]\n",
    "        except:\n",
    "            colors = [mpl['Plasma'][3][0]] + [mpl['Plasma'][3][1]]\n",
    "\n",
    "        len_not_merged = len(driver_df.loc[driver_df['merged_flag'] == 'Not Merged / Rejected'])\n",
    "        len_merged = len(driver_df.loc[driver_df['merged_flag'] == 'Merged / Accepted'])\n",
    "\n",
    "        title_beginning = '{}: '.format(repo_dict[repo_id]) \n",
    "        plot_width = 650\n",
    "        p = figure(y_range=y_groups, plot_height=450, plot_width=plot_width, # y_range=y_groups,#(pr_all[y_axis].min(),pr_all[y_axis].max()) #y_axis_type=\"datetime\",\n",
    "                   title='{} {}'.format(title_beginning, title.format(description)), toolbar_location=None)\n",
    "\n",
    "        possible_maximums= []\n",
    "        for y_value in y_groups:\n",
    "\n",
    "            y_merged_data = driver_df.loc[(driver_df[y_axis] == y_value) & (driver_df['merged_flag'] == 'Merged / Accepted')]\n",
    "            y_not_merged_data = driver_df.loc[(driver_df[y_axis] == y_value) & (driver_df['merged_flag'] == 'Not Merged / Rejected')]\n",
    "\n",
    "            if len(y_merged_data) > 0:\n",
    "                y_merged_data[x_axis + '_mean'] = round(y_merged_data[x_axis].mean()[-1],1)\n",
    "            else:\n",
    "                y_merged_data[x_axis + '_mean'] = 0.00\n",
    "\n",
    "            if len(y_not_merged_data) > 0:\n",
    "                y_not_merged_data[x_axis + '_mean'] = round(y_not_merged_data[x_axis].mean()[-1],1)\n",
    "            else:\n",
    "                y_not_merged_data[x_axis + '_mean'] = 0\n",
    "\n",
    "            not_merged_source = ColumnDataSource(y_not_merged_data)\n",
    "            merged_source = ColumnDataSource(y_merged_data)\n",
    "\n",
    "            possible_maximums.append(max(y_not_merged_data[x_axis + '_mean']))\n",
    "            possible_maximums.append(max(y_merged_data[x_axis + '_mean']))\n",
    "\n",
    "            # mean comment count for merged\n",
    "            merged_comment_count_glyph = p.hbar(y=dodge(y_axis, -0.1, range=p.y_range), left=0, right=x_axis + '_mean', height=0.04*len(driver_df[y_axis].unique()), \n",
    "                                         source=merged_source, fill_color=\"black\")#,legend_label=\"Mean Days to Close\",\n",
    "            # Data label \n",
    "            labels = LabelSet(x=x_axis + '_mean', y=dodge(y_axis, -0.1, range=p.y_range), text=x_axis + '_mean', y_offset=-8, x_offset=34,\n",
    "                      text_font_size=\"12pt\", text_color=\"black\",\n",
    "                      source=merged_source, text_align='center')\n",
    "            p.add_layout(labels)\n",
    "            # mean comment count For nonmerged\n",
    "            not_merged_comment_count_glyph = p.hbar(y=dodge(y_axis, 0.1, range=p.y_range), left=0, right=x_axis + '_mean', \n",
    "                                         height=0.04*len(driver_df[y_axis].unique()), source=not_merged_source, fill_color=\"#e84d60\")#legend_label=\"Mean Days to Close\",\n",
    "            # Data label \n",
    "            labels = LabelSet(x=x_axis + '_mean', y=dodge(y_axis, 0.1, range=p.y_range), text=x_axis + '_mean', y_offset=-8, x_offset=34,\n",
    "                      text_font_size=\"12pt\", text_color=\"#e84d60\",\n",
    "                      source=not_merged_source, text_align='center')\n",
    "            p.add_layout(labels)\n",
    "\n",
    "    #         p.y_range.range_padding = 0.1\n",
    "        p.ygrid.grid_line_color = None\n",
    "        p.legend.location = \"bottom_right\"\n",
    "        p.axis.minor_tick_line_color = None\n",
    "        p.outline_line_color = None\n",
    "        p.xaxis.axis_label = 'Average Comments / Pull Request'\n",
    "        p.yaxis.axis_label = 'Repository' if y_axis == 'repo_name' else 'Year Closed' if y_axis == 'closed_year' else ''\n",
    "\n",
    "        legend = Legend(\n",
    "                items=[\n",
    "                    (\"Merged Pull Request Mean Comment Count\", [merged_comment_count_glyph]),\n",
    "                    (\"Rejected Pull Request Mean Comment Count\", [not_merged_comment_count_glyph])\n",
    "                ],\n",
    "\n",
    "                location='center', \n",
    "                orientation='vertical',\n",
    "                border_line_color=\"black\"\n",
    "            )\n",
    "        p.add_layout(legend, \"below\")\n",
    "\n",
    "        p.title.text_font_size = \"16px\"\n",
    "        p.title.align = \"center\"\n",
    "\n",
    "        p.xaxis.axis_label_text_font_size = \"16px\"\n",
    "        p.xaxis.major_label_text_font_size = \"16px\"\n",
    "\n",
    "        p.yaxis.axis_label_text_font_size = \"16px\"\n",
    "        p.yaxis.major_label_text_font_size = \"16px\"\n",
    "\n",
    "        p.x_range = Range1d(0, max(possible_maximums)*1.15)\n",
    "        \n",
    "        plot = p\n",
    "        \n",
    "        p = figure(width = plot_width, height=200, margin = (0, 0, 0, 0))\n",
    "        caption = \"This graph shows the average number of comments per merged or not merged pull request.\"\n",
    "        p.add_layout(Label(\n",
    "        x = 0, # Change to shift caption left or right\n",
    "        y = 160, \n",
    "        x_units = 'screen',\n",
    "        y_units = 'screen',\n",
    "        text='{}'.format(caption),\n",
    "        text_font = 'times', # Use same font as paper\n",
    "        text_font_size = '15pt',\n",
    "        render_mode='css'\n",
    "        ))\n",
    "        p.outline_line_color = None\n",
    "\n",
    "        caption_plot = p\n",
    "\n",
    "        grid = gridplot([[plot], [caption_plot]])\n",
    "\n",
    "        show(grid)\n",
    "\n",
    "        #show(p, plot_width=1200, plot_height=300*len(y_groups) + 300)\n",
    "\n",
    "        if save_files:\n",
    "            repo_extension = 'All' if not repo_name else repo_name\n",
    "            export_png(grid, filename=\"./images/h_stacked_bar_mean_comments_merged_status/mean_comments_merged_status__{}_PRs__yaxis_{}__repo_{}.png\".format(description, y_axis, repo_dict[repo_id]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#horizontal_stacked_bar(pr_closed, repo_id=repo_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merged_ratio_vertical_grouped_bar(data_dict, repo_id, x_axis='closed_year', description=\"All Closed\", title=\"Count of {} Pull Requests by Merged Status\"):\n",
    "    \n",
    "    if type(repo_id) == type(repo_list):\n",
    "        repo_ids = repo_id\n",
    "    else:\n",
    "        repo_ids = [repo_id]\n",
    "    \n",
    "    for repo_id in repo_ids:\n",
    "        \n",
    "        output_notebook()\n",
    "\n",
    "        colors = mpl['Plasma'][6]\n",
    "\n",
    "        #if repo_name == 'mbed-os':\n",
    "            #colors = colors[::-1]\n",
    "\n",
    "        for data_desc, input_df in data_dict.items():\n",
    "            x_groups = sorted(list(input_df[x_axis].astype(str).unique()))\n",
    "            break\n",
    "\n",
    "        plot_width = 315 * len(x_groups)\n",
    "        title_beginning = repo_dict[repo_id] \n",
    "        p = figure(x_range=x_groups, plot_height=350, plot_width=plot_width,  \n",
    "                   title='{}: {}'.format(title_beginning, title.format(description)), toolbar_location=None)\n",
    "\n",
    "        dodge_amount = 0.12\n",
    "        color_index = 0\n",
    "        x_offset = 50\n",
    "\n",
    "        all_totals = []\n",
    "        for data_desc, input_df in data_dict.items():\n",
    "            driver_df = input_df.copy()\n",
    "\n",
    "            driver_df[x_axis] = driver_df[x_axis].astype(str)\n",
    "\n",
    "            groups = sorted(list(driver_df['merged_flag'].unique()))\n",
    "\n",
    "            driver_df = driver_df.loc[driver_df['repo_id'] == repo_id]\n",
    "\n",
    "            len_merged = []\n",
    "            zeros = []\n",
    "            len_not_merged = []\n",
    "            totals = []\n",
    "\n",
    "            for x_group in x_groups:\n",
    "\n",
    "                len_merged_entry = len(driver_df.loc[(driver_df['merged_flag'] == 'Merged / Accepted') & (driver_df[x_axis] == x_group)])\n",
    "                totals += [len(driver_df.loc[(driver_df['merged_flag'] == 'Not Merged / Rejected') & (driver_df[x_axis] == x_group)]) + len_merged_entry]\n",
    "                len_not_merged += [len(driver_df.loc[(driver_df['merged_flag'] == 'Not Merged / Rejected') & (driver_df[x_axis] == x_group)])]\n",
    "                len_merged += [len_merged_entry]\n",
    "                zeros.append(0)\n",
    "\n",
    "            data = {'X': x_groups}\n",
    "            for group in groups:\n",
    "                data[group] = []\n",
    "                for x_group in x_groups:\n",
    "                    data[group] += [len(driver_df.loc[(driver_df['merged_flag'] == group) & (driver_df[x_axis] == x_group)])]\n",
    "\n",
    "            data['len_merged'] = len_merged\n",
    "            data['len_not_merged'] = len_not_merged\n",
    "            data['totals'] = totals\n",
    "            data['zeros'] = zeros\n",
    "\n",
    "            if data_desc == \"All\":\n",
    "                all_totals = totals\n",
    "\n",
    "            source = ColumnDataSource(data)\n",
    "\n",
    "            stacked_bar = p.vbar_stack(groups, x=dodge('X', dodge_amount, range=p.x_range), width=0.2, source=source, color=colors[1:3], legend_label=[f\"{data_desc} \" + \"%s\" % x for x in groups])\n",
    "            # Data label for merged\n",
    "\n",
    "            p.add_layout(\n",
    "                LabelSet(x=dodge('X', dodge_amount, range=p.x_range), y='zeros', text='len_merged', y_offset=2, x_offset=x_offset,\n",
    "                      text_font_size=\"12pt\", text_color=colors[1:3][0],\n",
    "                      source=source, text_align='center')\n",
    "            )\n",
    "            if min(data['totals']) < 400:\n",
    "                y_offset = 15\n",
    "            else:\n",
    "                y_offset = 0\n",
    "            # Data label for not merged\n",
    "            p.add_layout(\n",
    "                LabelSet(x=dodge('X', dodge_amount, range=p.x_range), y='totals', text='len_not_merged', y_offset=y_offset, x_offset=x_offset,\n",
    "                      text_font_size=\"12pt\", text_color=colors[1:3][1],\n",
    "                      source=source, text_align='center')\n",
    "            )\n",
    "            # Data label for total\n",
    "            p.add_layout(\n",
    "                LabelSet(x=dodge('X', dodge_amount, range=p.x_range), y='totals', text='totals', y_offset=0, x_offset=0,\n",
    "                      text_font_size=\"12pt\", text_color='black',\n",
    "                      source=source, text_align='center')\n",
    "            )\n",
    "            dodge_amount *= -1\n",
    "            colors = colors[::-1]\n",
    "            x_offset *= -1\n",
    "\n",
    "        p.y_range = Range1d(0,  max(all_totals)*1.4)\n",
    "\n",
    "        p.xgrid.grid_line_color = None\n",
    "        p.legend.location = \"top_center\"\n",
    "        p.legend.orientation=\"horizontal\"\n",
    "        p.axis.minor_tick_line_color = None\n",
    "        p.outline_line_color = None\n",
    "        p.yaxis.axis_label = 'Count of Pull Requests'\n",
    "        p.xaxis.axis_label = 'Repository' if x_axis == 'repo_name' else 'Year Closed' if x_axis == 'closed_year' else ''\n",
    "\n",
    "        p.title.align = \"center\"\n",
    "        p.title.text_font_size = \"16px\"\n",
    "\n",
    "        p.xaxis.axis_label_text_font_size = \"16px\"\n",
    "        p.xaxis.major_label_text_font_size = \"16px\"\n",
    "\n",
    "        p.yaxis.axis_label_text_font_size = \"16px\"\n",
    "        p.yaxis.major_label_text_font_size = \"16px\"\n",
    "\n",
    "        p.outline_line_color = None\n",
    "        \n",
    "        plot = p\n",
    "        \n",
    "        p = figure(width = plot_width, height=200, margin = (0, 0, 0, 0))\n",
    "        caption = \"This graph shows the number of closed pull requests per year in four different categories. These four categories are All Merged, All Not Merged, Slowest 20% Merged, and Slowest 20% Not Merged.\"\n",
    "        p.add_layout(Label(\n",
    "        x = 0, # Change to shift caption left or right\n",
    "        y = 160, \n",
    "        x_units = 'screen',\n",
    "        y_units = 'screen',\n",
    "        text='{}'.format(caption),\n",
    "        text_font = 'times', # Use same font as paper\n",
    "        text_font_size = '15pt',\n",
    "        render_mode='css'\n",
    "        ))\n",
    "        p.outline_line_color = None\n",
    "\n",
    "        caption_plot = p\n",
    "\n",
    "        grid = gridplot([[plot], [caption_plot]])\n",
    "\n",
    "        show(grid)\n",
    "        \n",
    "        if save_files:\n",
    "            repo_extension = 'All' if not repo_id else repo_id\n",
    "            export_png(grid, filename=\"./images/v_stacked_bar_merged_status_count/stacked_bar_merged_status_count__{}_PRs__xaxis_{}__repo_{}.png\".format(description, x_axis, repo_dict[repo_id]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for repo_name in pr_all['repo_name'].unique():\n",
    "#erged_ratio_vertical_grouped_bar({'All':pr_closed,'Slowest 20%':pr_slow20_not_merged.append(pr_slow20_merged,ignore_index=True)}, repo_id = repo_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_mean_response_times(input_df, repo_id, time_unit='days', x_max=95,  y_axis='closed_year', description=\"All Closed\", legend_position=(410, 10)):\n",
    "    \n",
    "    if type(repo_id) == type(repo_list):\n",
    "        repo_ids = repo_id\n",
    "    else:\n",
    "        repo_ids = [repo_id]\n",
    "    \n",
    "    for repo_id in repo_ids:\n",
    "\n",
    "        output_notebook() # let bokeh show plot in jupyter cell output\n",
    "\n",
    "        driver_df = input_df.copy()[['repo_name', 'repo_id', 'merged_flag', y_axis, time_unit + '_to_first_response', time_unit + '_to_last_response', \n",
    "                                     time_unit + '_to_close']] # deep copy input data so we do not alter the external dataframe\n",
    "\n",
    "        # filter by repo_id param\n",
    "        driver_df = driver_df.loc[driver_df['repo_id'] == repo_id]\n",
    "\n",
    "        title_beginning = '{}: '.format(repo_dict[repo_id])\n",
    "        plot_width = 950\n",
    "        p = figure(toolbar_location=None, y_range=sorted(driver_df[y_axis].unique()), plot_width=plot_width, \n",
    "                   plot_height=450,#75*len(driver_df[y_axis].unique()),\n",
    "                   title=\"{}Mean Response Times for Pull Requests {}\".format(title_beginning, description))\n",
    "\n",
    "        first_response_glyphs = []\n",
    "        last_response_glyphs = []\n",
    "        merged_days_to_close_glyphs = []\n",
    "        not_merged_days_to_close_glyphs = []\n",
    "\n",
    "        possible_maximums = []\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for y_value in driver_df[y_axis].unique():\n",
    "\n",
    "            y_merged_data = driver_df.loc[(driver_df[y_axis] == y_value) & (driver_df['merged_flag'] == 'Merged / Accepted')]\n",
    "            y_not_merged_data = driver_df.loc[(driver_df[y_axis] == y_value) & (driver_df['merged_flag'] == 'Not Merged / Rejected')]\n",
    "\n",
    "            y_merged_data[time_unit + '_to_first_response_mean'] = y_merged_data[time_unit + '_to_first_response'].mean().round(1) if len(y_merged_data) > 0 else 0.00\n",
    "            y_merged_data[time_unit + '_to_last_response_mean'] = y_merged_data[time_unit + '_to_last_response'].mean().round(1) if len(y_merged_data) > 0 else 0.00\n",
    "            y_merged_data[time_unit + '_to_close_mean'] = y_merged_data[time_unit + '_to_close'].mean().round(1) if len(y_merged_data) > 0 else 0.00\n",
    "\n",
    "            y_not_merged_data[time_unit + '_to_first_response_mean'] = y_not_merged_data[time_unit + '_to_first_response'].mean().round(1) if len(y_not_merged_data) > 0 else 0.00\n",
    "            y_not_merged_data[time_unit + '_to_last_response_mean'] = y_not_merged_data[time_unit + '_to_last_response'].mean().round(1) if len(y_not_merged_data) > 0 else 0.00\n",
    "            y_not_merged_data[time_unit + '_to_close_mean'] = y_not_merged_data[time_unit + '_to_close'].mean().round(1) if len(y_not_merged_data) > 0 else 0.00\n",
    "\n",
    "            possible_maximums.append(max(y_merged_data[time_unit + '_to_close_mean']))\n",
    "            possible_maximums.append(max(y_not_merged_data[time_unit + '_to_close_mean']))\n",
    "            \n",
    "            maximum = max(possible_maximums)*1.15\n",
    "            ideal_difference = maximum*0.064\n",
    "            \n",
    "        for y_value in driver_df[y_axis].unique():\n",
    "\n",
    "            y_merged_data = driver_df.loc[(driver_df[y_axis] == y_value) & (driver_df['merged_flag'] == 'Merged / Accepted')]\n",
    "            y_not_merged_data = driver_df.loc[(driver_df[y_axis] == y_value) & (driver_df['merged_flag'] == 'Not Merged / Rejected')]\n",
    "\n",
    "            y_merged_data[time_unit + '_to_first_response_mean'] = y_merged_data[time_unit + '_to_first_response'].mean().round(1) if len(y_merged_data) > 0 else 0.00\n",
    "            y_merged_data[time_unit + '_to_last_response_mean'] = y_merged_data[time_unit + '_to_last_response'].mean().round(1) if len(y_merged_data) > 0 else 0.00\n",
    "            y_merged_data[time_unit + '_to_close_mean'] = y_merged_data[time_unit + '_to_close'].mean().round(1) if len(y_merged_data) > 0 else 0.00\n",
    "\n",
    "            y_not_merged_data[time_unit + '_to_first_response_mean'] = y_not_merged_data[time_unit + '_to_first_response'].mean().round(1) if len(y_not_merged_data) > 0 else 0.00\n",
    "            y_not_merged_data[time_unit + '_to_last_response_mean'] = y_not_merged_data[time_unit + '_to_last_response'].mean().round(1) if len(y_not_merged_data) > 0 else 0.00\n",
    "            y_not_merged_data[time_unit + '_to_close_mean'] = y_not_merged_data[time_unit + '_to_close'].mean().round(1) if len(y_not_merged_data) > 0 else 0.00\n",
    "\n",
    "            not_merged_source = ColumnDataSource(y_not_merged_data)\n",
    "            merged_source = ColumnDataSource(y_merged_data)\n",
    "\n",
    "            # mean PR length for merged\n",
    "            merged_days_to_close_glyph = p.hbar(y=dodge(y_axis, -0.1, range=p.y_range), left=0, right=time_unit + '_to_close_mean', height=0.04*len(driver_df[y_axis].unique()), \n",
    "                                         source=merged_source, fill_color=\"black\")#,legend_label=\"Mean Days to Close\",\n",
    "            merged_days_to_close_glyphs.append(merged_days_to_close_glyph)\n",
    "            # Data label \n",
    "            labels = LabelSet(x=time_unit + '_to_close_mean', y=dodge(y_axis, -0.1, range=p.y_range), text=time_unit + '_to_close_mean', y_offset=-8, x_offset=34, #34\n",
    "                      text_font_size=\"12pt\", text_color=\"black\",\n",
    "                      source=merged_source, text_align='center')\n",
    "            p.add_layout(labels)\n",
    "\n",
    "\n",
    "            # mean PR length For nonmerged\n",
    "            not_merged_days_to_close_glyph = p.hbar(y=dodge(y_axis, 0.1, range=p.y_range), left=0, right=time_unit + '_to_close_mean', \n",
    "                                         height=0.04*len(driver_df[y_axis].unique()), source=not_merged_source, fill_color=\"#e84d60\")#legend_label=\"Mean Days to Close\",\n",
    "            not_merged_days_to_close_glyphs.append(not_merged_days_to_close_glyph)\n",
    "            # Data label \n",
    "            labels = LabelSet(x=time_unit + '_to_close_mean', y=dodge(y_axis, 0.1, range=p.y_range), text=time_unit + '_to_close_mean', y_offset=-8, x_offset=44,\n",
    "                      text_font_size=\"12pt\", text_color=\"#e84d60\",\n",
    "                      source=not_merged_source, text_align='center')\n",
    "            p.add_layout(labels)\n",
    "\n",
    "            \n",
    "            #if the difference between two values is less than 6.4 percent move the second one to the right 30 pixels\n",
    "            if (max(y_merged_data[time_unit + '_to_last_response_mean']) - max(y_merged_data[time_unit + '_to_first_response_mean'])) < ideal_difference:\n",
    "                merged_x_offset = 30\n",
    "            else:\n",
    "                merged_x_offset = 0\n",
    "                \n",
    "            #if the difference between two values is less than 6.4 percent move the second one to the right 30 pixels\n",
    "            if (max(y_not_merged_data[time_unit + '_to_last_response_mean']) - max(y_not_merged_data[time_unit + '_to_first_response_mean'])) < ideal_difference:\n",
    "                not_merged_x_offset = 30\n",
    "            else:\n",
    "                not_merged_x_offset = 0\n",
    "                \n",
    "            #if there is only one bar set the y_offsets so the labels will not overlap the bars\n",
    "            if len(driver_df[y_axis].unique()) == 1:\n",
    "                merged_y_offset = -65\n",
    "                not_merged_y_offset = 45\n",
    "            else:\n",
    "                merged_y_offset = -45\n",
    "                not_merged_y_offset = 25\n",
    "            \n",
    "            \n",
    "            # mean time to first response\n",
    "            glyph = Rect(x=time_unit + '_to_first_response_mean', y=dodge(y_axis, -0.1, range=p.y_range), width=x_max/100, height=0.08*len(driver_df[y_axis].unique()), fill_color=colors[0])\n",
    "            first_response_glyph = p.add_glyph(merged_source, glyph)\n",
    "            first_response_glyphs.append(first_response_glyph)\n",
    "            # Data label \n",
    "            labels = LabelSet(x=time_unit + '_to_first_response_mean', y=dodge(y_axis, 0, range=p.y_range),text=time_unit + '_to_first_response_mean',x_offset = 0, y_offset=merged_y_offset,#-60,\n",
    "                      text_font_size=\"12pt\", text_color=colors[0],\n",
    "                      source=merged_source, text_align='center')\n",
    "            p.add_layout(labels)\n",
    "\n",
    "            #for nonmerged\n",
    "            glyph = Rect(x=time_unit + '_to_first_response_mean', y=dodge(y_axis, 0.1, range=p.y_range), width=x_max/100, height=0.08*len(driver_df[y_axis].unique()), fill_color=colors[0])\n",
    "            first_response_glyph = p.add_glyph(not_merged_source, glyph)\n",
    "            first_response_glyphs.append(first_response_glyph)\n",
    "            # Data label \n",
    "            labels = LabelSet(x=time_unit + '_to_first_response_mean', y=dodge(y_axis, 0, range=p.y_range),text=time_unit + '_to_first_response_mean',x_offset = 0, y_offset=not_merged_y_offset,#40,\n",
    "                              text_font_size=\"12pt\", text_color=colors[0],\n",
    "                      source=not_merged_source, text_align='center')\n",
    "            p.add_layout(labels)\n",
    "\n",
    "\n",
    "            # mean time to last response\n",
    "            glyph = Rect(x=time_unit + '_to_last_response_mean', y=dodge(y_axis, -0.1, range=p.y_range), width=x_max/100, height=0.08*len(driver_df[y_axis].unique()), fill_color=colors[1])\n",
    "            last_response_glyph = p.add_glyph(merged_source, glyph)\n",
    "            last_response_glyphs.append(last_response_glyph)\n",
    "            # Data label \n",
    "            labels = LabelSet(x=time_unit + '_to_last_response_mean', y=dodge(y_axis, 0, range=p.y_range), text=time_unit + '_to_last_response_mean', x_offset=merged_x_offset, y_offset=merged_y_offset,#-60,\n",
    "                      text_font_size=\"12pt\", text_color=colors[1],\n",
    "                      source=merged_source, text_align='center')\n",
    "            p.add_layout(labels)\n",
    "            \n",
    "\n",
    "            #for nonmerged\n",
    "            glyph = Rect(x=time_unit + '_to_last_response_mean', y=dodge(y_axis, 0.1, range=p.y_range), width=x_max/100, height=0.08*len(driver_df[y_axis].unique()), fill_color=colors[1])\n",
    "            last_response_glyph = p.add_glyph(not_merged_source, glyph)\n",
    "            last_response_glyphs.append(last_response_glyph)\n",
    "            # Data label \n",
    "            labels = LabelSet(x=time_unit + '_to_last_response_mean', y=dodge(y_axis, 0, range=p.y_range), text=time_unit + '_to_last_response_mean', x_offset = not_merged_x_offset, y_offset=not_merged_y_offset,#40,\n",
    "                      text_font_size=\"12pt\", text_color=colors[1],\n",
    "                      source=not_merged_source, text_align='center')\n",
    "            p.add_layout(labels)\n",
    "\n",
    "        p.title.align = \"center\"\n",
    "        p.title.text_font_size = \"16px\"\n",
    "\n",
    "        p.xaxis.axis_label = \"Days to Close\"\n",
    "        p.xaxis.axis_label_text_font_size = \"16px\"\n",
    "        p.xaxis.major_label_text_font_size = \"16px\"\n",
    "        \n",
    "        #adjust the starting point and ending point based on the maximum of maximum of the graph\n",
    "        p.x_range = Range1d(maximum/30 * -1, maximum*1.15)\n",
    "\n",
    "        p.yaxis.axis_label = \"Repository\" if y_axis == 'repo_name' else 'Year Closed' if y_axis == 'closed_year' else ''\n",
    "        p.yaxis.axis_label_text_font_size = \"16px\"\n",
    "        p.yaxis.major_label_text_font_size = \"16px\"\n",
    "        p.ygrid.grid_line_color = None\n",
    "        p.y_range.range_padding = 0.15\n",
    "\n",
    "        p.outline_line_color = None\n",
    "        p.toolbar.logo = None\n",
    "        p.toolbar_location = None\n",
    "\n",
    "        def add_legend(location, orientation, side):\n",
    "            legend = Legend(\n",
    "                items=[\n",
    "                    (\"Mean Days to First Response\", first_response_glyphs),\n",
    "                    (\"Mean Days to Last Response\", last_response_glyphs),\n",
    "                    (\"Merged Mean Days to Close\", merged_days_to_close_glyphs),\n",
    "                    (\"Not Merged Mean Days to Close\", not_merged_days_to_close_glyphs)\n",
    "                ],\n",
    "\n",
    "                location=location, \n",
    "                orientation=orientation,\n",
    "                border_line_color=\"black\"\n",
    "        #         title='Example Title'\n",
    "            )\n",
    "            p.add_layout(legend, side)\n",
    "\n",
    "    #     add_legend((150, 50), \"horizontal\", \"center\")\n",
    "        add_legend(legend_position, \"vertical\", \"right\")\n",
    "        \n",
    "        plot = p\n",
    "        \n",
    "        p = figure(width = plot_width, height = 200, margin = (0, 0, 0, 0))\n",
    "        caption = \"Caption Here\"\n",
    "        p.add_layout(Label(\n",
    "        x = 0, # Change to shift caption left or right\n",
    "        y = 160, \n",
    "        x_units = 'screen',\n",
    "        y_units = 'screen',\n",
    "        text='{}'.format(caption),\n",
    "        text_font = 'times', # Use same font as paper\n",
    "        text_font_size = '15pt',\n",
    "        render_mode='css'\n",
    "        ))\n",
    "        p.outline_line_color = None\n",
    "\n",
    "        caption_plot = p\n",
    "\n",
    "        grid = gridplot([[plot], [caption_plot]])\n",
    "\n",
    "        show(grid)\n",
    "\n",
    "        if save_files:\n",
    "            export_png(grid, filename=\"./images/hbar_response_times/mean_response_times__{}_PRs__yaxis_{}__repo_{}.png\".format(description, y_axis, repo_dict[repo_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for repo_name in pr_closed['repo_name'].unique():\n",
    "#visualize_mean_response_times(pr_closed, repo_id=repo_list, legend_position='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_mean_time_between_responses(data_dict, repo_id, time_unit='Days', x_axis='closed_yearmonth', description=\"All Closed\", line_group='merged_flag', y_axis='average_days_between_responses', num_outliers_repo_map={}):\n",
    "    \n",
    "    if type(repo_id) == type(repo_list):\n",
    "        repo_ids = repo_id\n",
    "    else:\n",
    "        repo_ids = [repo_id]\n",
    "    \n",
    "    for repo_id in repo_ids:\n",
    "    \n",
    "        output_notebook()\n",
    "        plot_width = 950\n",
    "        p1 = figure(x_axis_type=\"datetime\", title=\"{}: Mean {} Between Comments by Month Closed for {} Pull Requests\".format(repo_dict[repo_id], time_unit, description), plot_width=plot_width, x_range=(pr_all[x_axis].min(),pr_all[x_axis].max()), plot_height=500, toolbar_location=None)\n",
    "        colors = Category20[10][6:]\n",
    "        color_index = 0\n",
    "\n",
    "        glyphs = []\n",
    "\n",
    "        possible_maximums = []\n",
    "        for data_desc, input_df in data_dict.items():\n",
    "\n",
    "            driver_df = input_df.copy()\n",
    "\n",
    "            driver_df = remove_outliers(driver_df, y_axis, num_outliers_repo_map)\n",
    "\n",
    "            driver_df = driver_df.loc[driver_df['repo_id'] == repo_id]\n",
    "            index = 0\n",
    "\n",
    "            driver_df_mean = driver_df.groupby(['repo_id', line_group, x_axis],as_index=False).mean()\n",
    "\n",
    "            title_ending = ''\n",
    "            if repo_id:\n",
    "                title_ending += ' for Repo: {}'.format(repo_id)\n",
    "\n",
    "            for group_num, line_group_value in enumerate(driver_df[line_group].unique(), color_index):\n",
    "                glyphs.append(p1.line(driver_df_mean.loc[driver_df_mean[line_group] == line_group_value][x_axis], driver_df_mean.loc[driver_df_mean[line_group] == line_group_value][y_axis], color=colors[group_num], line_width = 3))\n",
    "                color_index += 1\n",
    "                possible_maximums.append(max(driver_df_mean.loc[driver_df_mean[line_group] == line_group_value][y_axis].dropna()))\n",
    "        for repo, num_outliers in num_outliers_repo_map.items():\n",
    "            if repo_name == repo:\n",
    "                p1.add_layout(Title(text=\"** {} outliers for {} were removed\".format(num_outliers, repo), align=\"center\"), \"below\")\n",
    "\n",
    "        p1.grid.grid_line_alpha = 0.3\n",
    "        p1.xaxis.axis_label = 'Month Closed'\n",
    "        p1.xaxis.ticker.desired_num_ticks = 15\n",
    "        p1.yaxis.axis_label = 'Mean {} Between Responses'.format(time_unit)\n",
    "        p1.legend.location = \"top_left\"\n",
    "\n",
    "        legend = Legend(\n",
    "            items=[\n",
    "                (\"All Not Merged / Rejected\", [glyphs[0]]),\n",
    "                (\"All Merged / Accepted\", [glyphs[1]]),\n",
    "                (\"Slowest 20% Not Merged / Rejected\", [glyphs[2]]),\n",
    "                (\"Slowest 20% Merged / Accepted\", [glyphs[3]])\n",
    "            ],\n",
    "\n",
    "            location='center_right', \n",
    "            orientation='vertical',\n",
    "            border_line_color=\"black\"\n",
    "        )\n",
    "\n",
    "        p1.add_layout(legend, 'right')\n",
    "\n",
    "        p1.title.text_font_size = \"16px\"\n",
    "\n",
    "        p1.xaxis.axis_label_text_font_size = \"16px\"\n",
    "        p1.xaxis.major_label_text_font_size = \"16px\"\n",
    "\n",
    "        p1.yaxis.axis_label_text_font_size = \"16px\"\n",
    "        p1.yaxis.major_label_text_font_size = \"16px\"\n",
    "        p1.xaxis.major_label_orientation = 45.0\n",
    "        \n",
    "        p1.y_range = Range1d(0,  max(possible_maximums)*1.15)\n",
    "        \n",
    "        plot = p1\n",
    "        \n",
    "        p = figure(width = plot_width, height=200, margin = (0, 0, 0, 0))\n",
    "        caption = \"This graph shows the average number of days between comments for all closed pull requests per month in four categories. These four categories are All Merged, All Not Merged, Slowest 20% Merged, and Slowest 20% Not Merged.\"\n",
    "        p.add_layout(Label(\n",
    "        x = 0, # Change to shift caption left or right\n",
    "        y = 160, \n",
    "        x_units = 'screen',\n",
    "        y_units = 'screen',\n",
    "        text='{}'.format(caption),\n",
    "        text_font = 'times', # Use same font as paper\n",
    "        text_font_size = '15pt',\n",
    "        render_mode='css'\n",
    "        ))\n",
    "        p.outline_line_color = None\n",
    "\n",
    "        caption_plot = p\n",
    "\n",
    "        grid = gridplot([[plot], [caption_plot]])\n",
    "\n",
    "        show(grid)\n",
    "\n",
    "        if save_files:\n",
    "            repo_extension = 'All' if not repo_name else repo_name\n",
    "            export_png(grid, filename=\"./images/line_mean_time_between_comments/line_mean_time_between_comments__{}_PRs__yaxis_{}__repo_{}.png\".format(description, y_axis, repo_dict[repo_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for repo_name in pr_all['repo_name'].unique():\n",
    "#visualize_mean_time_between_responses({'All':pr_closed,'Slowest 20%':pr_slow20_not_merged.append(pr_slow20_merged,ignore_index=True)}, repo_id = repo_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_time_to_first_comment(input_df, repo_id, x_axis='pr_closed_at', y_axis='days_to_first_response', description='All', num_outliers_repo_map={}, group_by='merged_flag', same_scales=True, columns=2, legend_position='top_right', remove_outliers = 0):\n",
    "    if type(repo_id) == type(repo_list):\n",
    "        repo_ids = repo_id\n",
    "    else:\n",
    "        repo_ids = [repo_id]\n",
    "    \n",
    "    for repo_id in repo_ids:\n",
    "    \n",
    "        output_notebook()\n",
    "\n",
    "        driver_df = input_df.copy()\n",
    "\n",
    "        driver_df = driver_df.loc[driver_df['repo_id'] == repo_id]\n",
    "\n",
    "        group_by_groups = sorted(driver_df[group_by].unique())\n",
    "\n",
    "        seconds = ((driver_df[x_axis].max() + datetime.timedelta(days=25))- (driver_df[x_axis].min() - datetime.timedelta(days=30))).total_seconds()\n",
    "        quarter_years = seconds / 10506240\n",
    "        quarter_years = round(quarter_years)\n",
    "\n",
    "        title_beginning = '{}: '.format(repo_dict[repo_id]) \n",
    "        plot_width = 180 * 5\n",
    "        p = figure(x_range=(driver_df[x_axis].min() - datetime.timedelta(days=30), driver_df[x_axis].max() + datetime.timedelta(days=25)), \n",
    "                  #(driver_df[y_axis].min(), driver_df[y_axis].max()), \n",
    "                   toolbar_location=None,\n",
    "                   title='{}Days to First Response for {} Closed Pull Requests'.format(title_beginning, description), plot_width=plot_width, \n",
    "                   plot_height=400, x_axis_type='datetime')\n",
    "\n",
    "        for index, group_by_group in enumerate(group_by_groups):\n",
    "            p.scatter(x_axis, y_axis, color=colors[index], marker=\"square\", source=driver_df.loc[driver_df[group_by] == group_by_group], legend_label=group_by_group)\n",
    "\n",
    "            if group_by_group == \"Merged / Accepted\":\n",
    "                merged_values = driver_df.loc[driver_df[group_by] == group_by_group][y_axis].dropna().values.tolist()\n",
    "            else:\n",
    "                not_merged_values = driver_df.loc[driver_df[group_by] == group_by_group][y_axis].dropna().values.tolist()\n",
    "\n",
    "        values = not_merged_values + merged_values\n",
    "        #values.fillna(0)\n",
    "\n",
    "        for value in range(0, remove_outliers):\n",
    "            values.remove(max(values))\n",
    " \n",
    "        #determine y_max by finding the max of the values and scaling it up a small amoutn\n",
    "        y_max = max(values)*1.0111\n",
    "        outliers = driver_df.loc[driver_df[y_axis] > y_max]\n",
    "        if len(outliers) > 0:\n",
    "            if repo_id:\n",
    "                p.add_layout(Title(text=\"** Outliers cut off at {} days: {} outlier(s) for {} were removed **\".format(y_max, len(outliers), repo_name), align=\"center\"), \"below\")\n",
    "            else:\n",
    "                p.add_layout(Title(text=\"** Outliers cut off at {} days: {} outlier(s) were removed **\".format(y_max, len(outliers)), align=\"center\"), \"below\")\n",
    "\n",
    "        p.xaxis.axis_label = 'Date Closed' if x_axis == 'pr_closed_at' else 'Date Created' if x_axis == 'pr_created_at' else 'Date'\n",
    "        p.yaxis.axis_label = 'Days to First Response'\n",
    "        p.legend.location = legend_position\n",
    "\n",
    "        p.title.align = \"center\"\n",
    "        p.title.text_font_size = \"16px\"\n",
    "\n",
    "        p.xaxis.axis_label_text_font_size = \"16px\"\n",
    "        p.xaxis.major_label_text_font_size = \"16px\"\n",
    "\n",
    "        p.yaxis.axis_label_text_font_size = \"16px\"\n",
    "        p.yaxis.major_label_text_font_size = \"16px\"\n",
    "\n",
    "        p.y_range = Range1d(0, y_max)\n",
    "        \n",
    "        plot = p\n",
    "        \n",
    "        p = figure(width = plot_width, height=200, margin = (0, 0, 0, 0))\n",
    "        caption = \"This graph shows the days to first reponse for individual pull requests, either Merged or Not Merged.\"\n",
    "        p.add_layout(Label(\n",
    "        x = 0, # Change to shift caption left or right\n",
    "        y = 160, \n",
    "        x_units = 'screen',\n",
    "        y_units = 'screen',\n",
    "        text='{}'.format(caption),\n",
    "        text_font = 'times', # Use same font as paper\n",
    "        text_font_size = '15pt',\n",
    "        render_mode='css'\n",
    "        ))\n",
    "        p.outline_line_color = None\n",
    "\n",
    "        caption_plot = p\n",
    "\n",
    "        grid = gridplot([[plot], [caption_plot]])\n",
    "\n",
    "        show(grid)\n",
    "\n",
    "        if save_files:\n",
    "            repo_extension = repo_ids\n",
    "            export_png(grid, filename=\"./images/first_comment_times/scatter_first_comment_times__{}_PRs__xaxis_{}__repo_{}.png\".format(description, x_axis, repo_dict[repo_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for repo_name in pr_all['repo_name'].unique():\n",
    "#visualize_time_to_first_comment(pr_closed, repo_id= repo_list, legend_position='top_right', remove_outliers = scatter_plot_outliers_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hex_to_RGB(hex):\n",
    "    ''' \"#FFFFFF\" -> [255,255,255] '''\n",
    "    # Pass 16 to the integer function for change of base\n",
    "    return [int(hex[i:i+2], 16) for i in range(1,6,2)]\n",
    "\n",
    "def color_dict(gradient):\n",
    "    ''' Takes in a list of RGB sub-lists and returns dictionary of\n",
    "    colors in RGB and hex form for use in a graphing function\n",
    "    defined later on '''\n",
    "    return {\"hex\":[RGB_to_hex(RGB) for RGB in gradient],\n",
    "      \"r\":[RGB[0] for RGB in gradient],\n",
    "      \"g\":[RGB[1] for RGB in gradient],\n",
    "      \"b\":[RGB[2] for RGB in gradient]}\n",
    "\n",
    "def RGB_to_hex(RGB):\n",
    "    ''' [255,255,255] -> \"#FFFFFF\" '''\n",
    "    # Components need to be integers for hex to make sense\n",
    "    RGB = [int(x) for x in RGB]\n",
    "    return \"#\"+\"\".join([\"0{0:x}\".format(v) if v < 16 else\n",
    "            \"{0:x}\".format(v) for v in RGB])\n",
    "\n",
    "def linear_gradient(start_hex, finish_hex=\"#FFFFFF\", n=10):\n",
    "    ''' returns a gradient list of (n) colors between\n",
    "    two hex colors. start_hex and finish_hex\n",
    "    should be the full six-digit color string,\n",
    "    inlcuding the number sign (\"#FFFFFF\") '''\n",
    "    # Starting and ending colors in RGB form\n",
    "    s = hex_to_RGB(start_hex)\n",
    "    f = hex_to_RGB(finish_hex)\n",
    "    # Initilize a list of the output colors with the starting color\n",
    "    RGB_list = [s]\n",
    "    # Calcuate a color at each evenly spaced value of t from 1 to n\n",
    "    for t in range(1, n):\n",
    "        # Interpolate RGB vector for color at the current value of t\n",
    "        curr_vector = [\n",
    "          int(s[j] + (float(t)/(n-1))*(f[j]-s[j]))\n",
    "          for j in range(3)\n",
    "        ]\n",
    "        # Add it to our list of output colors\n",
    "        RGB_list.append(curr_vector)\n",
    "\n",
    "    return color_dict(RGB_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bokeh'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbokeh\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BasicTicker, ColorBar, LinearColorMapper, PrintfTickFormatter, LogTicker, Label\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbokeh\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transform\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevents_types_heat_map\u001b[39m(input_df, repo_id, include_comments\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, x_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclosed_year\u001b[39m\u001b[38;5;124m'\u001b[39m, facet\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmerged_flag\u001b[39m\u001b[38;5;124m\"\u001b[39m,columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, x_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1100\u001b[39m, same_scales\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, y_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m, description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll Closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage Pull Request Event Types for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m Pull Requests\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bokeh'"
     ]
    }
   ],
   "source": [
    "from bokeh.models import BasicTicker, ColorBar, LinearColorMapper, PrintfTickFormatter, LogTicker, Label\n",
    "from bokeh.transform import transform\n",
    "        \n",
    "def events_types_heat_map(input_df, repo_id, include_comments=True, x_axis='closed_year', facet=\"merged_flag\",columns=2, x_max=1100, same_scales=True, y_axis='repo_name', description=\"All Closed\", title=\"Average Pull Request Event Types for {} Pull Requests\"):\n",
    "    if type(repo_id) == type(repo_list):\n",
    "        repo_ids = repo_id\n",
    "    else:\n",
    "        repo_ids = [repo_id]\n",
    "    \n",
    "    for repo_id in repo_ids:\n",
    "    \n",
    "        colors = linear_gradient('#f5f5dc', '#fff44f', 150)['hex']\n",
    "\n",
    "        driver_df = input_df.copy()\n",
    "        driver_df[x_axis] = driver_df[x_axis].astype(str)\n",
    "\n",
    "        driver_df = driver_df.loc[driver_df['repo_id'] == repo_id]\n",
    "\n",
    "        if facet == 'closed_year' or y_axis == 'closed_year':\n",
    "            driver_df['closed_year'] = driver_df['closed_year'].astype(int).astype(str)\n",
    "\n",
    "        optional_comments = ['comment_count'] if include_comments else []\n",
    "        driver_df = driver_df[['repo_id', 'repo_name',x_axis, 'assigned_count',\n",
    "              'review_requested_count',\n",
    "              'labeled_count',\n",
    "              'subscribed_count',\n",
    "              'mentioned_count',\n",
    "              'referenced_count',\n",
    "              'closed_count',\n",
    "              'head_ref_force_pushed_count',\n",
    "              'merged_count',\n",
    "              'milestoned_count',\n",
    "              'unlabeled_count',\n",
    "              'head_ref_deleted_count', facet ] + optional_comments]\n",
    "        y_groups = [\n",
    "              'review_requested_count',\n",
    "              'labeled_count',\n",
    "              'subscribed_count',\n",
    "              'referenced_count',\n",
    "              'closed_count',\n",
    "    #           'milestoned_count',\n",
    "              ] + optional_comments\n",
    "        output_notebook()\n",
    "        optional_group_comments = ['comment'] if include_comments else []\n",
    "    #     y_groups = ['subscribed', 'mentioned', 'labeled', 'review_requested', 'head_ref_force_pushed', 'referenced', 'closed', 'merged', 'unlabeled', 'head_ref_deleted', 'milestoned', 'assigned'] + optional_group_comments\n",
    "\n",
    "        x_groups = sorted(list(driver_df[x_axis].unique()))\n",
    "\n",
    "        grid_array = []\n",
    "        grid_row = []  \n",
    "\n",
    "        for index, facet_group in enumerate(sorted(driver_df[facet].unique())):\n",
    "\n",
    "            facet_data = driver_df.loc[driver_df[facet] == facet_group]\n",
    "    #         display(facet_data.sort_values('merged_count', ascending=False).head(50))\n",
    "            driver_df_mean = facet_data.groupby(['repo_id', 'repo_name', x_axis], as_index=False).mean().round(1)\n",
    "    #         data = {'Y' : y_groups}\n",
    "    #         for group in y_groups:\n",
    "    #             data[group] = driver_df_mean[group].tolist()\n",
    "            plot_width = 700\n",
    "            p = figure(y_range=y_groups, plot_height=500, plot_width=plot_width, x_range=x_groups, \n",
    "                       title='{}'.format(format(facet_group)))\n",
    "\n",
    "            for y_group in y_groups:\n",
    "                driver_df_mean['field'] = y_group\n",
    "                source = ColumnDataSource(driver_df_mean)\n",
    "                mapper = LinearColorMapper(palette=colors, low=driver_df_mean[y_group].min(), high=driver_df_mean[y_group].max())\n",
    "\n",
    "                p.rect(y='field', x=x_axis, width=1, height=1, source=source,\n",
    "                       line_color=None, fill_color=transform(y_group, mapper))\n",
    "                # Data label \n",
    "                labels = LabelSet(x=x_axis, y='field', text=y_group, y_offset=-8,\n",
    "                          text_font_size=\"12pt\", text_color='black',\n",
    "                          source=source, text_align='center')\n",
    "                p.add_layout(labels)\n",
    "\n",
    "                color_bar = ColorBar(color_mapper=mapper, location=(0, 0),\n",
    "                                     ticker=BasicTicker(desired_num_ticks=9),\n",
    "                                     formatter=PrintfTickFormatter(format=\"%d\"))\n",
    "    #         p.add_layout(color_bar, 'right')\n",
    "\n",
    "\n",
    "            p.y_range.range_padding = 0.1\n",
    "            p.ygrid.grid_line_color = None\n",
    "\n",
    "            p.legend.location = \"bottom_right\"\n",
    "            p.axis.minor_tick_line_color = None\n",
    "            p.outline_line_color = None\n",
    "\n",
    "            p.xaxis.axis_label = 'Year Closed'\n",
    "            p.yaxis.axis_label = 'Event Type'\n",
    "\n",
    "            p.title.align = \"center\"\n",
    "            p.title.text_font_size = \"15px\"\n",
    "\n",
    "            p.xaxis.axis_label_text_font_size = \"16px\"\n",
    "            p.xaxis.major_label_text_font_size = \"16px\"\n",
    "\n",
    "            p.yaxis.axis_label_text_font_size = \"16px\"\n",
    "            p.yaxis.major_label_text_font_size = \"16px\"\n",
    "\n",
    "            grid_row.append(p)\n",
    "            if index % columns == columns - 1:\n",
    "                grid_array.append(grid_row)\n",
    "                grid_row = []\n",
    "        grid = gridplot(grid_array)\n",
    "        \n",
    "        #add title, the title changes its x value based on the number of x_groups so that it stays centered\n",
    "        label=Label(x=-len(x_groups), y=6.9, text='{}: Average Pull Request Event Types for {} Closed Pull Requests'.format(repo_dict[repo_id], description), render_mode='css', text_font_size = '17px', text_font_style= 'bold')\n",
    "        p.add_layout(label)\n",
    "\n",
    "        show(grid, plot_width=1200, plot_height=1200)\n",
    "        if save_files:\n",
    "            comments_included = 'comments_included' if include_comments else 'comments_not_included'\n",
    "            repo_extension = 'All' if not repo_id else repo_id\n",
    "            export_png(grid, filename=\"./images/h_stacked_bar_mean_event_types/mean_event_types__facet_{}__{}_PRs__yaxis_{}__{}__repo_{}.png\".format(facet, description, y_axis, comments_included, repo_dict[repo_id]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for repo_name in pr_all['repo_name'].unique():\n",
    "#events_types_heat_map(pr_closed, repo_id=repo_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "red_green_gradient = linear_gradient('#0080FF', '#DC143C', 150)['hex']\n",
    "    #32CD32\n",
    "def heat_map(input_df, repo_id, x_axis='repo_name', group_by='merged_flag', y_axis='closed_yearmonth', same_scales=True, description=\"All Closed\", heat_field='pr_duration_days', columns=2, remove_outliers = 0):\n",
    "\n",
    "\n",
    "    output_notebook()\n",
    "\n",
    "    driver_df = input_df.copy()[['repo_id', y_axis, group_by, x_axis, heat_field]]\n",
    "    \n",
    "    print(driver_df)\n",
    "\n",
    "    if display_grouping == 'repo':\n",
    "        driver_df = driver_df.loc[driver_df['repo_id'] == repo_id]\n",
    "\n",
    "    driver_df[y_axis] = driver_df[y_axis].astype(str)\n",
    "\n",
    "    # add new group by + xaxis column \n",
    "    driver_df['grouped_x'] = driver_df[x_axis] + ' - ' + driver_df[group_by]\n",
    "\n",
    "    driver_df_mean = driver_df.groupby(['grouped_x', y_axis], as_index=False).mean()\n",
    "\n",
    "    colors = red_green_gradient\n",
    "    y_groups = driver_df_mean[y_axis].unique()\n",
    "    x_groups = sorted(driver_df[x_axis].unique())\n",
    "    grouped_x_groups = sorted(driver_df_mean['grouped_x'].unique())\n",
    "\n",
    "    values = driver_df_mean['pr_duration_days'].values.tolist()\n",
    "    for i in range(0, remove_outliers):\n",
    "        values.remove(max(values))\n",
    "\n",
    "    heat_max = max(values)* 1.02\n",
    "\n",
    "    mapper = LinearColorMapper(palette=colors, low=driver_df_mean[heat_field].min(), high=heat_max)#driver_df_mean[heat_field].max())\n",
    "\n",
    "    source = ColumnDataSource(driver_df_mean)\n",
    "    title_beginning = repo_dict[repo_id] + ':' if not type(repo_id) == type(repo_list) else ''\n",
    "    plot_width = 1100\n",
    "    p = figure(plot_width=plot_width, plot_height=300, title=\"{} Mean Duration (Days) {} Pull Requests\".format(title_beginning,description),\n",
    "               y_range=grouped_x_groups[::-1], x_range=y_groups,\n",
    "               toolbar_location=None, tools=\"\")#, x_axis_location=\"above\")\n",
    "\n",
    "    for x_group in x_groups:\n",
    "        outliers = driver_df_mean.loc[(driver_df_mean[heat_field] > heat_max) & (driver_df_mean['grouped_x'].str.contains(x_group))]\n",
    "\n",
    "        if len(outliers) > 0:\n",
    "            p.add_layout(Title(text=\"** Outliers capped at {} days: {} outlier(s) for {} were capped at {} **\".format(heat_max, len(outliers), x_group, heat_max), align=\"center\"), \"below\")\n",
    "\n",
    "    p.rect(x=y_axis, y='grouped_x', width=1, height=1, source=source,\n",
    "           line_color=None, fill_color=transform(heat_field, mapper))\n",
    "\n",
    "    color_bar = ColorBar(color_mapper=mapper, location=(0, 0),\n",
    "                         ticker=BasicTicker(desired_num_ticks=9),\n",
    "                         formatter=PrintfTickFormatter(format=\"%d\"))\n",
    "\n",
    "    p.add_layout(color_bar, 'right')\n",
    "\n",
    "    p.title.align = \"center\"\n",
    "    p.title.text_font_size = \"16px\"\n",
    "\n",
    "    p.axis.axis_line_color = None\n",
    "    p.axis.major_tick_line_color = None\n",
    "    p.axis.major_label_text_font_size = \"11pt\"\n",
    "    p.axis.major_label_standoff = 0\n",
    "    p.xaxis.major_label_orientation = 1.0\n",
    "    p.xaxis.axis_label = 'Month Closed' if y_axis[0:6] == 'closed' else 'Date Created' if y_axis[0:7] == 'created' else 'Repository' if y_axis == 'repo_name' else ''\n",
    "#     p.yaxis.axis_label = 'Merged Status'\n",
    "\n",
    "    p.title.text_font_size = \"16px\"\n",
    "\n",
    "    p.xaxis.axis_label_text_font_size = \"16px\"\n",
    "    p.xaxis.major_label_text_font_size = \"14px\"\n",
    "\n",
    "    p.yaxis.major_label_text_font_size = \"15px\"\n",
    "\n",
    "    plot = p\n",
    "\n",
    "    p = figure(width = plot_width, height=200, margin = (0, 0, 0, 0))\n",
    "    caption = \"Caption Here\"\n",
    "    p.add_layout(Label(\n",
    "    x = 0, # Change to shift caption left or right\n",
    "    y = 160, \n",
    "    x_units = 'screen',\n",
    "    y_units = 'screen',\n",
    "    text='{}'.format(caption),\n",
    "    text_font = 'times', # Use same font as paper\n",
    "    text_font_size = '15pt',\n",
    "    render_mode='css'\n",
    "    ))\n",
    "    p.outline_line_color = None\n",
    "\n",
    "    caption_plot = p\n",
    "\n",
    "    grid = gridplot([[plot], [caption_plot]])\n",
    "\n",
    "    show(grid)\n",
    "\n",
    "    if save_files:\n",
    "        repo_extension = 'All' if not repo_id else repo_id\n",
    "        export_png(grid, filename=\"./images/heat_map_pr_duration_merged_status/heat_map_duration_by_merged_status__{}_PRs__yaxis_{}__repo_{}.png\".format(description, y_axis, repo_dict[repo_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#heat_map(pr_closed, repo_id=25502)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vertical_grouped_bar' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m display_grouping \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m repo_id \u001b[38;5;129;01min\u001b[39;00m repo_set:\n\u001b[0;32m----> 3\u001b[0m         \u001b[43mvertical_grouped_bar\u001b[49m(pr_all, repo_id\u001b[38;5;241m=\u001b[39mrepo_id)\n\u001b[1;32m      4\u001b[0m         horizontal_stacked_bar(pr_closed, repo_id\u001b[38;5;241m=\u001b[39mrepo_id)\n\u001b[1;32m      5\u001b[0m         merged_ratio_vertical_grouped_bar({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAll\u001b[39m\u001b[38;5;124m'\u001b[39m:pr_closed,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSlowest 20\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m:pr_slow20_not_merged\u001b[38;5;241m.\u001b[39mappend(pr_slow20_merged,ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)}, repo_id \u001b[38;5;241m=\u001b[39m repo_id)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vertical_grouped_bar' is not defined"
     ]
    }
   ],
   "source": [
    "if display_grouping == 'repo':\n",
    "    for repo_id in repo_set:\n",
    "        vertical_grouped_bar(pr_all, repo_id=repo_id)\n",
    "        horizontal_stacked_bar(pr_closed, repo_id=repo_id)\n",
    "        merged_ratio_vertical_grouped_bar({'All':pr_closed,'Slowest 20%':pr_slow20_not_merged.append(pr_slow20_merged,ignore_index=True)}, repo_id = repo_id)\n",
    "        visualize_mean_response_times(pr_closed, repo_id=repo_id, legend_position='center')\n",
    "        visualize_mean_time_between_responses({'All':pr_closed,'Slowest 20%':pr_slow20_not_merged.append(pr_slow20_merged,ignore_index=True)}, repo_id = repo_id)\n",
    "        visualize_time_to_first_comment(pr_closed, repo_id= repo_id, legend_position='top_right', remove_outliers = scatter_plot_outliers_removed)\n",
    "        events_types_heat_map(pr_closed, repo_id=repo_id)\n",
    "        #print(pr_closed)\n",
    "        pr_duration_frame = pr_closed.assign(pr_duration=(pr_closed['pr_closed_at'] - pr_closed['pr_created_at']))\n",
    "        pr_duration_frame = pr_duration_frame.assign(pr_duration_days = (pr_duration_frame['pr_duration'] / datetime.timedelta(minutes=1))/60/24)\n",
    "        heat_map(pr_duration_frame, repo_id=repo_id)\n",
    "\n",
    "elif display_grouping == 'competitors':\n",
    "    vertical_grouped_bar(pr_all, repo_id=repo_list)\n",
    "    horizontal_stacked_bar(pr_closed, repo_id=repo_list)\n",
    "    merged_ratio_vertical_grouped_bar({'All':pr_closed,'Slowest 20%':pr_slow20_not_merged.append(pr_slow20_merged,ignore_index=True)}, repo_id = repo_list)\n",
    "    visualize_mean_response_times(pr_closed, repo_id=repo_list, legend_position='center')\n",
    "    visualize_mean_time_between_responses({'All':pr_closed,'Slowest 20%':pr_slow20_not_merged.append(pr_slow20_merged,ignore_index=True)}, repo_id = repo_list)\n",
    "    visualize_time_to_first_comment(pr_closed, repo_id= repo_list, legend_position='top_right', remove_outliers = scatter_plot_outliers_removed)\n",
    "    events_types_heat_map(pr_closed, repo_id=repo_list)\n",
    "    pr_duration_frame = pr_closed.assign(pr_duration=(pr_closed['pr_closed_at'] - pr_closed['pr_created_at']))\n",
    "    pr_duration_frame = pr_duration_frame.assign(pr_duration_days = (pr_duration_frame['pr_duration'] / datetime.timedelta(minutes=1))/60/24)\n",
    "    heat_map(pr_duration_frame, repo_id=repo_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
